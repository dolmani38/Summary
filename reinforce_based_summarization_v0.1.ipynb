{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "korean_abstractive_summarizaion_v1.2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dolmani38/Summary/blob/master/reinforce_based_summarization_v0.1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdM3q73ReHxs"
      },
      "source": [
        "# **Korean Summarizer Using Multiple Discriminators**\n",
        "\n",
        "참조 : https://www.kaggle.com/shivamb/beginners-guide-to-text-generation-using-lstms\n",
        "\n",
        "참조 : https://github.com/williamSYSU/TextGAN-PyTorch\n",
        "\n",
        "* 2020년12월27일 v1.0 완전히 실패...\n",
        "* 2020년12월27일 오후 Generator 다시 만들고... 역시 실패 한듯... v1.2 완전히 실패...\n",
        "* 문법 Discriminator를 먼저 학습하고... transfer learning을 사용해 보자.\n",
        "* 그래도 역시 Generator는 다시 만들어야 할 듯."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNBNW5dMZ13G",
        "trusted": true
      },
      "source": [
        "DO_ALL = True # 전체 실행하면서 시간 걸리는 걸 Pass 하려면 이걸 False ...\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "FlvsCFJaeHxt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ac798fe-76e0-459b-da46-bc3c7646c8d9"
      },
      "source": [
        "\n",
        "if DO_ALL:\n",
        "    !pip install sentence-transformers==0.3.0\n",
        "    !pip install transformers==3.0.2\n",
        "    !pip install wikipedia\n",
        "    !pip install konlpy"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers==0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/23/833e0620753a36cb2f18e2e4a4f72fd8c49c123c3f07744b69f8a592e083/sentence-transformers-0.3.0.tar.gz (61kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.9MB/s \n",
            "\u001b[?25hCollecting transformers>=3.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 25.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.0) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.0) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.0) (1.19.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.0) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.0) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.0) (3.2.5)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 50.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (0.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (20.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 54.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.1->sentence-transformers==0.3.0) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.1->sentence-transformers==0.3.0) (3.7.4.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers==0.3.0) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers==0.3.0) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers>=3.0.2->sentence-transformers==0.3.0) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=3.0.2->sentence-transformers==0.3.0) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers, sacremoses\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.3.0-cp36-none-any.whl size=86754 sha256=7779ff7e58580487c09f34a59a84b35b412f2d6cc8dbe8e24e46fe054641302f\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/23/85/85d6a9a6c68f0625a1ecdaad903bb0a78df058c10cf74f9de4\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=70f1c84d74def44f68e4eca7ac6d48161f4f4bed35a9573dc934b5d6018b52a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sentence-transformers sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers, sentence-transformers\n",
            "Successfully installed sacremoses-0.0.43 sentence-transformers-0.3.0 tokenizers-0.9.4 transformers-4.1.1\n",
            "Collecting transformers==3.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 12.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (20.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (1.19.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (2.23.0)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 59.7MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 28.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.0.43)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (4.41.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.0.2) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2) (1.15.0)\n",
            "Installing collected packages: sentencepiece, tokenizers, transformers\n",
            "  Found existing installation: tokenizers 0.9.4\n",
            "    Uninstalling tokenizers-0.9.4:\n",
            "      Successfully uninstalled tokenizers-0.9.4\n",
            "  Found existing installation: transformers 4.1.1\n",
            "    Uninstalling transformers-4.1.1:\n",
            "      Successfully uninstalled transformers-4.1.1\n",
            "Successfully installed sentencepiece-0.1.94 tokenizers-0.8.1rc1 transformers-3.0.2\n",
            "Collecting wikipedia\n",
            "  Downloading https://files.pythonhosted.org/packages/67/35/25e68fbc99e672127cc6fbb14b8ec1ba3dfef035bf1e4c90f78f24a80b7d/wikipedia-1.4.0.tar.gz\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-cp36-none-any.whl size=11686 sha256=24a02e3b4b49c4bfa027a82afbf2907ac3804ee3ded174830897c6462b7d0763\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/2a/18/4e471fd96d12114d16fe4a446d00c3b38fb9efcb744bd31f4a\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n",
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 1.3MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 12.5MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/21/9e2c0dbf9df856e6392a1aec1d18006c60b175aa4e31d351e8278a8a63c0/JPype1-1.2.0-cp36-cp36m-manylinux2010_x86_64.whl (453kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 52.8MB/s \n",
            "\u001b[?25hCollecting tweepy>=3.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/67/c3/6bed87f3b1e5ed2f34bd58bf7978e308c86e255193916be76e5a5ce5dfca/tweepy-3.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.19.4)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: beautifulsoup4, JPype1, tweepy, colorama, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "  Found existing installation: tweepy 3.6.0\n",
            "    Uninstalling tweepy-3.6.0:\n",
            "      Successfully uninstalled tweepy-3.6.0\n",
            "Successfully installed JPype1-1.2.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2 tweepy-3.10.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Em1oCkJceHxz"
      },
      "source": [
        "# keras module for building LSTM \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "from tensorflow.python.framework import tensor_shape\n",
        "import keras.utils as ku \n",
        "\n",
        "# set seeds for reproducability\n",
        "from tensorflow.random import set_seed\n",
        "from numpy.random import seed\n",
        "set_seed(2)\n",
        "seed(1)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string, os \n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFdj0QfSeHx1"
      },
      "source": [
        "# 학습을 위한 데이터 준비"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94NlJEeHeHx3"
      },
      "source": [
        "네이버 뉴스에서 아무거나 하나 Text를 얻어옴\n",
        "\n",
        "이것을 '요약' 목표"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "lz5XtC9MeHx5"
      },
      "source": [
        "org_text = \"\"\"\n",
        "옛날 어느 집에 귀여운 여자 아기가 태어났어요.\n",
        "아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요.\n",
        "그러던 어느날, 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요.\n",
        "소녀의 아버지는 홀로 남은 소녀가 걱정되었어요.\n",
        "그래서 얼마 후 새어머니를 맞이했어요.\n",
        "새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요.\n",
        "그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요.\n",
        "새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요.\n",
        "그런데 이번에는 아버지마저 돌아가셨어요.\n",
        "소녀는 하녀처럼 하루 종일 쓸고, 닦고, 집안일을 도맡아 했어요.\n",
        "해도 해도 끝이 없는 집안일이 힘들어 지칠때면\n",
        "난롯가에 앉아서 잠시 쉬곤 했지요.\n",
        "\"엄마, 저애를 신데렐라라고 불러야겠어요.\"\n",
        "\"온통 재투성이잖아요. 호호호!\" 두 언니는 소녀를 놀려 댔어요.\n",
        "어느 날, 왕궁에서 무도회가 열렸어요.\n",
        "신데렐라의 집에도 초대장이 왔어요.\n",
        "새어머니는 언니들을 데리고 무도회장으로 떠났어요.\n",
        "신데렐라도 무도회에 가고 싶었어요.\n",
        "혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요.\n",
        "\"신데렐라, 너도 무도회에 가고 싶니?\"\n",
        "신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요.\n",
        "\"내가 너를 무도회에 보내주마 호박 한개와 생쥐 두마리, 도마뱀을 구해 오렴.\"\n",
        "마법사 할머니가 주문을 외웠어요.\n",
        "그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금 마차로 변했어요.\n",
        "이번에는 생쥐와 도마뱀을 건드렸어요.\n",
        "그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했답니다.\n",
        "신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요.\n",
        "\"신데렐라, 발을 내밀어 보거라.\"\n",
        "할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요.\n",
        "\"신데렐라, 밤 열두시가 되면 모든게 처음대로 돌아간단다. 황금 마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼. 그러니까 반드시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지?\"\n",
        "왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요.\n",
        "왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고,\n",
        "신데렐라하고만 춤을 추었어요.\n",
        "신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요.\n",
        "땡, 땡, 땡...... 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요.\n",
        "신데렐라가 허둥지둥 왕궁을 빠져나가는데,\n",
        "유리 구두 한 짝이 벗겨졌어요.\n",
        "하지만 구두를 주울 틈이 없었어요.\n",
        "신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요.\n",
        "왕자님은 유리 구두를 가지고 임금님께 가서 말했어요.\n",
        "\"이 유리 구두의 주인과 결혼하겠어요.\"\n",
        "그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요.\n",
        "언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만 한눈에 보기에도 유리 구두는 너무 작았어요.\n",
        "그때, 신데렐라가 조용히 다가와 말했어요.\n",
        "\"저도 한번 신어 볼 수 있나요?\"\n",
        "신데렐라는 신하게 건넨 유리 구두를 신었어요,\n",
        "유리 구두는 신데렐라의 발에 꼭 맞았어요.\n",
        "신하들은 신데렐라를 왕궁으로 데리고 갔어요.\n",
        "그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요.\n",
        "\"\"\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "qa-qIW1h3DkA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "f031ce4b-8af7-4244-ef9b-e14b2ee1b503"
      },
      "source": [
        "# 간단한 전처리\n",
        "def clean_text(txt):\n",
        "    txt = txt.replace('\\n',' ')\n",
        "    txt = txt.replace('=','')\n",
        "    txt = txt.replace('\\\"','')   \n",
        "    txt = txt.replace('\\'','')\n",
        "    txt = txt.replace(',','')\n",
        "    txt = txt.replace('..','')\n",
        "    txt = txt.replace('.','. ')\n",
        "    txt = txt.replace('  ',' ')\n",
        "    return txt \n",
        "\n",
        "org_text = clean_text(org_text).strip()\n",
        "org_text"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'옛날 어느 집에 귀여운 여자 아기가 태어났어요. 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요. 그러던 어느날 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요. 소녀의 아버지는 홀로 남은 소녀가 걱정되었어요. 그래서 얼마 후 새어머니를 맞이했어요. 새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요. 그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요. 새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요. 그런데 이번에는 아버지마저 돌아가셨어요. 소녀는 하녀처럼 하루 종일 쓸고 닦고 집안일을 도맡아 했어요. 해도 해도 끝이 없는 집안일이 힘들어 지칠때면 난롯가에 앉아서 잠시 쉬곤 했지요. 엄마 저애를 신데렐라라고 불러야겠어요. 온통 재투성이잖아요. 호호호! 두 언니는 소녀를 놀려 댔어요. 어느 날 왕궁에서 무도회가 열렸어요. 신데렐라의 집에도 초대장이 왔어요. 새어머니는 언니들을 데리고 무도회장으로 떠났어요. 신데렐라도 무도회에 가고 싶었어요. 혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요. 신데렐라 너도 무도회에 가고 싶니? 신데렐라가 고개를 들어보니 마법사 할머니가 빙그레 웃고 있었어요. 내가 너를 무도회에 보내주마 호박 한개와 생쥐 두마리 도마뱀을 구해 오렴. 마법사 할머니가 주문을 외웠어요. 그리고 지팡이로 호박을 건드리자 호박이 화려한 황금 마차로 변했어요. 이번에는 생쥐와 도마뱀을 건드렸어요. 그랬더니 생쥐는 흰말로 도마뱀은 멋진 마부로 변했답니다. 신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요. 신데렐라 발을 내밀어 보거라. 할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요. 신데렐라 밤 열두시가 되면 모든게 처음대로 돌아간단다. 황금 마차는 호박으로 흰말은 생쥐로 마부는 도마뱀으로 변하게 돼. 그러니까 반드시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지? 왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요. 왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고 신데렐라하고만 춤을 추었어요. 신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요. 땡 땡 땡 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요. 신데렐라가 허둥지둥 왕궁을 빠져나가는데 유리 구두 한 짝이 벗겨졌어요. 하지만 구두를 주울 틈이 없었어요. 신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요. 왕자님은 유리 구두를 가지고 임금님께 가서 말했어요. 이 유리 구두의 주인과 결혼하겠어요. 그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요. 언니들은 발을 오므려도 보고 구두를 늘려도 보았지만 한눈에 보기에도 유리 구두는 너무 작았어요. 그때 신데렐라가 조용히 다가와 말했어요. 저도 한번 신어 볼 수 있나요? 신데렐라는 신하게 건넨 유리 구두를 신었어요 유리 구두는 신데렐라의 발에 꼭 맞았어요. 신하들은 신데렐라를 왕궁으로 데리고 갔어요. 그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-k66tHNeHx6"
      },
      "source": [
        "## 한국어 문법 구분 discriminator 학습\n",
        "\n",
        "* '한글 위키백과'에서 임의의 문장을 수집 함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "oi6AfKSzeHx7"
      },
      "source": [
        "#한국어 위키백과에서 스크랩핑\n",
        "\n",
        "import wikipedia as wiki\n",
        "wiki.set_lang('ko')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ekFwbQVxeHx7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eecd24eb-63dc-4753-d542-58c3642a9107"
      },
      "source": [
        "# '전래동화' 라는 keyword로 100개 page의 Text를 취득\n",
        "\n",
        "def __search_from_wiki(question,max_rank):\n",
        "    results = wiki.search(question,results=max_rank)\n",
        "    print(results)\n",
        "    contents = []\n",
        "    for result in results:\n",
        "        try:\n",
        "            page = wiki.page(result)\n",
        "            #print(f\"Top wiki result: {page}\")\n",
        "            text = page.content\n",
        "            ln = len(text)\n",
        "            #print(f'Collecting page : {page} , text length {str(ln)}')\n",
        "            #if ln < 4000:\n",
        "            #  contents.append(text)\n",
        "            #else:\n",
        "            #  contents.append(text[0:4000])\n",
        "            contents.append(text)\n",
        "        except Exception as ex:\n",
        "            print(ex)\n",
        "    return contents\n",
        "\n",
        "if DO_ALL:\n",
        "    ko_grammar_set_raw = __search_from_wiki(\"동화\",300)\n",
        "    #ko_grammar_set_raw = __search_from_wiki(\"소설\",300)\n",
        "    #ko_grammar_set_raw = __search_from_wiki(\"사설\",300)\n",
        "    #ko_grammar_set_raw = __search_from_wiki(\"설명\",300)\n",
        "    print(f'전체 수집한 Page Count : {len(ko_grammar_set_raw)}')\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['동화', '니코니코 동화', '동화중학교 (경기)', '문화 동화', '조곡 니코니코 동화', '동화사', '신에이 동화', '동화작용', '번암초등학교 동화분교', '책상 서랍 속의 동화', '동화초등학교 (경기)', '동화약품', '동화기업', '동화면세점', '동화역', '동화고등학교', '동화초등학교 (전남)', '화성동화중학교', '그림 동화', '동화초등학교 (강원)', '단백동화 스테로이드', '가을동화', '동화 (언어학)', '애니메이션', '동화초등학교 (충북)', '수성알파시티 동화아이위시', '동화도서관', '훈1등 욱일동화대수장', '동화초등학교', '도에이 애니메이션', '동화중학교 (강원)', '동화은행', '대전동화초등학교', '전북동화중학교', 'TV로 보는 원작동화', '동윤', '신 전래동화', '대전동화중학교', '동화초등학교 (제주)', '한스 크리스티안 안데르센', '도라에몽 (1973년 애니메이션)', '대원씨아이', '빌헬름 하우프', '동화면', '그림 형제', '동화 읽어주는 TV', '빛가람장성로', '아동 문학', 'TV동화 행복한 세상', '권설 설측 접근음', '촉한', '정이서', '동화운수 (인천)', '질소 고정', '대구막항', '동문항', '대구 동화사 비로암 삼층석탑', '전한', '대한민국의 국회의원 선거구 목록', '유글레나류', '리하르트 빌슈테터', '코파반장의 동화수사대', '쥐', '팔공산', '리루프릿', '스튜디오 지브리', '1844년', '동화 (후한)', '동화처럼', '동화운수 (광주)', '대구 동화사 당간지주', '주엽어린이도서관', '아돌프 폰 바이어', '必UP되다', '신-맨', '산서초등학교 (전북)', '대구 동화사 마애여래좌상', '샤를 페로', '등장인물', '동화 (화폐)', '오엘엠', '삼성물산', '란티스 조곡 feat.Nico Nico Artists', '스튜디오 딘', '루트비히 티크', '일본동화협회', '한국의 사찰', '곰돌이 푸', '김지우 (1983년)', '안녕 자두야', '사랑의 시나리오', '동화나라 꿈동산', '쇼 바이 락!!', '도라에몽 그리기', '순천 동화사 삼층석탑', '불도그', '도가코보', '귀스타브 도레', '주화', '장성소방서', '보그 (스타 트렉)', '은비까비의 옛날옛적에', '호리구치 유키코', '호시조라 미유키', '동화 (영화)', '백설 공주', '야코프 그림', '도라에몽: 스탠바이미', '황민화 정책', '짱구는 못말려 극장판: 태풍을 부르는 장엄한 전설의 전투', 'Oneiric Diary (幻想日記)', '정범균', '한다시', '불꽃놀이 (1997년 드라마)', '짱구는 못말려 극장판: 태풍을 부르는 노래하는 엉덩이 폭탄!', '잠자는 숲속의 미녀', '조류 (수생 생물)', '방정환', '일본의 애니메이션', '서울동화축제', '삽화가', '장 드 라 퐁텐', '권정생', '아이 러브 이태리', '프린세스 츄츄', '옛날 옛적에 (애니메이션)', '지방도 제743호선', '슈렉', 'TVB 코리아', '책', '클레오', '스튜어트 리틀', '영심이', '남세균', '계몽사', '머털도사', '잠자는 숲속의 미녀 (1959년 영화)', '판타지 문학', '피노키오', '짱구는 못말려 극장판: 어른 제국의 역습', '오로라 (디즈니)', '라푼첼', '천사의 분노', '마야', '극장판 도라에몽: 진구의 달 탐사기', '와우중학교', '황록조류', '짱구는 못말려 극장판: 폭풍을 부르는 정글', '이산 (태국)', '그림명작극장', '대구 동화사 대웅전', '신데렐라 (디즈니)', '블랙★록 슈터', '동우에이앤이', '리미티드 애니메이션', '벨 (디즈니)', '대구 동화사 부도군', '대구 동화사 비로암 석조비로자나불좌상', '이세진 (희극인)', '바다의 전설 장보고', '구성주의 (교육)', '허구 국가', '마해송', '대구 동화사 보조국사 지눌 진영', '지방도 제312호선', '예른 안데르센', '유대인', '서울동화프로덕션', '정상희', '빌리빌리', '인어 공주 (1989년 영화)', '봉담고등학교', '오즈의 마법사 (1939년 영화)', '대구 동화사 극락전', 'KBS 키즈', '우뢰매', '한국일보', '헨젤과 그레텔', '조안 (배우)', '마크 트웨인', '검정고무신', '루이스 세풀베다', '복현규', '동화 만화 캘린더', '에어맨을 쓰러뜨릴 수 없어', '빨간 자전거', '유루캠Δ', '슈퍼 시로', '내 여동생이 이렇게 귀여울 리가 없어의 에피소드 목록', '출판사', '서울시립 어린이도서관', '간현역', '아이누', '신춘문예', '극장판 도라에몽 진구의 아프리카 모험: 베코와 5인의 탐험대', '라이먼 프랭크 바움', '아스트리드 린드그렌', '꾸러기 수비대', '아기공룡 둘리', '원주민', '떠돌이 까치', 'P.A. 웍스', '이화작용', '청자 동화연화문 표주박모양 주전자', '니코니코 생방송', '이언 플레밍', '일본인', '슈퍼 태권 V', '산서중학교', '대구 동화사 염불암 청석탑', '효행로', '물망초 (드라마)', '채은정', '겨울연가', '비의', '아바르', '에리얼 (디즈니)', '허브 (영화)', 'DR 무비', '오세암', '으랏차차 짠돌이네', 'MBC 베스트셀러극장', '제비', '인어', 'ATC 코드 A14', '사이비 종교', 'SUPER SHOW 2', '공주와 개구리', '욱일장', '동게르만족', '눈의 여왕', '음운 규칙', 'Love in me', '강제동화', '이광모', '백설 공주와 일곱 난쟁이 (영화)', '이카보드와 토드경의 모험', '토키와 타카코', '나카시마 테츠야', '이동윤', '대구 동화사 아미타회상도', '지방도 제616호선', '프랜시스 호지슨 버넷', '대사경로', '순수의 시대 (드라마)', '짱구는 못말려 극장판: 폭풍을 부르는 석양의 떡잎마을 방범대', '꼬비꼬비', 'ㅥ', '독고탁 2 - 내 이름은 독고탁', '이솝 우화', '동화 (동음이의)', '대구 동화사 염불암 마애여래좌상 및 보살좌상', '이종혁 (성우)', '봉담읍', '남자친구 (드라마)', '신춘호', '일선동조론', '핑두시', '신데렐라 (1950년 영화)', '나두야 간다', '류칭윈', '메틸말로닐-CoA', '동요', '총림', '대구 동화사 사명당 유정 진영', '이원수 (작가)', '비밀정원', '걸리버 여행기', 'Into The New World (콘서트)', '셉티미우스 세베루스', '대공원역 (과천)', '여름향기', '바이윈구 (광저우시)', '위세 성', '그린세이버', '흑마녀 나가신다!!', '자전거 도둑 (박완서)', '짱구는 못말려 극장판: 태풍을 부르는 영광의 불고기 로드', '파파야 (음악 그룹)', '물질대사', '1805년', '대구 동화사 삼장보살도', '니코니코 대백과', '흙꼭두장군', '최강희 (배우)', '송정천 (울산)', '의왕백운호수축제', '멜랑슈', '문학', '귄터 폰 클루게', '하종오', '산서고등학교', '빨간 두건', '녹색조류', '사토 준이치', '대구 동화사 금당암 동·서 삼층석탑', '1875년', '1835년', '쇼와 국가주의', '베이후구']\n",
            "\"동화초등학교\" may refer to: \n",
            "동화초등학교\n",
            "동화초등학교\n",
            "동화초등학교\n",
            "동화초등학교\n",
            "동화초등학교\n",
            "대전동화초등학교\n",
            "\"마야\" may refer to: \n",
            "마야 문명\n",
            "마야 (종교)\n",
            "마야 달력\n",
            "마야 숫자\n",
            "마야 문자\n",
            "마야 역\n",
            "마야 (소프트웨어)\n",
            "MAYA\n",
            "마야 (가수)\n",
            "마야\n",
            "요시다 마야\n",
            "유네스키 마야\n",
            "마야 미키\n",
            "마야 산사\n",
            "마야 루돌프\n",
            "마야 앤절로\n",
            "코이즈미 마야\n",
            "마야 로렌스\n",
            "마야 카린\n",
            "마야 스토얀\n",
            "마야 아바바자니\n",
            "마야 보호시에비치\n",
            "마야 카잔\n",
            "마야 알데린\n",
            "마야 쿨리예바\n",
            "모리 마야\n",
            "마야 오스타셰프스카\n",
            "마야 히르슈\n",
            "마야 다간\n",
            "마야 디아브\n",
            "마야 나세르\n",
            "에드워드 마야\n",
            "마야 야게르\n",
            "마야 눌키츠\n",
            "내 이름은 마야\n",
            "꿀벌 마야\n",
            "마야 (1966년 영화)\n",
            "마야 (2014년 영화)\n",
            "마야 아일랜드 항공\n",
            "\"동화 (동음이의)\" may refer to: \n",
            "동화\n",
            "동화\n",
            "동화\n",
            "동화\n",
            "동화\n",
            "동화\n",
            "동화\n",
            "동화\n",
            "동화 (가수)\n",
            "전체 수집한 Page Count : 297\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Tu_6jZAmeHx9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22c296b5-85de-4476-96bf-e046c9628553"
      },
      "source": [
        "# 간단한 전처리\n",
        "\n",
        "ko_grammar_set_raw = [clean_text(x) for x in ko_grammar_set_raw]\n",
        "print('Sample text : ')\n",
        "print('--------------------------------------------------------------------------------------------')\n",
        "print(ko_grammar_set_raw[296])\n",
        "print('--------------------------------------------------------------------------------------------')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample text : \n",
            "--------------------------------------------------------------------------------------------\n",
            "베이후 구(한국어: 북호구 중국어: 北湖区 병음: Běihú Qū)는 중화 인민 공화국 후난성 천저우 시의 현급 행정구역이다. 넓이는 815km2이고 인구는 2007년 기준으로 330000명이다.   행정 구역 4개 가도 5개 진 7개 향 2개 민족향을 관할한다.  가도: 人民路街道 北湖街道 燕泉街道 下湄桥街道. 진: 石盖塘镇 华塘镇 鲁塘镇 郴江镇 万华岩镇. 향: 江口乡 市郊乡 同和乡 保和乡 芙蓉乡 永春乡 南溪乡. 민족향: 大塘瑶族乡 月峰瑶族乡. \n",
            "--------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjF0LSmMsH96"
      },
      "source": [
        "# 긴 소설하나 줍줍\r\n",
        "\r\n",
        "filepath = 'korean_sample.txt'\r\n",
        "korean_sample = \"\"\r\n",
        "file1 = open(filepath, 'r') \r\n",
        "Lines = file1.readlines()\r\n",
        "for line in Lines:\r\n",
        "    line = line.strip()\r\n",
        "    if line.startswith(('\\\"','\\'',' ')) or line.endswith(('\\\"','\\'')) or line=='':\r\n",
        "        pass\r\n",
        "    else:\r\n",
        "        if len(line) > 10:\r\n",
        "            #print(line)\r\n",
        "            korean_sample += line\r\n",
        "\r\n",
        "korean_sample = clean_text(korean_sample)\r\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biGCujc4xhS8",
        "outputId": "af07d58a-34f3-4758-8ef4-acd15bd1ae95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ko_grammar_set_raw += [korean_sample]\r\n",
        "print(f'전체 수집한 Page Count : {len(ko_grammar_set_raw)}')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "전체 수집한 Page Count : 298\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqJHLPaReHx-"
      },
      "source": [
        "문장으로 잘라 낸다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "_b8WKqYXeHx_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cf34de9-08b4-4a26-ac43-96e2949c457a"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "#Split the document into sentences\n",
        "ko_grammar_sentences = []\n",
        "for document in ko_grammar_set_raw:\n",
        "    ko_grammar_sentences += nltk.sent_tokenize(document)\n",
        "\n",
        "print(\"Num sentences:\", len(ko_grammar_sentences))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Num sentences: 32705\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEsoSpdFwRVt",
        "outputId": "a2c480d1-b37e-47c8-d542-5b88d21d6f61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "ko_grammar_sentences[30010]"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'공간이동을 한두 번 겪은 것도 아니었지만 틸라크에서 헤모시아를 거쳐 다시 아라사로 이동하는 수천 파르상이나 되는 장거리 공간이동은 처음이었기에 현기증을 느끼다못해 구토까지 일 것 같았다.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doo-8nFPThrQ"
      },
      "source": [
        "# false 문장을 만들기 위해 shffle 함수 준비\r\n",
        "import random\r\n",
        "\r\n",
        "def shuffling(txt):\r\n",
        "    txt_list = txt.split(' ')\r\n",
        "    random.shuffle(txt_list)\r\n",
        "    return ' '.join(txt_list)\r\n",
        "\r\n",
        "# true 문장, false 문장의 생성\r\n",
        "ko_grammar_dataset = []\r\n",
        "for txt in ko_grammar_sentences:\r\n",
        "    ko_grammar_dataset.append([txt,1])\r\n",
        "    ko_grammar_dataset.append([shuffling(txt),0])\r\n",
        "    \r\n",
        "# dataset을 전체적으로 다시 썩는다.\r\n",
        "random.shuffle(ko_grammar_dataset)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdj8wrm8eHyC"
      },
      "source": [
        "형태소 분리하여 모든 문장을 형태소 Code로 변환 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "OY8VTXraeHyF"
      },
      "source": [
        "from konlpy.tag import Twitter\n",
        "twitter = Twitter()\n",
        "\n",
        "# 형태소 Code table의 구성\n",
        "\n",
        "_MAX_MORP_LENGTH = 128\n",
        "_PADDING_CODE = 0  # padding code\n",
        "_MISMATCH_CODE = 1 # mismatch word code               ex) @@@\n",
        "_MISMATCH_WORD = '@@@' # 이거 아래에서 쓴다.\n",
        "\n",
        "morpheme_table = {}\n",
        "morp_code = _MISMATCH_CODE+1\n",
        "morpheme_table['Pad'] = _PADDING_CODE \n",
        "morpheme_table['Mst'] = _MISMATCH_CODE \n",
        "for sentence in ko_grammar_sentences[:7000]:\n",
        "    morphemes = twitter.pos(sentence)\n",
        "    for (word,morp) in morphemes:\n",
        "        if morp in morpheme_table:\n",
        "            pass\n",
        "        else:\n",
        "            morpheme_table[morp] = morp_code\n",
        "            morp_code += 1\n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0IzjdedJf09"
      },
      "source": [
        "morpheme_table['Hashtag']=20 # 빠진거 채워넣음...\r\n",
        "morpheme_table['URL']=21 # 빠진거 채워넣음..."
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "asiTeu8SeHyG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b4362e7-6d45-44d1-a634-799932547014"
      },
      "source": [
        "print('Korean morpheme code table')\n",
        "print('----------------------------------------------------------')\n",
        "print('  Morpheme        Code')\n",
        "print('')\n",
        "for morp in morpheme_table.keys():\n",
        "    print(f' {morp.ljust(15)}   {morpheme_table[morp]}')\n",
        "print('----------------------------------------------------------')\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Korean morpheme code table\n",
            "----------------------------------------------------------\n",
            "  Morpheme        Code\n",
            "\n",
            " Pad               0\n",
            " Mst               1\n",
            " Noun              2\n",
            " Punctuation       3\n",
            " Foreign           4\n",
            " Josa              5\n",
            " Verb              6\n",
            " Modifier          7\n",
            " Adjective         8\n",
            " Suffix            9\n",
            " Adverb            10\n",
            " Number            11\n",
            " Alpha             12\n",
            " Determiner        13\n",
            " Conjunction       14\n",
            " Exclamation       15\n",
            " KoreanParticle    16\n",
            " VerbPrefix        17\n",
            " Eomi              18\n",
            " PreEomi           19\n",
            " Hashtag           20\n",
            " URL               21\n",
            "----------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "RQfjoYbceHyH"
      },
      "source": [
        "# morpheme 코드 변환기 준비\n",
        "def morpheme_encode(sentence):\n",
        "    encode=[]\n",
        "    morphemes = twitter.pos(sentence)\n",
        "    for (word,morp) in morphemes:\n",
        "        encode.append(_MISMATCH_CODE if word==_MISMATCH_WORD else morpheme_table[morp])\n",
        "\n",
        "    if len(encode) <= _MAX_MORP_LENGTH:\n",
        "        encode = encode + [_PADDING_CODE for i in range(_MAX_MORP_LENGTH-len(encode))]\n",
        "    else:\n",
        "        encode = encode[:_MAX_MORP_LENGTH]       \n",
        "    return encode\n",
        "\n",
        "#true / false 문장을 morpheme 코드로 모두 변환\n",
        "ko_morpheme_x = []\n",
        "ko_morpheme_y = []\n",
        "for (txt,label) in ko_grammar_dataset:\n",
        "    ko_morpheme_x.append(morpheme_encode(txt))\n",
        "    ko_morpheme_y.append([label])\n",
        "\n",
        "ko_morpheme_x = np.asarray(ko_morpheme_x)\n",
        "ko_morpheme_y = np.asarray(ko_morpheme_y)  "
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Knw_P_zswIXC"
      },
      "source": [
        "trainset 과 testset의 분리 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5vbAwY4Znc2",
        "outputId": "1bd93bdb-33b0-4bde-efb9-089b06bb3d31"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "# 20%를 testset으로 사용.,,\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(ko_morpheme_x,ko_morpheme_y,test_size=0.2)\r\n",
        "\r\n",
        "print(f'Shape of X_train;{X_train.shape}')\r\n",
        "print(f'Shape of X_test ;{X_test.shape}')\r\n",
        "print(f'Shape of y_train;{y_train.shape}')\r\n",
        "print(f'Shape of y_test ;{y_test.shape}')"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of X_train;(52328, 128)\n",
            "Shape of X_test ;(13082, 128)\n",
            "Shape of y_train;(52328, 1)\n",
            "Shape of y_test ;(13082, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Es6toINDwSQa"
      },
      "source": [
        "Model을 생성하고 학습 시킨다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxG8Wtf1cShr",
        "outputId": "c4517c6c-2433-4268-fb54-4665c65e4748"
      },
      "source": [
        "# model build\r\n",
        "\r\n",
        "morpheme_model = Sequential()\r\n",
        "morpheme_model.add(Dense(500, input_dim=128, activation= \"relu\"))\r\n",
        "morpheme_model.add(Dense(1000, activation= \"relu\"))\r\n",
        "morpheme_model.add(Dense(100, activation= \"relu\"))\r\n",
        "morpheme_model.add(Dense(50, activation= \"relu\"))\r\n",
        "morpheme_model.add(Dense(1, activation= \"sigmoid\"))\r\n",
        "morpheme_model.summary() #Print model Summary\r\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_5 (Dense)              (None, 500)               64500     \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1000)              501000    \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 100)               100100    \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 50)                5050      \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 670,701\n",
            "Trainable params: 670,701\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOA1zDvoK2HE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b317c900-930d-45d5-a5ba-295d279032d5"
      },
      "source": [
        "morpheme_model.compile(loss= \"mean_squared_error\" , optimizer=\"adam\", metrics=[\"mean_squared_error\"])\r\n",
        "morpheme_model.fit(X_train, y_train, epochs=100)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1636/1636 [==============================] - 6s 2ms/step - loss: 0.2487 - mean_squared_error: 0.2487\n",
            "Epoch 2/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.2111 - mean_squared_error: 0.2111\n",
            "Epoch 3/100\n",
            "1636/1636 [==============================] - 3s 2ms/step - loss: 0.1741 - mean_squared_error: 0.1741\n",
            "Epoch 4/100\n",
            "1636/1636 [==============================] - 3s 2ms/step - loss: 0.1593 - mean_squared_error: 0.1593\n",
            "Epoch 5/100\n",
            "1636/1636 [==============================] - 3s 2ms/step - loss: 0.1482 - mean_squared_error: 0.1482\n",
            "Epoch 6/100\n",
            "1636/1636 [==============================] - 3s 2ms/step - loss: 0.1420 - mean_squared_error: 0.1420\n",
            "Epoch 7/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.1371 - mean_squared_error: 0.1371\n",
            "Epoch 8/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.1293 - mean_squared_error: 0.1293\n",
            "Epoch 9/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.1262 - mean_squared_error: 0.1262\n",
            "Epoch 10/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.1198 - mean_squared_error: 0.1198\n",
            "Epoch 11/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.1164 - mean_squared_error: 0.1164\n",
            "Epoch 12/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.1128 - mean_squared_error: 0.1128\n",
            "Epoch 13/100\n",
            "1636/1636 [==============================] - 3s 2ms/step - loss: 0.1083 - mean_squared_error: 0.1083\n",
            "Epoch 14/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.1021 - mean_squared_error: 0.1021\n",
            "Epoch 15/100\n",
            "1636/1636 [==============================] - 3s 2ms/step - loss: 0.0993 - mean_squared_error: 0.0993\n",
            "Epoch 16/100\n",
            "1636/1636 [==============================] - 3s 2ms/step - loss: 0.0969 - mean_squared_error: 0.0969\n",
            "Epoch 17/100\n",
            "1636/1636 [==============================] - 3s 2ms/step - loss: 0.0906 - mean_squared_error: 0.0906\n",
            "Epoch 18/100\n",
            "1636/1636 [==============================] - 3s 2ms/step - loss: 0.0879 - mean_squared_error: 0.0879\n",
            "Epoch 19/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0835 - mean_squared_error: 0.0835\n",
            "Epoch 20/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0816 - mean_squared_error: 0.0816\n",
            "Epoch 21/100\n",
            "1636/1636 [==============================] - 3s 2ms/step - loss: 0.0769 - mean_squared_error: 0.0769\n",
            "Epoch 22/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0746 - mean_squared_error: 0.0746\n",
            "Epoch 23/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0727 - mean_squared_error: 0.0727\n",
            "Epoch 24/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0712 - mean_squared_error: 0.0712\n",
            "Epoch 25/100\n",
            "1636/1636 [==============================] - 3s 2ms/step - loss: 0.0685 - mean_squared_error: 0.0685\n",
            "Epoch 26/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0699 - mean_squared_error: 0.0699\n",
            "Epoch 27/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0683 - mean_squared_error: 0.0683\n",
            "Epoch 28/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0641 - mean_squared_error: 0.0641\n",
            "Epoch 29/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0627 - mean_squared_error: 0.0627\n",
            "Epoch 30/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0618 - mean_squared_error: 0.0618\n",
            "Epoch 31/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0604 - mean_squared_error: 0.0604\n",
            "Epoch 32/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0591 - mean_squared_error: 0.0591\n",
            "Epoch 33/100\n",
            "1636/1636 [==============================] - 3s 2ms/step - loss: 0.0580 - mean_squared_error: 0.0580\n",
            "Epoch 34/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0555 - mean_squared_error: 0.0555\n",
            "Epoch 35/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0554 - mean_squared_error: 0.0554\n",
            "Epoch 36/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0555 - mean_squared_error: 0.0555\n",
            "Epoch 37/100\n",
            "1636/1636 [==============================] - 3s 2ms/step - loss: 0.0538 - mean_squared_error: 0.0538\n",
            "Epoch 38/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0551 - mean_squared_error: 0.0551\n",
            "Epoch 39/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0525 - mean_squared_error: 0.0525\n",
            "Epoch 40/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0516 - mean_squared_error: 0.0516\n",
            "Epoch 41/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0525 - mean_squared_error: 0.0525\n",
            "Epoch 42/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0495 - mean_squared_error: 0.0495\n",
            "Epoch 43/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0494 - mean_squared_error: 0.0494\n",
            "Epoch 44/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0512 - mean_squared_error: 0.0512\n",
            "Epoch 45/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0496 - mean_squared_error: 0.0496\n",
            "Epoch 46/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0479 - mean_squared_error: 0.0479\n",
            "Epoch 47/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0461 - mean_squared_error: 0.0461\n",
            "Epoch 48/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0488 - mean_squared_error: 0.0488\n",
            "Epoch 49/100\n",
            "1636/1636 [==============================] - 3s 2ms/step - loss: 0.0462 - mean_squared_error: 0.0462\n",
            "Epoch 50/100\n",
            "1636/1636 [==============================] - 3s 2ms/step - loss: 0.0470 - mean_squared_error: 0.0470\n",
            "Epoch 51/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0440 - mean_squared_error: 0.0440\n",
            "Epoch 52/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0446 - mean_squared_error: 0.0446\n",
            "Epoch 53/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0445 - mean_squared_error: 0.0445\n",
            "Epoch 54/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0441 - mean_squared_error: 0.0441\n",
            "Epoch 55/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0429 - mean_squared_error: 0.0429\n",
            "Epoch 56/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0443 - mean_squared_error: 0.0443\n",
            "Epoch 57/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0416 - mean_squared_error: 0.0416\n",
            "Epoch 58/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0428 - mean_squared_error: 0.0428\n",
            "Epoch 59/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0418 - mean_squared_error: 0.0418\n",
            "Epoch 60/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0423 - mean_squared_error: 0.0423\n",
            "Epoch 61/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0432 - mean_squared_error: 0.0432\n",
            "Epoch 62/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0407 - mean_squared_error: 0.0407\n",
            "Epoch 63/100\n",
            "1636/1636 [==============================] - 3s 2ms/step - loss: 0.0419 - mean_squared_error: 0.0419\n",
            "Epoch 64/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0410 - mean_squared_error: 0.0410\n",
            "Epoch 65/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0412 - mean_squared_error: 0.0412\n",
            "Epoch 66/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0386 - mean_squared_error: 0.0386\n",
            "Epoch 67/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0396 - mean_squared_error: 0.0396\n",
            "Epoch 68/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0411 - mean_squared_error: 0.0411\n",
            "Epoch 69/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0391 - mean_squared_error: 0.0391\n",
            "Epoch 70/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0395 - mean_squared_error: 0.0395\n",
            "Epoch 71/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0408 - mean_squared_error: 0.0408\n",
            "Epoch 72/100\n",
            "1636/1636 [==============================] - 3s 2ms/step - loss: 0.0401 - mean_squared_error: 0.0401\n",
            "Epoch 73/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0377 - mean_squared_error: 0.0377\n",
            "Epoch 74/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0400 - mean_squared_error: 0.0400\n",
            "Epoch 75/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0388 - mean_squared_error: 0.0388\n",
            "Epoch 76/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0371 - mean_squared_error: 0.0371\n",
            "Epoch 77/100\n",
            "1636/1636 [==============================] - 3s 2ms/step - loss: 0.0362 - mean_squared_error: 0.0362\n",
            "Epoch 78/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0375 - mean_squared_error: 0.0375\n",
            "Epoch 79/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0367 - mean_squared_error: 0.0367\n",
            "Epoch 80/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0386 - mean_squared_error: 0.0386\n",
            "Epoch 81/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0366 - mean_squared_error: 0.0366\n",
            "Epoch 82/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0356 - mean_squared_error: 0.0356\n",
            "Epoch 83/100\n",
            "1636/1636 [==============================] - 3s 2ms/step - loss: 0.0366 - mean_squared_error: 0.0366\n",
            "Epoch 84/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0349 - mean_squared_error: 0.0349\n",
            "Epoch 85/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0345 - mean_squared_error: 0.0345\n",
            "Epoch 86/100\n",
            "1636/1636 [==============================] - 3s 2ms/step - loss: 0.0365 - mean_squared_error: 0.0365\n",
            "Epoch 87/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0349 - mean_squared_error: 0.0349\n",
            "Epoch 88/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0349 - mean_squared_error: 0.0349\n",
            "Epoch 89/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0369 - mean_squared_error: 0.0369\n",
            "Epoch 90/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0330 - mean_squared_error: 0.0330\n",
            "Epoch 91/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0359 - mean_squared_error: 0.0359\n",
            "Epoch 92/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0349 - mean_squared_error: 0.0349\n",
            "Epoch 93/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0330 - mean_squared_error: 0.0330\n",
            "Epoch 94/100\n",
            "1636/1636 [==============================] - 3s 2ms/step - loss: 0.0341 - mean_squared_error: 0.0341\n",
            "Epoch 95/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0350 - mean_squared_error: 0.0350\n",
            "Epoch 96/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0318 - mean_squared_error: 0.0318\n",
            "Epoch 97/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0379 - mean_squared_error: 0.0379\n",
            "Epoch 98/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0346 - mean_squared_error: 0.0346\n",
            "Epoch 99/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0325 - mean_squared_error: 0.0325\n",
            "Epoch 100/100\n",
            "1636/1636 [==============================] - 4s 2ms/step - loss: 0.0343 - mean_squared_error: 0.0343\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa48b042fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvrVAccWpwAm",
        "outputId": "f4280869-d672-4b5c-d3fc-6d549c9cd57e"
      },
      "source": [
        "# 학습결과 확인\r\n",
        "\r\n",
        "results = morpheme_model.evaluate(X_test, y_test)\r\n",
        "print(results)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "409/409 [==============================] - 1s 2ms/step - loss: 0.1686 - mean_squared_error: 0.1686\n",
            "[0.1685619205236435, 0.1685619205236435]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5i2wtQvsWyY",
        "outputId": "e28b84e5-9103-4265-ff83-d264ebacf6d8"
      },
      "source": [
        "# 학습 결과의 확인\r\n",
        "predicts = morpheme_model.predict(X_test)\r\n",
        "predicts = np.asarray(predicts)\r\n",
        "predicts = [ 1 if x>0.5 else 0 for [x] in predicts]\r\n",
        "y = np.asarray(y_test)\r\n",
        "y = y.reshape(y.shape[0],)\r\n",
        "\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "print(classification_report(y, predicts))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.75      0.79      6556\n",
            "           1       0.77      0.85      0.81      6526\n",
            "\n",
            "    accuracy                           0.80     13082\n",
            "   macro avg       0.81      0.80      0.80     13082\n",
            "weighted avg       0.81      0.80      0.80     13082\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdnf-b-ywwup"
      },
      "source": [
        "# 모델의 저장\r\n",
        "\r\n",
        "morpheme_model.save('morpheme_model.h5')\r\n",
        "morpheme_model.save_weights('morpheme_model.weights')"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bkJ7Abs2XG3"
      },
      "source": [
        "def morpheme_discriminator(queries):\r\n",
        "    # queries : 복수의 문장의 2차원 배열 (None,1)\r\n",
        "    # return : 결과 score 배열 (None,)\r\n",
        "    x_codes = []\r\n",
        "    for query in queries:\r\n",
        "        x_codes.append(morpheme_encode(query))\r\n",
        "    scores = morpheme_model.predict(x_codes)\r\n",
        "    return scores"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgjWTMGt3SC6",
        "outputId": "13c8dfb5-3896-4fe6-9b83-6fac7da8f631",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ko_grammar_sentences[100:102]"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['바로 옆에 동화고등학교가 있다.', '건물은 사랑관 소망관 믿음관 송학관 여호수아홀이 있다.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KU7aOemq3WqY",
        "outputId": "5b63b895-2f88-4f87-9dab-7ac324d9f447",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "morpheme_discriminator(ko_grammar_sentences[100:102])"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.9388542 ],\n",
              "       [0.97550076]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZF3lwa70lB0"
      },
      "source": [
        "## 문서 유사도 Discriminator 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKNnwvwx0kLN",
        "outputId": "b5c8088e-27c2-40ff-9e85-520f7e190111",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\r\n",
        "from transformers import BertTokenizer\r\n",
        "\r\n",
        "if DO_ALL:\r\n",
        "    # embedder download...\r\n",
        "    embedder = SentenceTransformer('xlm-r-large-en-ko-nli-ststb')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1.80G/1.80G [01:14<00:00, 24.3MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiICVLIm1SZv"
      },
      "source": [
        "\r\n",
        "# 생성된 문장의 원문 유사도를 측정하기 위한 함수\r\n",
        "\r\n",
        "import scipy\r\n",
        "#print(doc_emb)\r\n",
        "def similarity_discriminator(queries,org_embedding):\r\n",
        "    # queries : 복수의 문장의 2차원 배열 (None,1)\r\n",
        "    # org_embedding : 비교 대상의 원문 embedding 1차원 배열 (1,)\r\n",
        "    # return : 결과 score 배열 (None,)\r\n",
        "    total_score = 0\r\n",
        "    query_embeddings = embedder.encode(queries,show_progress_bar=False)\r\n",
        "    for query, query_embedding in zip(queries, query_embeddings):\r\n",
        "        distances = scipy.spatial.distance.cdist([query_embedding], [org_embedding], \"cosine\")[0]\r\n",
        "        results = zip(range(len(distances)), distances)\r\n",
        "        for idx, distance in results:\r\n",
        "            total_score += 1-distance\r\n",
        "    return total_score"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrAYyyNP4HXf"
      },
      "source": [
        "# 원문의 embedding...\r\n",
        "org_text_emb = embedder.encode([org_text],show_progress_bar=False)[0]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXrhGn_W4VZF",
        "outputId": "c4ea20a4-a26a-4278-810f-9e4fa00a6dc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "org_text_emb"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.15200967,  0.92120737, -0.1766923 , ..., -0.72806704,\n",
              "        0.742245  , -0.41552904], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gypz5XEH4cUv"
      },
      "source": [
        "## 원문 base의 generator 준비 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p66D6hso4t3N"
      },
      "source": [
        "org_term_set = org_text.split(' ')\r\n",
        "\r\n",
        "_MAX_GEN_TOKEN = 40\r\n",
        "_NOISE_DIM = len(org_term_set)\r\n",
        "\r\n",
        "word_table = {}\r\n",
        "\r\n",
        "for index, word in zip(range(len(org_term_set)),org_term_set):\r\n",
        "    word_table[index] = word"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rv5g266R42Y6",
        "outputId": "cf905718-61b5-40cf-eefe-c88cae07d160",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('Token table of origin text')\r\n",
        "print('---------------------------------------------')\r\n",
        "print(' Code         Token      ')\r\n",
        "print('')\r\n",
        "for k in word_table.keys():\r\n",
        "  print( f'  {str(k).ljust(8)}    {word_table[k]}')\r\n",
        "print('---------------------------------------------')"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token table of origin text\n",
            "---------------------------------------------\n",
            " Code         Token      \n",
            "\n",
            "  0           옛날\n",
            "  1           어느\n",
            "  2           집에\n",
            "  3           귀여운\n",
            "  4           여자\n",
            "  5           아기가\n",
            "  6           태어났어요.\n",
            "  7           아기는\n",
            "  8           무럭무럭\n",
            "  9           자라서\n",
            "  10          예쁘고\n",
            "  11          마음씨\n",
            "  12          고운\n",
            "  13          소녀가\n",
            "  14          되었어요.\n",
            "  15          그러던\n",
            "  16          어느날\n",
            "  17          소녀의\n",
            "  18          어머니가\n",
            "  19          병이들어\n",
            "  20          그만\n",
            "  21          세상을\n",
            "  22          떠나고\n",
            "  23          말았어요.\n",
            "  24          소녀의\n",
            "  25          아버지는\n",
            "  26          홀로\n",
            "  27          남은\n",
            "  28          소녀가\n",
            "  29          걱정되었어요.\n",
            "  30          그래서\n",
            "  31          얼마\n",
            "  32          후\n",
            "  33          새어머니를\n",
            "  34          맞이했어요.\n",
            "  35          새어머니는\n",
            "  36          소녀보다\n",
            "  37          나이가\n",
            "  38          위인\n",
            "  39          두\n",
            "  40          딸을\n",
            "  41          데리고\n",
            "  42          왔어요.\n",
            "  43          그러나\n",
            "  44          새어머니와\n",
            "  45          언니들은\n",
            "  46          성질이\n",
            "  47          고약한\n",
            "  48          심술쟁이들이었어요.\n",
            "  49          새어머니는\n",
            "  50          소녀가\n",
            "  51          자기\n",
            "  52          딸들보다\n",
            "  53          예쁘고\n",
            "  54          착한\n",
            "  55          게\n",
            "  56          못마땅했어요.\n",
            "  57          그런데\n",
            "  58          이번에는\n",
            "  59          아버지마저\n",
            "  60          돌아가셨어요.\n",
            "  61          소녀는\n",
            "  62          하녀처럼\n",
            "  63          하루\n",
            "  64          종일\n",
            "  65          쓸고\n",
            "  66          닦고\n",
            "  67          집안일을\n",
            "  68          도맡아\n",
            "  69          했어요.\n",
            "  70          해도\n",
            "  71          해도\n",
            "  72          끝이\n",
            "  73          없는\n",
            "  74          집안일이\n",
            "  75          힘들어\n",
            "  76          지칠때면\n",
            "  77          난롯가에\n",
            "  78          앉아서\n",
            "  79          잠시\n",
            "  80          쉬곤\n",
            "  81          했지요.\n",
            "  82          엄마\n",
            "  83          저애를\n",
            "  84          신데렐라라고\n",
            "  85          불러야겠어요.\n",
            "  86          온통\n",
            "  87          재투성이잖아요.\n",
            "  88          호호호!\n",
            "  89          두\n",
            "  90          언니는\n",
            "  91          소녀를\n",
            "  92          놀려\n",
            "  93          댔어요.\n",
            "  94          어느\n",
            "  95          날\n",
            "  96          왕궁에서\n",
            "  97          무도회가\n",
            "  98          열렸어요.\n",
            "  99          신데렐라의\n",
            "  100         집에도\n",
            "  101         초대장이\n",
            "  102         왔어요.\n",
            "  103         새어머니는\n",
            "  104         언니들을\n",
            "  105         데리고\n",
            "  106         무도회장으로\n",
            "  107         떠났어요.\n",
            "  108         신데렐라도\n",
            "  109         무도회에\n",
            "  110         가고\n",
            "  111         싶었어요.\n",
            "  112         혼자\n",
            "  113         남은\n",
            "  114         신데렐라는\n",
            "  115         훌쩍훌쩍\n",
            "  116         울기\n",
            "  117         시작했어요.\n",
            "  118         신데렐라\n",
            "  119         너도\n",
            "  120         무도회에\n",
            "  121         가고\n",
            "  122         싶니?\n",
            "  123         신데렐라가\n",
            "  124         고개를\n",
            "  125         들어보니\n",
            "  126         마법사\n",
            "  127         할머니가\n",
            "  128         빙그레\n",
            "  129         웃고\n",
            "  130         있었어요.\n",
            "  131         내가\n",
            "  132         너를\n",
            "  133         무도회에\n",
            "  134         보내주마\n",
            "  135         호박\n",
            "  136         한개와\n",
            "  137         생쥐\n",
            "  138         두마리\n",
            "  139         도마뱀을\n",
            "  140         구해\n",
            "  141         오렴.\n",
            "  142         마법사\n",
            "  143         할머니가\n",
            "  144         주문을\n",
            "  145         외웠어요.\n",
            "  146         그리고\n",
            "  147         지팡이로\n",
            "  148         호박을\n",
            "  149         건드리자\n",
            "  150         호박이\n",
            "  151         화려한\n",
            "  152         황금\n",
            "  153         마차로\n",
            "  154         변했어요.\n",
            "  155         이번에는\n",
            "  156         생쥐와\n",
            "  157         도마뱀을\n",
            "  158         건드렸어요.\n",
            "  159         그랬더니\n",
            "  160         생쥐는\n",
            "  161         흰말로\n",
            "  162         도마뱀은\n",
            "  163         멋진\n",
            "  164         마부로\n",
            "  165         변했답니다.\n",
            "  166         신데렐라의\n",
            "  167         옷도\n",
            "  168         구슬\n",
            "  169         장식이\n",
            "  170         반짝이는\n",
            "  171         예쁜\n",
            "  172         드레스로\n",
            "  173         바뀌웠어요.\n",
            "  174         신데렐라\n",
            "  175         발을\n",
            "  176         내밀어\n",
            "  177         보거라.\n",
            "  178         할머니는\n",
            "  179         신데렐라에게\n",
            "  180         반짝반짝\n",
            "  181         빛나는\n",
            "  182         유리\n",
            "  183         구두를\n",
            "  184         신겨\n",
            "  185         주었어요.\n",
            "  186         신데렐라\n",
            "  187         밤\n",
            "  188         열두시가\n",
            "  189         되면\n",
            "  190         모든게\n",
            "  191         처음대로\n",
            "  192         돌아간단다.\n",
            "  193         황금\n",
            "  194         마차는\n",
            "  195         호박으로\n",
            "  196         흰말은\n",
            "  197         생쥐로\n",
            "  198         마부는\n",
            "  199         도마뱀으로\n",
            "  200         변하게\n",
            "  201         돼.\n",
            "  202         그러니까\n",
            "  203         반드시\n",
            "  204         밤\n",
            "  205         열두\n",
            "  206         시가\n",
            "  207         되기\n",
            "  208         전에\n",
            "  209         돌아와야\n",
            "  210         해.\n",
            "  211         알겠지?\n",
            "  212         왕자님도\n",
            "  213         아름다운\n",
            "  214         신데렐라에게\n",
            "  215         마음을\n",
            "  216         빼았겼어요.\n",
            "  217         왕자님은\n",
            "  218         무도회장에\n",
            "  219         모인\n",
            "  220         다른\n",
            "  221         아가씨들은\n",
            "  222         쳐다보지도\n",
            "  223         않고\n",
            "  224         신데렐라하고만\n",
            "  225         춤을\n",
            "  226         추었어요.\n",
            "  227         신데렐라는\n",
            "  228         왕자님과\n",
            "  229         춤을\n",
            "  230         추느라\n",
            "  231         시간\n",
            "  232         가는\n",
            "  233         줄도\n",
            "  234         몰랐어요.\n",
            "  235         땡\n",
            "  236         땡\n",
            "  237         땡\n",
            "  238         벽시계가\n",
            "  239         열두\n",
            "  240         시를\n",
            "  241         알리는\n",
            "  242         소리에\n",
            "  243         신데렐라는\n",
            "  244         화들짝\n",
            "  245         놀랐어요.\n",
            "  246         신데렐라가\n",
            "  247         허둥지둥\n",
            "  248         왕궁을\n",
            "  249         빠져나가는데\n",
            "  250         유리\n",
            "  251         구두\n",
            "  252         한\n",
            "  253         짝이\n",
            "  254         벗겨졌어요.\n",
            "  255         하지만\n",
            "  256         구두를\n",
            "  257         주울\n",
            "  258         틈이\n",
            "  259         없었어요.\n",
            "  260         신데렐라를\n",
            "  261         뛰쫓아오던\n",
            "  262         왕자님은\n",
            "  263         층계에서\n",
            "  264         유리\n",
            "  265         구두\n",
            "  266         한\n",
            "  267         짝을\n",
            "  268         주웠어요.\n",
            "  269         왕자님은\n",
            "  270         유리\n",
            "  271         구두를\n",
            "  272         가지고\n",
            "  273         임금님께\n",
            "  274         가서\n",
            "  275         말했어요.\n",
            "  276         이\n",
            "  277         유리\n",
            "  278         구두의\n",
            "  279         주인과\n",
            "  280         결혼하겠어요.\n",
            "  281         그래서\n",
            "  282         신하들은\n",
            "  283         유리\n",
            "  284         구두의\n",
            "  285         주인을\n",
            "  286         찾아\n",
            "  287         온\n",
            "  288         나라를\n",
            "  289         돌아다녔어요.\n",
            "  290         언니들은\n",
            "  291         발을\n",
            "  292         오므려도\n",
            "  293         보고\n",
            "  294         구두를\n",
            "  295         늘려도\n",
            "  296         보았지만\n",
            "  297         한눈에\n",
            "  298         보기에도\n",
            "  299         유리\n",
            "  300         구두는\n",
            "  301         너무\n",
            "  302         작았어요.\n",
            "  303         그때\n",
            "  304         신데렐라가\n",
            "  305         조용히\n",
            "  306         다가와\n",
            "  307         말했어요.\n",
            "  308         저도\n",
            "  309         한번\n",
            "  310         신어\n",
            "  311         볼\n",
            "  312         수\n",
            "  313         있나요?\n",
            "  314         신데렐라는\n",
            "  315         신하게\n",
            "  316         건넨\n",
            "  317         유리\n",
            "  318         구두를\n",
            "  319         신었어요\n",
            "  320         유리\n",
            "  321         구두는\n",
            "  322         신데렐라의\n",
            "  323         발에\n",
            "  324         꼭\n",
            "  325         맞았어요.\n",
            "  326         신하들은\n",
            "  327         신데렐라를\n",
            "  328         왕궁으로\n",
            "  329         데리고\n",
            "  330         갔어요.\n",
            "  331         그\n",
            "  332         뒤\n",
            "  333         신데렐라는\n",
            "  334         왕자님과\n",
            "  335         결혼하여\n",
            "  336         오래오래\n",
            "  337         행복하게\n",
            "  338         살았대요.\n",
            "---------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lejMbMq654is"
      },
      "source": [
        "_MAX_GEN_TOKEN = 40\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29sMOSXK5GYR",
        "outputId": "a86c13c9-dcbd-4c87-f7f3-1e8d1f56c26b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tc = 5\r\n",
        "a = [1,2,3,4,5,6,7,8,5,10]\r\n",
        "s_a = np.sort(a)[::-1]\r\n",
        "print(s_a)\r\n",
        "\r\n",
        "o = np.where(a > s_a[tc+1])[0][-tc:]\r\n",
        "print('order:',o)\r\n",
        "print('value:',np.asarray(a)[o])\r\n",
        "\r\n",
        "print(len(o))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[10  8  7  6  5  5  4  3  2  1]\n",
            "order: [5 6 7 8 9]\n",
            "value: [ 6  7  8  5 10]\n",
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E04x981k7Gzh"
      },
      "source": [
        "noise = np.random.rand(_NOISE_DIM,)\r\n",
        "def text_gen(noise):\r\n",
        "    text = \"\"\r\n",
        "    sorted_noise = np.sort(noise)[::-1]\r\n",
        "    order = np.where(noise > sorted_noise[_MAX_GEN_TOKEN+1])[0][-_MAX_GEN_TOKEN:]\r\n",
        "    assert len(order) == _MAX_GEN_TOKEN\r\n",
        "    for k in order:\r\n",
        "        text += word_table[k]+' '    \r\n",
        "    return text,order \r\n",
        "\r\n",
        "text,order = text_gen(noise)\r\n",
        "\r\n",
        "text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sweJG7zR_MAb"
      },
      "source": [
        "'''\r\n",
        "for i in range(10):\r\n",
        "    noise = tf.random.normal([1,_NOISE_DIM])\r\n",
        "    texts,embeddings,compratios,morpcodes = generator(noise, training=True)\r\n",
        "    p_score = 0\r\n",
        "    for txt in texts:\r\n",
        "        embedding = embedder.encode([txt.numpy().decode('utf-8')],show_progress_bar=False)\r\n",
        "        distances = scipy.spatial.distance.cdist(embedding, [org_text_emb], \"cosine\")[0]\r\n",
        "        score = 1-distances[0]\r\n",
        "        reward = p_score - score\r\n",
        "        if p_score == 0:\r\n",
        "            reward = -reward\r\n",
        "        print(f'score:{score} reward:{reward}') # 1-distance\r\n",
        "        p_score = score\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
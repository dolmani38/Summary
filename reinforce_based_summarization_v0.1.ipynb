{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "korean_abstractive_summarizaion_v1.2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dolmani38/Summary/blob/master/reinforce_based_summarization_v0.1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdM3q73ReHxs"
      },
      "source": [
        "# **Korean Summarizer Using Multiple Discriminators**\n",
        "\n",
        "참조 : https://www.kaggle.com/shivamb/beginners-guide-to-text-generation-using-lstms\n",
        "\n",
        "참조 : https://github.com/williamSYSU/TextGAN-PyTorch\n",
        "\n",
        "* 2020년12월27일 v1.0 완전히 실패...\n",
        "* 2020년12월27일 오후 Generator 다시 만들고... 역시 실패 한듯... v1.2 완전히 실패...\n",
        "* 문법 Discriminator를 먼저 학습하고... transfer learning을 사용해 보자.\n",
        "* 그래도 역시 Generator는 다시 만들어야 할 듯."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNBNW5dMZ13G",
        "trusted": true
      },
      "source": [
        "DO_ALL = True # 전체 실행하면서 시간 걸리는 걸 Pass 하려면 이걸 False ...\n"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "FlvsCFJaeHxt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16f51034-a2e6-4a4b-f0d1-e4b4d0aba899"
      },
      "source": [
        "\n",
        "if DO_ALL:\n",
        "    !pip install sentence-transformers==0.3.0\n",
        "    !pip install transformers==3.0.2\n",
        "    !pip install wikipedia\n",
        "    !pip install konlpy"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers==0.3.0 in /usr/local/lib/python3.6/dist-packages (0.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.0) (1.19.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.0) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.0) (3.2.5)\n",
            "Requirement already satisfied: transformers>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.0) (3.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.0) (0.22.2.post1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.0) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.0) (1.7.0+cu101)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers==0.3.0) (1.15.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (0.1.94)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (0.8.1rc1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (0.8)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (0.0.43)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (20.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (2.23.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers==0.3.0) (1.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.1->sentence-transformers==0.3.0) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.1->sentence-transformers==0.3.0) (0.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=3.0.2->sentence-transformers==0.3.0) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers>=3.0.2->sentence-transformers==0.3.0) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (1.24.3)\n",
            "Requirement already satisfied: transformers==3.0.2 in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.0.43)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.8.1rc1)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.1.94)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (1.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (20.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.0.2) (2.4.7)\n",
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (2.23.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (4.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.12.5)\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.6/dist-packages (0.5.2)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.6.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from konlpy) (0.4.4)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.19.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Em1oCkJceHxz"
      },
      "source": [
        "# keras module for building LSTM \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "from tensorflow.python.framework import tensor_shape\n",
        "import keras.utils as ku \n",
        "\n",
        "# set seeds for reproducability\n",
        "from tensorflow.random import set_seed\n",
        "from numpy.random import seed\n",
        "set_seed(2)\n",
        "seed(1)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string, os \n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFdj0QfSeHx1"
      },
      "source": [
        "# 학습을 위한 데이터 준비"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94NlJEeHeHx3"
      },
      "source": [
        "네이버 뉴스에서 아무거나 하나 Text를 얻어옴\n",
        "\n",
        "이것을 '요약' 목표"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "lz5XtC9MeHx5"
      },
      "source": [
        "org_text = \"\"\"\n",
        "옛날 어느 집에 귀여운 여자 아기가 태어났어요.\n",
        "아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요.\n",
        "그러던 어느날, 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요.\n",
        "소녀의 아버지는 홀로 남은 소녀가 걱정되었어요.\n",
        "그래서 얼마 후 새어머니를 맞이했어요.\n",
        "새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요.\n",
        "그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요.\n",
        "새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요.\n",
        "그런데 이번에는 아버지마저 돌아가셨어요.\n",
        "소녀는 하녀처럼 하루 종일 쓸고, 닦고, 집안일을 도맡아 했어요.\n",
        "해도 해도 끝이 없는 집안일이 힘들어 지칠때면\n",
        "난롯가에 앉아서 잠시 쉬곤 했지요.\n",
        "\"엄마, 저애를 신데렐라라고 불러야겠어요.\"\n",
        "\"온통 재투성이잖아요. 호호호!\" 두 언니는 소녀를 놀려 댔어요.\n",
        "어느 날, 왕궁에서 무도회가 열렸어요.\n",
        "신데렐라의 집에도 초대장이 왔어요.\n",
        "새어머니는 언니들을 데리고 무도회장으로 떠났어요.\n",
        "신데렐라도 무도회에 가고 싶었어요.\n",
        "혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요.\n",
        "\"신데렐라, 너도 무도회에 가고 싶니?\"\n",
        "신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요.\n",
        "\"내가 너를 무도회에 보내주마 호박 한개와 생쥐 두마리, 도마뱀을 구해 오렴.\"\n",
        "마법사 할머니가 주문을 외웠어요.\n",
        "그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금 마차로 변했어요.\n",
        "이번에는 생쥐와 도마뱀을 건드렸어요.\n",
        "그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했답니다.\n",
        "신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요.\n",
        "\"신데렐라, 발을 내밀어 보거라.\"\n",
        "할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요.\n",
        "\"신데렐라, 밤 열두시가 되면 모든게 처음대로 돌아간단다. 황금 마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼. 그러니까 반드시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지?\"\n",
        "왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요.\n",
        "왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고,\n",
        "신데렐라하고만 춤을 추었어요.\n",
        "신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요.\n",
        "땡, 땡, 땡...... 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요.\n",
        "신데렐라가 허둥지둥 왕궁을 빠져나가는데,\n",
        "유리 구두 한 짝이 벗겨졌어요.\n",
        "하지만 구두를 주울 틈이 없었어요.\n",
        "신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요.\n",
        "왕자님은 유리 구두를 가지고 임금님께 가서 말했어요.\n",
        "\"이 유리 구두의 주인과 결혼하겠어요.\"\n",
        "그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요.\n",
        "언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만 한눈에 보기에도 유리 구두는 너무 작았어요.\n",
        "그때, 신데렐라가 조용히 다가와 말했어요.\n",
        "\"저도 한번 신어 볼 수 있나요?\"\n",
        "신데렐라는 신하게 건넨 유리 구두를 신었어요,\n",
        "유리 구두는 신데렐라의 발에 꼭 맞았어요.\n",
        "신하들은 신데렐라를 왕궁으로 데리고 갔어요.\n",
        "그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요.\n",
        "\"\"\""
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "qa-qIW1h3DkA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "a9d26eeb-59c9-4e2e-c47b-6537c4bce4ec"
      },
      "source": [
        "# 간단한 전처리\n",
        "def clean_text(txt):\n",
        "    txt = txt.replace('\\n','')\n",
        "    txt = txt.replace('=','')\n",
        "    txt = txt.replace('\\\"','')   \n",
        "    txt = txt.replace('\\'','')\n",
        "    txt = txt.replace(',','')\n",
        "    txt = txt.replace('..','')\n",
        "    txt = txt.replace('.','. ')\n",
        "    txt = txt.replace('  ',' ')\n",
        "    return txt \n",
        "\n",
        "org_text = clean_text(org_text)\n",
        "org_text"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'옛날 어느 집에 귀여운 여자 아기가 태어났어요. 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요. 그러던 어느날 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요. 소녀의 아버지는 홀로 남은 소녀가 걱정되었어요. 그래서 얼마 후 새어머니를 맞이했어요. 새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요. 그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요. 새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요. 그런데 이번에는 아버지마저 돌아가셨어요. 소녀는 하녀처럼 하루 종일 쓸고 닦고 집안일을 도맡아 했어요. 해도 해도 끝이 없는 집안일이 힘들어 지칠때면난롯가에 앉아서 잠시 쉬곤 했지요. 엄마 저애를 신데렐라라고 불러야겠어요. 온통 재투성이잖아요. 호호호! 두 언니는 소녀를 놀려 댔어요. 어느 날 왕궁에서 무도회가 열렸어요. 신데렐라의 집에도 초대장이 왔어요. 새어머니는 언니들을 데리고 무도회장으로 떠났어요. 신데렐라도 무도회에 가고 싶었어요. 혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요. 신데렐라 너도 무도회에 가고 싶니?신데렐라가 고개를 들어보니 마법사 할머니가 빙그레 웃고 있었어요. 내가 너를 무도회에 보내주마 호박 한개와 생쥐 두마리 도마뱀을 구해 오렴. 마법사 할머니가 주문을 외웠어요. 그리고 지팡이로 호박을 건드리자 호박이 화려한 황금 마차로 변했어요. 이번에는 생쥐와 도마뱀을 건드렸어요. 그랬더니 생쥐는 흰말로 도마뱀은 멋진 마부로 변했답니다. 신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요. 신데렐라 발을 내밀어 보거라. 할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요. 신데렐라 밤 열두시가 되면 모든게 처음대로 돌아간단다. 황금 마차는 호박으로 흰말은 생쥐로 마부는 도마뱀으로 변하게 돼. 그러니까 반드시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지?왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요. 왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고신데렐라하고만 춤을 추었어요. 신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요. 땡 땡 땡 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요. 신데렐라가 허둥지둥 왕궁을 빠져나가는데유리 구두 한 짝이 벗겨졌어요. 하지만 구두를 주울 틈이 없었어요. 신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요. 왕자님은 유리 구두를 가지고 임금님께 가서 말했어요. 이 유리 구두의 주인과 결혼하겠어요. 그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요. 언니들은 발을 오므려도 보고 구두를 늘려도 보았지만 한눈에 보기에도 유리 구두는 너무 작았어요. 그때 신데렐라가 조용히 다가와 말했어요. 저도 한번 신어 볼 수 있나요?신데렐라는 신하게 건넨 유리 구두를 신었어요유리 구두는 신데렐라의 발에 꼭 맞았어요. 신하들은 신데렐라를 왕궁으로 데리고 갔어요. 그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요. '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-k66tHNeHx6"
      },
      "source": [
        "## 한국어 문법 구분 discriminator 학습\n",
        "\n",
        "* '한글 위키백과'에서 임의의 문장을 수집 함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "oi6AfKSzeHx7"
      },
      "source": [
        "#한국어 위키백과에서 스크랩핑\n",
        "\n",
        "import wikipedia as wiki\n",
        "wiki.set_lang('ko')"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ekFwbQVxeHx7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c745e984-fadd-427f-abb0-c26d6b21e8db"
      },
      "source": [
        "# '전래동화' 라는 keyword로 100개 page의 Text를 취득\n",
        "\n",
        "def __search_from_wiki(question,max_rank):\n",
        "    results = wiki.search(question,results=max_rank)\n",
        "    print(results)\n",
        "    contents = []\n",
        "    for result in results:\n",
        "        try:\n",
        "            page = wiki.page(result)\n",
        "            #print(f\"Top wiki result: {page}\")\n",
        "            text = page.content\n",
        "            ln = len(text)\n",
        "            #print(f'Collecting page : {page} , text length {str(ln)}')\n",
        "            #if ln < 4000:\n",
        "            #  contents.append(text)\n",
        "            #else:\n",
        "            #  contents.append(text[0:4000])\n",
        "            contents.append(text)\n",
        "        except Exception as ex:\n",
        "            print(ex)\n",
        "    return contents\n",
        "\n",
        "if DO_ALL:\n",
        "    ko_grammar_set_raw = __search_from_wiki(\"동화\",300)\n",
        "    ko_grammar_set_raw = __search_from_wiki(\"소설\",300)\n",
        "    ko_grammar_set_raw = __search_from_wiki(\"사설\",300)\n",
        "    ko_grammar_set_raw = __search_from_wiki(\"설명\",300)\n",
        "    print(f'전체 수집한 Page Count : {len(ko_grammar_set_raw)}')\n"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['동화', '니코니코 동화', '동화중학교 (경기)', '문화 동화', '조곡 니코니코 동화', '동화사', '신에이 동화', '번암초등학교 동화분교', '동화작용', '책상 서랍 속의 동화', '동화초등학교 (경기)', '동화약품', '동화기업', '동화고등학교', '동화면세점', '동화역', '동화초등학교 (전남)', '화성동화중학교', '그림 동화', '동화초등학교 (강원)', '가을동화', '단백동화 스테로이드', '동화 (언어학)', '애니메이션', '동화초등학교 (충북)', '수성알파시티 동화아이위시', '동화도서관', '훈1등 욱일동화대수장', '동화초등학교', '도에이 애니메이션', '동화중학교 (강원)', '동화은행', '전북동화중학교', '대전동화초등학교', 'TV로 보는 원작동화', '동윤', '대전동화중학교', '신 전래동화', '동화초등학교 (제주)', '도라에몽 (1973년 애니메이션)', '한스 크리스티안 안데르센', '대원씨아이', '빌헬름 하우프', '그림 형제', '동화면', '빛가람장성로', '동화 읽어주는 TV', '아동 문학', 'TV동화 행복한 세상', '촉한', '권설 설측 접근음', '정이서', '전한', '질소 고정', '동화운수 (인천)', '대구막항', '동문항', '대구 동화사 비로암 삼층석탑', '유글레나류', '대한민국의 국회의원 선거구 목록', '리하르트 빌슈테터', '쥐', '팔공산', '코파반장의 동화수사대', '리루프릿', '스튜디오 지브리', '1844년', '동화 (후한)', '동화운수 (광주)', '대구 동화사 당간지주', '동화처럼', '주엽어린이도서관', '아돌프 폰 바이어', '必UP되다', '신-맨', '산서초등학교 (전북)', '스튜디오 딘', '오엘엠', '삼성물산', '대구 동화사 마애여래좌상', '등장인물', '샤를 페로', '란티스 조곡 feat.Nico Nico Artists', '루트비히 티크', '한국의 사찰', '동화 (화폐)', '안녕 자두야', '일본동화협회', '곰돌이 푸', '사랑의 시나리오', '김지우 (1983년)', '쇼 바이 락!!', '동화나라 꿈동산', '도라에몽 그리기', '순천 동화사 삼층석탑', '불도그', '도가코보', '주화', '귀스타브 도레', '장성소방서', '백설 공주', '보그 (스타 트렉)', '은비까비의 옛날옛적에', '호시조라 미유키', '호리구치 유키코', '도라에몽: 스탠바이미', '황민화 정책', '짱구는 못말려 극장판: 태풍을 부르는 장엄한 전설의 전투', '동화 (영화)', '야코프 그림', '정범균', '조류 (수생 생물)', '방정환', '일본의 애니메이션', 'Oneiric Diary (幻想日記)', '짱구는 못말려 극장판: 태풍을 부르는 노래하는 엉덩이 폭탄!', '불꽃놀이 (1997년 드라마)', '한다시', '잠자는 숲속의 미녀', '장 드 라 퐁텐', '서울동화축제', '삽화가', '권정생', '슈렉', '아이 러브 이태리', '프린세스 츄츄', '지방도 제743호선', '책', 'TVB 코리아', '옛날 옛적에 (애니메이션)', '영심이', '스튜어트 리틀', '클레오', '남세균', '계몽사', '머털도사', '잠자는 숲속의 미녀 (1959년 영화)', '판타지 문학', '극장판 도라에몽: 진구의 달 탐사기', '피노키오', '짱구는 못말려 극장판: 어른 제국의 역습', '마야', '오로라 (디즈니)', '와우중학교', '라푼첼', '천사의 분노', '이산 (태국)', '황록조류', '짱구는 못말려 극장판: 폭풍을 부르는 정글', '블랙★록 슈터', '그림명작극장', '대구 동화사 대웅전', '신데렐라 (디즈니)', '동우에이앤이', '리미티드 애니메이션', '대구 동화사 부도군', '이세진 (희극인)', '구성주의 (교육)', '바다의 전설 장보고', '허구 국가', '벨 (디즈니)', '대구 동화사 비로암 석조비로자나불좌상', '마해송', '지방도 제312호선', '예른 안데르센', '대구 동화사 보조국사 지눌 진영', '유대인', '서울동화프로덕션', '정상희', '빌리빌리', '인어 공주 (1989년 영화)', '조안 (배우)', '오즈의 마법사 (1939년 영화)', 'KBS 키즈', '한국일보', '봉담고등학교', '헨젤과 그레텔', '복현규', '검정고무신', '루이스 세풀베다', '우뢰매', '대구 동화사 극락전', '마크 트웨인', '동화 만화 캘린더', '에어맨을 쓰러뜨릴 수 없어', '슈퍼 시로', '빨간 자전거', '내 여동생이 이렇게 귀여울 리가 없어의 에피소드 목록', '출판사', '유루캠Δ', '아이누', '극장판 도라에몽 진구의 아프리카 모험: 베코와 5인의 탐험대', '서울시립 어린이도서관', '라이먼 프랭크 바움', '아기공룡 둘리', '간현역', '꾸러기 수비대', '이언 플레밍', '신춘문예', '아스트리드 린드그렌', '원주민', 'P.A. 웍스', '일본인', '떠돌이 까치', '산서중학교', '청자 동화연화문 표주박모양 주전자', '이화작용', '니코니코 생방송', '대구 동화사 염불암 청석탑', '슈퍼 태권 V', '인어', '겨울연가', '아바르', '물망초 (드라마)', '채은정', '비의', '효행로', 'MBC 베스트셀러극장', '오세암', '허브 (영화)', '에리얼 (디즈니)', '으랏차차 짠돌이네', '사이비 종교', '제비', '공주와 개구리', 'SUPER SHOW 2', 'DR 무비', 'ATC 코드 A14', '욱일장', '눈의 여왕', '음운 규칙', '동게르만족', '백설 공주와 일곱 난쟁이 (영화)', '이광모', 'Love in me', '토키와 타카코', '강제동화', '이카보드와 토드경의 모험', '나카시마 테츠야', '지방도 제616호선', '대구 동화사 아미타회상도', '이동윤', '대사경로', '짱구는 못말려 극장판: 폭풍을 부르는 석양의 떡잎마을 방범대', '프랜시스 호지슨 버넷', 'ㅥ', '순수의 시대 (드라마)', '봉담읍', '이종혁 (성우)', '이솝 우화', '꼬비꼬비', '남자친구 (드라마)', '대구 동화사 염불암 마애여래좌상 및 보살좌상', '독고탁 2 - 내 이름은 독고탁', '메틸말로닐-CoA', '일선동조론', '핑두시', '동화 (동음이의)', '신데렐라 (1950년 영화)', '나두야 간다', '동요', '셉티미우스 세베루스', '대공원역 (과천)', '신춘호', 'Into The New World (콘서트)', '총림', '비밀정원', '류칭윈', '걸리버 여행기', '흑마녀 나가신다!!', '이원수 (작가)', '대구 동화사 사명당 유정 진영', '바이윈구 (광저우시)', '여름향기', '물질대사', '짱구는 못말려 극장판: 태풍을 부르는 영광의 불고기 로드', '1805년', '위세 성', '자전거 도둑 (박완서)', '파파야 (음악 그룹)', '그린세이버', '대구 동화사 삼장보살도', '최강희 (배우)', '빨간 두건', '니코니코 대백과', '문학', '송정천 (울산)', '하종오', '귄터 폰 클루게', '멜랑슈', '산서고등학교', '녹색조류', '흙꼭두장군', '의왕백운호수축제', '사토 준이치', '대구 동화사 금당암 동·서 삼층석탑', '1875년', '쇼와 국가주의', '1835년', '4월 2일']\n",
            "\"동화초등학교\" may refer to: \n",
            "동화초등학교\n",
            "동화초등학교\n",
            "동화초등학교\n",
            "동화초등학교\n",
            "동화초등학교\n",
            "대전동화초등학교\n",
            "\"마야\" may refer to: \n",
            "마야 문명\n",
            "마야 (종교)\n",
            "마야 달력\n",
            "마야 숫자\n",
            "마야 문자\n",
            "마야 역\n",
            "마야 (소프트웨어)\n",
            "MAYA\n",
            "마야 (가수)\n",
            "마야\n",
            "요시다 마야\n",
            "유네스키 마야\n",
            "마야 미키\n",
            "마야 산사\n",
            "마야 루돌프\n",
            "마야 앤절로\n",
            "코이즈미 마야\n",
            "마야 로렌스\n",
            "마야 카린\n",
            "마야 스토얀\n",
            "마야 아바바자니\n",
            "마야 보호시에비치\n",
            "마야 카잔\n",
            "마야 알데린\n",
            "마야 쿨리예바\n",
            "모리 마야\n",
            "마야 오스타셰프스카\n",
            "마야 히르슈\n",
            "마야 다간\n",
            "마야 디아브\n",
            "마야 나세르\n",
            "에드워드 마야\n",
            "마야 야게르\n",
            "마야 눌키츠\n",
            "내 이름은 마야\n",
            "꿀벌 마야\n",
            "마야 (1966년 영화)\n",
            "마야 (2014년 영화)\n",
            "마야 아일랜드 항공\n",
            "\"동화 (동음이의)\" may refer to: \n",
            "동화\n",
            "동화\n",
            "동화\n",
            "동화\n",
            "동화\n",
            "동화\n",
            "동화\n",
            "동화\n",
            "동화 (가수)\n",
            "['소설', '연애 소설', '소설 목민심서', 'SF (장르)', '모험 소설', '소설가', '중편 소설', 'KBS 2TV TV 소설', '서간체 소설', '단편 소설', '추리물', '온라인 소설', '판타지 문학', '전쟁 소설', '역사소설', '장편 소설', '코미디 소설', '휴대 전화 소설', '연속 TV 소설', '사변물', '인터넷 사변 소설 데이터베이스', '무협', '문학', '고딕 소설', '대체 역사', '게임 소설', '공포물', '민왕 (소설)', '판타지', '813', '라이트 노벨', '골든타임 (소설)', '뉴문', '스릴러', '호빗 (소설)', '소나기 (소설)', '전기소설', '번안소설', '덴게키 소설대상', '러시안 소설', '개미 (소설)', '루이스 세풀베다', '1984년 (소설)', '소설가가 되자', '신소설', '소설극장', '트와일라잇 (소설)', '고전소설', '은하 TV 소설', '장르', '장르 소설', '소설 (절기)', '상록수', '월터 스콧', '해리 포터', '28', '피카레스크 소설', '문학동네', '어니스트 헤밍웨이', '연속 TV 소설 작품 목록', '올가 토카르추크', '논픽션', '교양 소설', 'KBS TV 문학관', '브레이킹 던', '러키☆스타 (소설)', '편지 (소설)', '7월 21일', '쥐덫 (소설)', '엘프리데 옐리네크', '토지 (소설)', '희극', '오리정', '토니 모리슨', '작은 아씨들', '율리시스 (소설)', '악의 (소설)', '이무영 (소설가)', '미겔 앙헬 아스투리아스', '히페리온', '와호장룡', '빙점 (소설)', '뇌 (소설)', '희곡', '덴게키 문고', '쿠오 바디스 (소설)', '침묵 (소설)', '신 (소설)', '연애소설 (영화)', '이 멋진 세계에 축복을!', '쥬라기 공원 (영화)', '검은 고양이 (단편소설)', '나니아 연대기', '커튼 (소설)', '피터 팬', '이클립스 (소설)', '오정희 (소설가)', '집념 (드라마)', '톰 존스의 화려한 모험', '김유정 (소설가)', '제임스 본드', '파라다이스 (소설)', '6월 16일', '다이브 (소설)', '사이버펑크', '무라카미 하루키', 'GO (소설)', '그것 (소설)', '메리 셸리', '6월 5일', '나무 (소설)', '대원군 (소설)', '찰스 디킨스', '가즈오 이시구로', '애거사 크리스티', '천둥소리', '비에른스티에르네 비에른손', '레몬 (소설)', '팬 픽션', '빅토르 위고', '제임스 조이스', '웃음 (소설)', '주제 사라마구', '철도원 (소설)', '알메이다 가헤트', '비밀 (소설)', '아침이 온다', '새턴상', '페르 라게르크비스트', 'SF의 장르', '9월 21일', '슬레이어즈', '삽화가', '소설가 (제자백가)', '도련님 (소설)', 'H. G. 웰스', '장임', '책벌레의 하극상', '이호철', '헝거 게임 (소설 시리즈)', '머글', '트와일라잇 (소설 시리즈)', '엠마 (소설)', '황석영', '왕좌의 게임', '1937년', '에밀 졸라', '은하수를 여행하는 히치하이커를 위한 안내서', '네뷸러상', '아나톨 프랑스', '미스터리', '잘로', 'SF', '우리 생애 최고의 해', '서사시', '1935년', '브와디스와프 레이몬트', '등신불 (드라마)', '윌리엄 골드먼', '9월 17일', '연재소설', '이광수', '허쉬 (드라마)', '12월 28일', '이인화', '신세계에서 (소설)', '조지 버나드 쇼', '마오둔', '작가', '아서 코난 도일', '세상의 중심에서 사랑을 외치다', '구름계단 (소설)', '베르나르 베르베르', '스티븐 킹', '건담 센티넬', '토마스 만', '살인 소설', '파피용 (소설)', '다다미 넉 장 반 세계일주', '김동인', '수호전', '액자 구조', '쥬라기 공원', '본 아이덴티티', '상록구', '미하일 레르몬토프', '파트리크 모디아노', '미겔 데 세르반테스', '청춘 소설', '파울 요한 루트비히 폰 하이제', '휴고상', '알렉상드르 뒤마', '태백산맥 (소설)', '크누트 함순', '테오필 고티에', '여자의 남자', '사랑 이야기 (소설)', '잃어버린 세계', '윌리엄 피터 블래티', '아인 랜드', '길 (1995년 드라마)', '플랫폼', '1월 17일', '우주 전쟁 (소설)', '도쿠가와 이에야스 (소설)', '노르웨이의 숲 (소설)', '을화 (드라마)', '보헤미아니즘', '링 (스즈키 고지의 소설)', '드라큘라', '이야기 (소설 시리즈)', '박상연', '김수현 (작가)', '사뮈엘 베케트', '오노레 드 발자크', '도싯주', '장정일', '대하소설', '산문', '이형표', '스탕달', '귀향', '6.25 (드라마)', '11월 13일', '동물 농장', '1902년', '구해줘', '12월 20일', '지킬 박사와 하이드 씨', '1870년', '바이센테니얼맨', '박연희', '귀스타브 플로베르', '일본 문학', '레 미제라블', '이케부쿠로 웨스트 게이트 파크', '철학물', '풀 메탈 패닉!', '소설, 영화와 만나다', '조세희', '지괴소설', '마법', '파라다이스', '1892년', '신춘문예', '마크 트웨인', '헤르만 헤세', '이야기 종류 목록', '지지 (1958년 영화)', '은비령', '강영숙 (소설가)', '허준 (드라마)', '12월 19일', '내 여동생이 이렇게 귀여울 리가 없어', '황순원', '히가시노 게이고', '4월 1일', '코바리드', '김성동 (소설가)', '이청준', '박완서', '나관중', '허구', '반동인물', '8월 30일', '밀양 (영화)', '카밀루 카스텔루 브랑쿠', '주동인물', '마음 (소설)', '김원일 (소설가)', '삼국지 가공인물', '6월 25일', '원미동 사람들 (드라마)', '백야행', '12월 21일', '마거릿 애트우드', '로맨스', '무정 (소설)', '자야', '한강 (소설가)', '아리아드네 올리버', '알라바마 이야기', '무인도', '윌리엄 골딩', '프리드리히 횔덜린', '펄 S. 벅', '장일호', '이병주 (소설가)', '수이전', '인터넷 작가', '7월 23일', '이언 플레밍', '스즈모토 유이치', '김정한 (소설가)', '불멸의 이순신', '고블린', '허구문', '1월 4일', '빛나거나 미치거나', '기요스 회의']\n",
            "\"히페리온\" may refer to: \n",
            "히페리온 (신화)\n",
            "히페리온 (위성)\n",
            "히페리온 원시 초은하단\n",
            "히페리온 (키츠 시)\n",
            "히페리온 (시먼스 소설)\n",
            "히페리온 (롱펠로 소설)\n",
            "히페리온 (횔덜린 소설)\n",
            "히페리온 (잡지)\n",
            "디즈니 하이페리온\n",
            "스코피우스 하이페리온 말포이\n",
            "하이페리온 (컴퓨터)\n",
            "하이페리온 엔터테인먼트\n",
            "하이페리온 솔루션스\n",
            "하이페리온 (만화)\n",
            "하이페리온 호텔\n",
            "히페리온 (스타크래프트)\n",
            "히페리온 (은하 영웅 전설)\n",
            "하이페리온 클래스 (바빌론 5)\n",
            "EAS 하이페리온\n",
            "하이페리온 비행선\n",
            "CAT1-X 하이페리온 건담 시리즈\n",
            "하이페리온 (목동)\n",
            "하이페리온 (해운대)\n",
            "하이페리온 (말)\n",
            "하이페리온 (나무)\n",
            "하이페리온 (요트)\n",
            "하이피어리언 (캘리포니아)\n",
            "HMS 하이페리온\n",
            "하이페리온 픽처스\n",
            "하이페리온 파워 제너레이션\n",
            "하이페리온 레코즈\n",
            "\"SF\" may refer to: \n",
            "SF (장르)\n",
            "사변물\n",
            "과학 판타지\n",
            "샌프란시스코\n",
            "샌프란시스코 자이언츠\n",
            "스몰 포워드\n",
            "소스포지.넷\n",
            "스트리트 파이터\n",
            "스페셜포스\n",
            "스타폭스\n",
            "어몽 어스\n",
            "슈퍼 패미컴\n",
            "Schweizer Fernsehen\n",
            "Syfy\n",
            "음악\n",
            "SF 매거진\n",
            "\"잃어버린 세계\" may refer to: \n",
            "잃어버린 세계 (장르)\n",
            "잃어버린 세계 (아서 코난 도일의 소설)\n",
            "잃어버린 세계 (1925년 영화)\n",
            "잃어버린 세계 (1960년 영화)\n",
            "잃어버린 세계 (마이클 크라이튼의 소설)\n",
            "쥬라기 공원 2: 잃어버린 세계\n",
            "로스트 월드 (만화)\n",
            "로스트 월드 (게임북)\n",
            "로스트 월드 (게임)\n",
            "소닉 로스트 월드\n",
            "\"플랫폼\" may refer to: \n",
            "승강장\n",
            "컴퓨팅 플랫폼\n",
            "플랫폼 (자동차)\n",
            "플랫폼 (소설)\n",
            "창고바인더\n",
            "\"귀향\" may refer to: \n",
            "귀향 (2016년 영화)\n",
            "귀향 (2009년 영화)\n",
            "귀향 (2006년 영화)\n",
            "귀향 (2004년 영화)\n",
            "귀향 (1998년 영화)\n",
            "귀향 (1978년 영화)\n",
            "귀향 (1940년 영화)\n",
            "귀향 (김동률의 음반)\n",
            "귀향 (남일해의 음반)\n",
            "귀향 (1878년 소설)\n",
            "귀향 (1984년 소설)\n",
            "\"구해줘\" may refer to: \n",
            "구해줘 (드라마)\n",
            "구해줘 2\n",
            "구해줘 (소설)\n",
            "구해줘!\n",
            "\"파라다이스\" may refer to: \n",
            "파라다이스(주)\n",
            "파라다이스\n",
            "파라다이스\n",
            "파라다이스\n",
            "파라다이스\n",
            "파라다이스\n",
            "Paradise (콜드플레이의 노래)\n",
            "파라다이스 GoGo!!\n",
            "파라다이스 앵무\n",
            "패러다이스 (캘리포니아주)\n",
            "패러다이스 (캔자스주)\n",
            "패러다이스 (켄터키주)\n",
            "패러다이스 (미시간주)\n",
            "패러다이스 (몬태나주)\n",
            "패러다이스 (네바다주)\n",
            "패러다이스 (오리건주)\n",
            "패러다이스 (펜실베이니아주)\n",
            "패러다이스 (텍사스주)\n",
            "패러다이스 (유타주)\n",
            "패러다이스 (워싱턴주)\n",
            "패러다이스 (노바스코샤주)\n",
            "패러다이스 (뉴펀들랜드 래브라도 주)\n",
            "\"로맨스\" may refer to: \n",
            "로망스 어군\n",
            "로맨스\n",
            "로맨스\n",
            "낭만주의\n",
            "로망스\n",
            "로맨스\n",
            "로맨스\n",
            "로맨스\n",
            "로맨스\n",
            "ROMANCE\n",
            "Romance\n",
            "로맨스 (아칸소주)\n",
            "로맨스 (미주리주)\n",
            "로맨스 (위스콘신주)\n",
            "로망스\n",
            "\"자야\" may refer to: \n",
            "자야 (소설)\n",
            "자야 (시설)\n",
            "자야 (사람)\n",
            "자야 (가수)\n",
            "자야 바찬\n",
            "자야 시로지로\n",
            "자야\n",
            "['사설', '태안 사설 해병대 캠프 실종 사고', '성호사설', '사유 철도', '광지원초등학교', '평택중앙초등학교', '퓰리처상 사설 부문', '학원', '가상사설망', '로동신문', '갈육초등학교', '서원', '가상 사설 서버', '석파정', '광역 통신망', 'LG상남도서관', '사설 서버', '안명초등학교', '갈현초등학교', '온양민속박물관', '제주도순환궤도', '경북선', '고대 우주비행사설', '금강산선', '애니메 뉴스 네트워크', '시조', '서원 철폐', '영동선', '개성 숭양서원', '며느리와 사위', 'PC통신', '남천초등학교 (부산)', '왕도 (드라마)', '벽제초등학교', '청리역', '2013년 7월', '국제법률경영대학원대학교', '대창중학교', '서울경제', '오산대역', '.kp', '매일신문', '근화여자고등학교', '사설망', '미국 야구 명예의 전당', '약산초등학교', '조선총독부 철도국', '금강산 관광객 피격 사망 사건', 'DB하이텍', '산케이 신문', '윌리엄 허긴스', '수필', '수려선', '뉴욕 타임스', '함안역', '퓰리처상', '파평초등학교', '판소리', '사립탐정', '철마초등학교', '블리자드 엔터테인먼트', '상북초등학교 소호분교', '동두천송내초등학교', '구 (언어학)', '대창고등학교', '가마보코', '푸러형 증기 기관차', '발림', '프리스트 (드라마)', '이양역', '화이: 괴물을 삼킨 아이', '약과', '서울 지하철 4호선', '신재효', '8월 12일', '팔로마 천문대', '협궤', '노동 빈곤층', '울산문수야구장', '기술 표준', '장연선', '사무이 공항', '전국연합학력평가', '황성신문', '수임료', '포항영흥초등학교', '프린스턴 고등연구소', '배영초등학교 (부산)', '한국중부발전', '상침 송씨', '벌교역', '편집', '대한민국 중앙선거관리위원회', '가메이도 사건', '유니버설 아트센터', '석보중학교', '에르스탈 국립공장', '강병규 (1972년)', '조성역', '1920년', '횡성 고씨', '조산 운동', '서호선', '단가', '날, 보러와요', '11월 7일', '바리공주', '카날 그란데', '심청가', '코리안 심포니 오케스트라', '상주역', '예천역', '마이니치 신문', '옹고집타령', '황라열', '유퉁', '예산역', '흥보가', '환구시보', '유라시안 필하모닉 오케스트라', '너름새', '아이스테이션', '좀비탐정', '고추장', '정현 (후한)', '한겨레', '판교역 (서천)', '피에트로 로레단', '웅천역', '안정복', '금골선', '개포역', '나무위키', '쓰쿠바 (순양전함)', '신기역 (삼척)', '구덕야구장', '젠틀맨', '능주역', '수풍선', '영주 의산서원', '충남선 (1966년)', '허천선', '인디펜던트', '신례원역', '독일 항공우주 센터', '신음역', '하인리히 하이네', '김용만 (희극인)', '삽교역', '소요 학파', '아이폰 6', '압록강선', '김상태 (1930년)', '박은식', '반성역', '세이부 철도', '군북역', '사설시조', '용궁역', '전용선', '옥산역', '잡가 (노래)', '사무이섬', '동아일보', '은률선', '서천역', '한국 민요', '기보법', '방탄복', '의견', '양지역 (철원)', '대한민국의 부통령', '보성역', '홍성역', '우주 센터', '바리스타', '정연역', '문천항선', '출입국·외국인정책본부', '4·19혁명기념도서관', '노동 시간', '서당', '웹 호스팅 서비스', '도고온천역', '우리집 (드라마)', '조선일보에 대한 비판', '장미가 없는 꽃집', '동철원역', '효천역', '중리역', '서호역', '가루지기타령', '하고사리역', '구평공산당', '니시자와 모모카', '집', '전라선', '서울 지하철 3호선', '네트워크 주소 변환', '2기 신도시', '득량역', '로마의 도로', '온동역', '제과사', '올 엘리트 레슬링', '호랭총각', '가도 (철산군)', '마전역 (안성)', '아니리', '클레이 수학연구소', '고상전역', '경남일보', '옹진선', '원죽역', '한국전화번호부', '원창역', '워싱턴 해군 군축 조약', '명봉역', '애니매트릭스', '파놉티콘', '여수항역', '장끼타령', '주천역', '생채', '핑커톤', '사학', '화순역', '보훔 루르 대학교', '시부사와 게이조', '어묵', '진성역', '입석리역', '매산역', '행죽역', '월곡역 (대한민국 철도청)', '경파역', '사설 교환', '비상근', '공주대학교 사범대학 부설고등학교', '청양 장곡사 설선당', '안동역', '문학역', '덕양역', '정창업', '9월 25일', '이케부쿠로 역', '제주지방항공청', '다사도선', '고사리역', '김화역', '안성선', '경북풍산역', '화계역 (금강산선)', '성호기념관', '히토리시즈카', '나카가와 히데나오', '금곡역 (금강산선)', '창도역', '호명역', '한국교육과정평가원', '금성역', '궤도 (교통)', '대천역', '전국경제인연합회', 'MOBA', '불어라 미풍아', '제빵사', '조리사', '타이포그래피', '표준궤', '미용사', '지명수배', 'IMG 아카데미', '위키트리', '인증 기관', '간첩 행위', '정하역', '서울시립교향악단', '시나 웨이보', '효자역 (포항)', '평촌역 (진주)', '고추', '현리역', '동항초등학교', '고난의 행군', '서버 에뮬레이터', '주포역', '오가역', '행정역', '3·20 전산 대란', '동해중부선 (일제 강점기)', '1966년', '백양역 (김화)', '하소역', '호구포역', '성동역', '금곡역 (남양주)', '스팀 (소프트웨어)', '박근혜에 대한 비판', '순창군']\n",
            "\"사학\" may refer to: \n",
            "사학\n",
            "사학\n",
            "사학\n",
            "사학\n",
            "사학\n",
            "\"MOBA\" may refer to: \n",
            "멀티플레이어 온라인 배틀 아레나\n",
            "배드 아트 미술관\n",
            "['설명', '설명 (이야기)', '내 몸 사용설명서', 'YTN라디오', '대한민국 형사소송법 제245조의2', '대한민국 형사소송법 제279조의2', 'CAS 등록번호', '정치부 회의', '통일사용설명서', '태국의 불교', '전자 배치', '물리학', '말풍선 (위젯)', 'PubChem', '과학적 실재론', '생물 다양성', '자유의지', '취급 설명서 (노래)', '아이티어 위키백과', '사용 설명서', '도미니카 연방', 'Man page', 'FFV1', '도리스 (신화)', '시외버스', '음양', '캄보디아의 불교', '결정론', '설명기보법', '위키데이터', '유두양털박쥐', '팔대성지', '셰이머스 히니', '임부 투여 안전성', '오디세아스 엘리티스', '빛', '카를 아돌프 기엘레루프', '원자핵', '1752년', '남자사용설명서', '귀추법', '사슴쥐', '세계의 포유류 종', '이야기 종류 목록', '대승기신론', '과학철학', '대한민국 형사소송법 제43조', '설수', '한자 모양 설명 문자', '비디오1', '위파사나', '중력', '토머스 헌트 모건', '수트라', '마야콥스카야 역 (모스크바 지하철)', '아이작 바셰비스 싱어', '신화', '대한민국 형사소송법 제171조', '에스와티니', '승만경', '슬로바키아의 유로 주화', '대한민국 형사소송법 제259조', '대한민국 형사소송법 제160조', '윈도우 음성 인식', '이집트의 코로나19 범유행', '청색 낙오성', '대한민국 형사소송법 제291조', '차륜 배치', '이란의 코로나19 범유행', '카타르의 코로나19 범유행', '홍콩의 코로나19 범유행', '아시아의 코로나19 범유행', '캐나다의 코로나19 범유행', '체코의 코로나19 범유행', '인도의 코로나19 범유행', '에스토니아의 코로나19 범유행', '유럽의 코로나19 범유행', '포르투갈의 코로나19 범유행', '이스라엘의 코로나19 범유행', '코소보의 코로나19 범유행', '태국의 코로나19 범유행', '핀란드의 코로나19 범유행', '아일랜드의 코로나19 범유행', '트베르스카야 역', '싱가포르의 코로나19 범유행', '터키의 코로나19 범유행', '양력', '인도네시아의 대통령', '프랑스의 코로나19 범유행', '슬로베니아의 코로나19 범유행', '에리트레아의 코로나19 범유행', '압하지야의 코로나19 범유행', '알제리의 코로나19 범유행', '북아메리카의 코로나19 범유행', '레위니옹의 코로나19 범유행', '캄차카반도', '부탄의 코로나19 범유행', '테아트랄나야 역', '마다가스카르의 코로나19 범유행', '코트디부아르의 코로나19 범유행', '우간다의 코로나19 범유행', '중국 대륙의 코로나19 범유행', '아프리카의 코로나19 범유행', '몽골의 코로나19 범유행', '자메이카의 코로나19 범유행', '저지섬의 국가', '무상', '지부티의 코로나19 범유행', '카보베르데의 코로나19 범유행', '산마리노의 코로나19 범유행', '그리스의 코로나19 범유행', '스페인의 코로나19 범유행', '독일의 코로나19 범유행', '노르웨이의 코로나19 범유행', '키르기스스탄의 코로나19 범유행', '남아메리카의 코로나19 범유행', '르완다의 코로나19 범유행', '가봉의 코로나19 범유행', '라오스의 코로나19 범유행', '말라위의 코로나19 범유행', '양자화학', '왈리스 푸투나', '모잠비크의 코로나19 범유행', '대한민국 형사소송법 제179조의2', '나미비아의 코로나19 범유행', '폴란드의 코로나19 범유행', '코로나19 범유행의 경과', '오스트리아의 코로나19 범유행', '과테말라의 코로나19 범유행', '아르헨티나의 코로나19 범유행', '필리핀의 코로나19 범유행', '오스트레일리아의 코로나19 범유행', '감비아의 코로나19 범유행', '세네갈의 코로나19 범유행', '아이티의 코로나19 범유행', '변신자동차 또봇', '베냉의 코로나19 범유행', '광자', '지리학', '막스 폰 라우에', '에스와티니의 코로나19 범유행', '자동 초점', '타임라인 (dps의 노래)', '대한민국의 공공기관', '벨기에의 코로나19 범유행', '뉴질랜드의 코로나19 범유행', '타이완의 코로나19 범유행', '쿠웨이트의 코로나19 범유행', '137', '마카오의 코로나19 범유행', '보츠와나의 코로나19 범유행', '튀니지의 코로나19 범유행', '말레이시아의 코로나19 범유행', '콩고 공화국의 코로나19 범유행', '세인트루시아', '불가리아의 코로나19 범유행', '모델', '계산물리학', '모토로라 모토 X (2014년)', '미국의 코로나19 범유행', '일본의 코로나19 범유행', '솔로몬 제도의 코로나19 범유행', '블라디미르 (도시)', '베네수엘라의 코로나19 범유행', '레바논의 코로나19 범유행', '수단의 코로나19 범유행', '기니의 코로나19 범유행', '아이슬란드의 코로나19 범유행', '헨드릭 로런츠', '레킹 크루', 'Mixi', '사카로미케스아문', '행위자 연결망 이론', '리얼오디오', '자본', '필리핀 전역 (1941년~1942년)', '베르사유궁', '마가단주', '양자역학', '이탈리안 그레이하운드', '십자가형 어둠', '상투메 프린시페의 코로나19 범유행', '바레인의 코로나19 범유행', '나라 목록', '프레젠테이션 소프트웨어', '축치 자치구', '앤 설리번', '도미니카 공화국의 코로나19 범유행', '자본축적론', '리처드 스윈번', '광학', '코로나19의 유람선 감염', '내몽골 자치구의 코로나19 범유행', '나홋카', '지구', '에리크 악셀 카를펠트', '아비달마구사론', '신 에다', '비비레보 역', '패키지 관리자', '아랍에미리트의 코로나19 범유행', '애니콜 네온', '클라우디오스 프톨레마이오스', '서력기원', '에너지 준위', 'AIFF', '루시퍼 (암호)', '수리철학', '염기', '항공 승무원', '나선은하', '밀레토스 학파', '제2차 세계 대전 기간 미국의 군사 역사', '자메이카', '유닉스 셸', '대한민국의 코로나19 범유행', '라트비아의 국기', '설명책임', '한국의 독립운동', '설일체유부', '예술', '아무르주', '위신호 제거', '엔키 카테나', '신라의 대외 관계', '자막', '발명', '정형외과', '마요트', '결정계수', '비블리오테카 이메니 레니나 역', '4대 정령', '미국의 전쟁 범죄', 'Ap/Bp 별', '고려의 불교', '크게 휘두르며의 등장인물 목록', '크라스노프레스넨스카야 역', '세계의 주요 종교', '말리의 코로나19 범유행', '이사금', '초끈 이론', '원자', '토고의 코로나19 범유행', '두타산 (충북)', '저술가', '설교', '유진 위그너', '오트라드노예 역', '윌리엄 앨프리드 파울러', '수즈달', '오컴의 면도날', '리투아니아의 국기', '대한민국 상법 제300조', '전체론', '카보베르데', '환신경동물', '불가지론', '기니비사우', '과학적 방법', '도브리닌스카야 역', '시네팩', '필리핀 연방', '능가경', '케냐의 코로나19 범유행', '잠비아의 코로나19 범유행', 'GNU 선언문', '인도네시아의 코로나19 범유행', '벨기에의 행정 구역', '미스터리', '파벨레츠카야 역 (콜체바야 선)', '옥탸브리스카야 역 (콜체바야 선)', '남오세티야의 코로나19 범유행', 'HTTP 404', 'Sysprep', '보르 (노르드 신화의 여신)', '지장경', '그리스 신화', 'JPEG', '성서비평학', '1838년 미국 하원의원 선거', '원심력', '천문학자', '모음', '미국의 항공사 목록', '대통일 이론', '과테말라 요리', '성사', '아낙시만드로스', '과학', '떡박물관', '앨버트 반두라', '주석 (해석학)', '소권', '애착', '생명', '티베트 자치구의 코로나19 범유행', '설명가능 인공지능', '플레이초이스-10', '구카이', '기독교와 타종교들']\n",
            "\"모델\" may refer to: \n",
            "모델 (자연과학)\n",
            "모델 (경제학)\n",
            "모형 (논리학)\n",
            "모형 이론\n",
            "컴퓨터 모델\n",
            "데이터 모델\n",
            "표준 모형\n",
            "비즈니스 모델\n",
            "사이버 모델\n",
            "모델링 (심리학)\n",
            "3차원 모델링\n",
            "역할 모델\n",
            "모델 (직업)\n",
            "발터 모델\n",
            "루이스 모델\n",
            "모델 (드라마)\n",
            "전체 수집한 Page Count : 299\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Tu_6jZAmeHx9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a877179-a529-40b6-c2c2-6149fb7f33c6"
      },
      "source": [
        "# 간단한 전처리\n",
        "\n",
        "ko_grammar_set_raw = [clean_text(x) for x in ko_grammar_set_raw]\n",
        "print('Sample text : ')\n",
        "print('--------------------------------------------------------------------------------------------')\n",
        "print(ko_grammar_set_raw[50])\n",
        "print('--------------------------------------------------------------------------------------------')"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample text : \n",
            "--------------------------------------------------------------------------------------------\n",
            "위파사나(위빠사나 위빠싸나 Vipassanā विपश्यना 觀 Vipaśyanā) 또는 관(觀)은 불교의 명상법이다. 동국대학교 한글대장경 사이트에서는 비파사나(毘婆舍那) 비바사나(毘婆舍那) 비발사나(毘鉢舍那)로 검색되며 천태종의 지관수행의 관이라고도 하며 보조국사 지눌(知訥)의 정혜쌍수나 계정혜삼학에서 혜(慧)라고도 설명되어 있다. 한글대장경에 비바사나는 불교만이 아니라 고대 인도에서 내내 행해지던 수행법이라고 주석에 설명되어 있고 대승아비달마잡집론 주석에는 사마타와 비발사나가 요가의 방편이라고 설명되어 있다. 수식관 대중부 불교는 계율을 지키고 사마타와 위빳사나 두가지를 수행해야 깨달음을 얻을 수 있다고 설법한다. 일명 계 정 혜. 다른 불경에서는 부정관과 수식관을 수행해야 깨달을 수 있다고 한다. 수식관은 아나빠나사띠를 말하며 사마타 즉 선정 수행이다. 그리고 위빳사나는 선정력을 기반으로 지혜를 닦는 수행이기 때문에 수식관을 통해 닦인 선정의 힘으로 위빳사나 수행을 하게 된다. 부정관 역시 선정을 닦는 사마타 수행이지만 부정관을 통해 몸 혹은 자아에 대한 집착을 버리고 무상 고 무아를 알아 깨달음을 얻을 수 있게 된다는 점에서 지혜를 닦는 수행이기도 하다. 경전 비바사나(毘婆舍那)는 관(觀)을 말한다. 지(止)는 생각을 그치고 마음의 동요를 진정시키며 본원적인 진리에 머무르는 것을 의미하며 관(觀)은 움직이지 않는 마음이 지혜의 작용이 되어 사물을 진리에 합치시키며 올바르게 관찰하는 것을 의미한다. 지는 정(定) 관은 혜(慧)에 해당하며 여기에서는 오른팔에 그 의미를 부여하였다. 마찬가지로 왼팔은 사마타에 해당한다. 비바사나 수행법 이전부터 있었던 인도의 정신집중 수행법. 사마타(奢摩他)는 의역하여 지(止)ㆍ지식(止息)ㆍ적정(寂靜)ㆍ능멸(能滅)이라 번역한다. 산란한 마음을 멈추고 한 가지 대상에 집중하는 수행법이다. 그래서 비바사나가 관(觀) 수행법(修行法)이라면 사마타는 지(止) 수행법이라고 할 수 있다. 이를 합쳐 지관(止觀)이라 하며 불교 천태종(天台宗)의 근본교리이기도 하다. 사마타와 비바사나는 우열의 문제가 아니라 선후의 문제라고 할 수 있다. 우선 사마타에 의해 자아몰입에 들어간 후 지혜를 끌어내어 대상을 보는 비바사나 수행에 들어가는 것이다. 이러한 집중과 관찰은 불도수행에 있어서 동전의 양면과 같다. 여기서 사마타는 정(定)에 해당되고 비바사나는 혜(慧)에 해당된다고 볼 수 있다. 지관불이(止觀不二)라고 해야 할 것이다. 사마타(奢摩他)는 능히 없앤다[能滅] 이름하나니 온갖 번뇌를 없애는 연고며 또 사마타는 능히 조복한다 이름하나니 모든 근의 악하고 선하지 못한 것을 조복하는 연고며 또 사마타는 고요하다 이름하나니 3업을 고요하게 하는 연고며 또 사마타는 멀리 여읜다 이름하나니 중생으로 하여금 5욕락을 멀리 여의게 하는 연고며 또 사마타는 능히 맑힌다 이름하나니 탐욕·성내는 일·어리석음의 흐린 법을 맑히는 연고니라. 이런 뜻으로 선정의 모양[定相]이라 이름하느니라. 비바사나(毘婆舍那)는 바르게 본다[正見] 이름하며 또 분명히 본다[了見] 이름하며 또 능히 본다[能見] 이름하며 두루 본다[遍見]·차례로 본다[次第見]·딴 모양으로 본다[別相見] 이름하나니 이것을 지혜라 하느니라. 가섭아 나는 온갖 하늘과 사람들에게 항상 사마타와 비바사나를 닦아 자기 자신을 조복하라. 세간에는 당연히 믿고 좋아하는 바라문과 거사들이 있어서 사리에 공양하게 될 것이다라는 이런 법을 말하였느니라. 부처님께서 여러 비구들의 생각을 아시고 그들을 타이르셨다. 누구나 사마타(奢摩他)와 비바사나(毘婆舍那)를 닦으면 반드시 번뇌를 다할 수 있고 만약에 그것을 닦지 않는 자라면 번뇌를 다할 수 없으며 또 이미 그것을 보았거나 알았다면 비록 비천한 종성에 태어났더라도 아라한의 과위를 얻을 수 있느니라. 이제 바다라와 같은 자는 알지도 못하고 보지도 못했으므로 비록 수승한 종족에 태어났더라도 아라한을 얻지 못하였으니 그러므로 여래는 평등하게 법을 설하여 치우침이 없느니라. 또 사리자야 보살마하살은 반야바라밀다를 수행하는 까닭에 도의 선교[道善巧]를 닦나니 도의 선교에는 또 두 가지가 있느니라. 어떤 것이 두 가지인가 하면 사마타(奢摩他)와 비발사나(毘鉢舍那)이니 이것을 두 가지라 하느니라. 사마타란. 비발사나란. 상세한 설명보살이 한 가지 법을 성취하면 모든 악한 길과 악한 벗을 떠나 빨리 아뇩다라삼먁삼보리를 증득하게 되느니라. 어떤 것이 그 한 가지인가 하면 이른바 훌륭한 뜻과 좋아함으로 보리의 마음을 일으키는 것이니 이것을 하나라 하느니라. 미륵아 다시 두 가지 법이 있어서 모든 악한 길과 악한 벗을 떠나 빨리 아뇩다라삼먁삼보리를 증득하게 되느니라. 어떤 것이 두 가지 법인가 하면 첫째는 사마타(奢摩他)를 항상 부지런히 닦아 익히는 것이요 둘째는 비발사나(毘鉢舍那)에서 교묘함을 얻는 것이니 이것을 두 가지라 하느니라. ‘사마타관의 뜻을 수순하며 비발사나관의 뜻을 수순한다’는 것은 범어(梵語)인 사마타를 한어(漢語)로 번역하여 지(止)라고 한 것이며 비발사나를 한어로 번역하여 관(觀)이라고 한 것이다. 다만 이제 이 『기신론』을 번역한 이가 방편과 정관(正觀)을 구별하기 위해서 정관에는 그대로 범어를 음사하여 저쪽의 말을 따른 것이다. 청정한 시라에 머무는 것에 의지하여허물없는 지(止)와 관(觀)을 닦아세밀하게 근(根)과 뜻[意]을 보호하면감로(甘露)의 열반 법을 증득할 것이다. 지법(止法)을 닦으면 마음이 조복되고마음이 조복되면 탐욕을 여의나니탐욕을 여읜 이는 해탈을 증득하며해탈을 얻은 이는 마음이 평등하리. 관법(觀法)을 닦으면 지혜가 밝아지며지혜가 맑으면 어리석음 멸하리. 어리석음 멸하면 해탈을 증득하고해탈을 증득하면 마음이 평등하리. 그러므로 너희 비구들아정진하여서 방일하지 말고언제나 시라에 머무는 것에 의지해허물없는 지와 관을 닦아 익히라. 호흡 인도의 요가와 중국의 단전호흡에서는 호흡을 매우 중요시 한다. 우파니샤드이래 인도의 모든 종교에서는 깨달음의 수단으로 요가를 주장하며 석가모니가 수식법을 했다는 것도 요가명상을 한 것이다. 고대 인도인들은 숨인 프라나를 생명의 기운 생명 그 자체 우주의 근본원리하고 보았다. 리그베다의 푸루샤 수크타라는 찬가에는 푸루샤의 숨으로부터 바람이 생겼다고 한다. 우파니샤드에서는 숨을 우주의 원리인 브라흐마와 아트만이라고 했다. 아타르바 베다에는 숨이 세상의 지배자 여신이라며 찬양하는 시가 있다. 이렇게 호흡을 절대시하는 사상적 전통은 인도만이 아니라 인도와 접경한 중국의 도교에서도 마찬가지로서 단전호흡을 하면 신선이 되어 영원히 죽지 않는다고 한다. 이렇게 호흡을 대상으로 하는 수행은 다양한 수행전통에 존재한다. 하지만 부처님은 수식관 혹은 아나빠나사띠 그 자체에서 얻은 선정력 만으로는 깨달음에 이를 수 없다고 판단 세상의 진리를 있는 그대로 보는 위빳사나를 (이번 불법 시대에는) 최초로 시도하셨다. 그런데 일반적으로 지혜를 기르는 위빳사나 수행에는 사마타의 선정력이 기반이 되어야 한다. 아주 특이하게 호흡수행 즉 선정수행 없이 위빳사나 수행만으로 깨달음에 이르는 경우도 있다. 이렇게 깨달음에 이른 사람들을 마른위빳사나를 닦은 자들 이라고 한다. 석가모니 당시 500 아라한 중에 320명이 위빠사나만으로 깨달은 혜해탈자 60명이 사마타의 심해탈과 위빠사나의 혜해탈을 둘 다 깨달은 양분해탈자였다. 따라서 결국 불교 수행은 계행을 철저하게 지키면서 호흡수행 등 사마타 수행 으로 선정을 닦은 후 그 선정력으로 모든 것을 있는 그대로 보고 나와 세상에 대한 무명과 갈애를 타파하여 번뇌를 소멸하고 열반에 이르는 지혜를 기르고 완성하는 위빳사나 수행을 하는 것이다. 즉 불교수행에서 호흡이란 지혜의 기반이 되는 선정을 기르기 위한 선정수행의 대상이 된다. 혹은 대념처경의 신수심법 중 신념처이다. 다른 종교에도 호흡수행은 존재한다. 하지만 불교의 호흡수행을 통한 선정과 다른 종교의 호흡수행을 통한 선정은 다르다. 다른 종교의 호흡수행은 수행을 통한 고요함과 선정에 끝난다. 반면에 불교의 호흡수행은 수행을 통해 얻은 고요함과 선정력이 갈애와 무명을 타파하고 깨달음에 이르게 하는 지혜로 전환되는 위빳사나 수행과 연결되는 정견의 메커니즘 속에서 이루어진다. 그리고 이 전체 수행메커니즘은 철저하게 불교의 정견에 기반을 두고 행해진다. 그래서 불교에서는 바른 삼매(정견을 바탕에 둔 삼매)와 삿된 삼매(정견을 바탕으로 두지 않은 삼매)로 호흡수행이나 기타 사마타 수행을 통한 삼매를 구분한다. 대승불교 요가불교 3세기 용수 스님이 대승불교를 만들어 이전의 종파를 소승불교라 하여 무시했다. 그와 함께 수행 보다는 믿음을 중시하며 석가모니의 수식법을 소승의 수행법이라 하여 멀리했다. 그러나 이러한 수행배제 믿음중시의 초기대승불교의 움직임은 다시 중기대승불교에 이르러 변한다. 4세기 무착 스님은 요가불교를 만들어 중기대승불교를 새로 여는데 요가불교에서는 호흡법인 요가를 다시 중시한다. 석가모니가 6년간 수행했다는 호흡법인 수식법도 요가였다. 5세기 보리달마에 의해 중국으로 불교명상법이 전수된 이후 12세기에 대혜종고가 간화선을 만들었는데 호흡법이 일체 배제되었다. 그러나 20세기 한국의 불교명상을 이끈 전강 스님은 간화선에 다시 호흡법을 포함시켰다. 조동종 12세기 중국 임제종에서 간화선이 만들어질 당시 조동종에서는 전통의 수식관을 통한 묵조선을 강조하여 서로 비난을 해가면서 대립하였다. 후에 조동종이 쇠퇴하고 임제종이 동북아 불교를 장악해 오늘날에 이르고 있다. 간화선 요즘 한국불교에서는 간화선 대 위파사나의 비교가 언론에 보도되기도 하였다. 수식법 - 석가모니가 6년간 수식법을 하여 깨달음을 얻었다. 석가모니가 만든 명상법이다. 호흡법이 중심이다. 안반수의경이 핵심경전이다. 간화선 - 12세기 중국의 대혜종고 스님이 만든 명상법이다. 호흡법이 빠졌다. 각주 더 보기 삼매 · 삼마지지관사마타관조4제현관6현관명상조식법 - 단전호흡이라고도 한다. 요가 또는 수식법이 인도의 영생불사 방법이라면 조식법은 중국의 영생불사 방법이다. 위파사나 명상센터\n",
            "--------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqJHLPaReHx-"
      },
      "source": [
        "문장으로 잘라 낸다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "_b8WKqYXeHx_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41ca9dd2-0636-4dfc-9992-8f59582ced6f"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "#Split the document into sentences\n",
        "ko_grammar_sentences = []\n",
        "for document in ko_grammar_set_raw:\n",
        "    ko_grammar_sentences += nltk.sent_tokenize(document)\n",
        "\n",
        "print(\"Num sentences:\", len(ko_grammar_sentences))"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Num sentences: 9723\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scF3dYlQWv62"
      },
      "source": [
        "f = open(\"ko_sentence_set.txt\", \"a\")\r\n",
        "for t in ko_grammar_sentences:\r\n",
        "    f.write(t+'\\n')\r\n",
        "f.close()"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doo-8nFPThrQ"
      },
      "source": [
        "# false 문장을 만들기 위해 shffle 함수 준비\r\n",
        "import random\r\n",
        "\r\n",
        "def shuffling(txt):\r\n",
        "    txt_list = txt.split(' ')\r\n",
        "    random.shuffle(txt_list)\r\n",
        "    return ' '.join(txt_list)\r\n",
        "\r\n",
        "# true 문장, false 문장의 생성\r\n",
        "ko_grammar_dataset = []\r\n",
        "for txt in ko_grammar_sentences:\r\n",
        "    ko_grammar_dataset.append([txt,1])\r\n",
        "    ko_grammar_dataset.append([shuffling(txt),0])\r\n",
        "    \r\n",
        "# dataset을 전체적으로 다시 썩는다.\r\n",
        "random.shuffle(ko_grammar_dataset)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdj8wrm8eHyC"
      },
      "source": [
        "형태소 분리하여 모든 문장을 형태소 Code로 변환 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "OY8VTXraeHyF"
      },
      "source": [
        "from konlpy.tag import Twitter\n",
        "twitter = Twitter()\n",
        "\n",
        "# 형태소 Code table의 구성\n",
        "\n",
        "_MAX_MORP_LENGTH = 128\n",
        "_PADDING_CODE = 0  # padding code\n",
        "_MISMATCH_CODE = 1 # mismatch word code               ex) @@@\n",
        "_MISMATCH_WORD = '@@@' # 이거 아래에서 쓴다.\n",
        "\n",
        "morpheme_table = {}\n",
        "morp_code = _MISMATCH_CODE+1\n",
        "morpheme_table['Pad'] = _PADDING_CODE \n",
        "morpheme_table['Mst'] = _MISMATCH_CODE \n",
        "for sentence in ko_grammar_sentences:\n",
        "    morphemes = twitter.pos(sentence)\n",
        "    for (word,morp) in morphemes:\n",
        "        if morp in morpheme_table:\n",
        "            pass\n",
        "        else:\n",
        "            morpheme_table[morp] = morp_code\n",
        "            morp_code += 1\n"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0IzjdedJf09"
      },
      "source": [
        "\r\n",
        "morpheme_table['URL']=21 # 빠진거 채워넣음..."
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "asiTeu8SeHyG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c95d5dc-9fa8-4c91-d8bb-1e163a8d9e12"
      },
      "source": [
        "print('Korean morpheme code table')\n",
        "print('----------------------------------------------------------')\n",
        "print('  Morpheme        Code')\n",
        "print('')\n",
        "for morp in morpheme_table.keys():\n",
        "    print(f' {morp.ljust(15)}   {morpheme_table[morp]}')\n",
        "print('----------------------------------------------------------')\n"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Korean morpheme code table\n",
            "----------------------------------------------------------\n",
            "  Morpheme        Code\n",
            "\n",
            " Pad               0\n",
            " Mst               1\n",
            " Noun              2\n",
            " Punctuation       3\n",
            " Foreign           4\n",
            " Alpha             5\n",
            " Adjective         6\n",
            " Josa              7\n",
            " Verb              8\n",
            " Modifier          9\n",
            " Adverb            10\n",
            " Suffix            11\n",
            " Determiner        12\n",
            " Number            13\n",
            " VerbPrefix        14\n",
            " Conjunction       15\n",
            " Exclamation       16\n",
            " Eomi              17\n",
            " PreEomi           18\n",
            " KoreanParticle    19\n",
            " Hashtag           20\n",
            " URL               21\n",
            "----------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "RQfjoYbceHyH"
      },
      "source": [
        "# morpheme 코드 변환기 준비\n",
        "def morpheme_encode(sentence):\n",
        "    encode=[]\n",
        "    morphemes = twitter.pos(sentence)\n",
        "    for (word,morp) in morphemes:\n",
        "        encode.append(_MISMATCH_CODE if word==_MISMATCH_WORD else morpheme_table[morp])\n",
        "\n",
        "    if len(encode) <= _MAX_MORP_LENGTH:\n",
        "        encode = encode + [_PADDING_CODE for i in range(_MAX_MORP_LENGTH-len(encode))]\n",
        "    else:\n",
        "        encode = encode[:_MAX_MORP_LENGTH]       \n",
        "    return encode\n",
        "\n",
        "#true / false 문장을 morpheme 코드로 모두 변환\n",
        "ko_morpheme_x = []\n",
        "ko_morpheme_y = []\n",
        "for (txt,label) in ko_grammar_dataset:\n",
        "    ko_morpheme_x.append(morpheme_encode(txt))\n",
        "    ko_morpheme_y.append([label])\n",
        "\n",
        "ko_morpheme_x = np.asarray(ko_morpheme_x)\n",
        "ko_morpheme_y = np.asarray(ko_morpheme_y)  "
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Knw_P_zswIXC"
      },
      "source": [
        "trainset 과 testset의 분리 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5vbAwY4Znc2",
        "outputId": "237840f2-d8ca-4817-ea75-0d360d46c082"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "# 20%를 testset으로 사용.,,\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(ko_morpheme_x,ko_morpheme_y,test_size=0.2)\r\n",
        "\r\n",
        "print(f'Shape of X_train;{X_train.shape}')\r\n",
        "print(f'Shape of X_test ;{X_test.shape}')\r\n",
        "print(f'Shape of y_train;{y_train.shape}')\r\n",
        "print(f'Shape of y_test ;{y_test.shape}')"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of X_train;(15556, 128)\n",
            "Shape of X_test ;(3890, 128)\n",
            "Shape of y_train;(15556, 1)\n",
            "Shape of y_test ;(3890, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Es6toINDwSQa"
      },
      "source": [
        "Model을 생성하고 학습 시킨다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxG8Wtf1cShr",
        "outputId": "f35ff903-2624-49fe-ec62-272d42e081c1"
      },
      "source": [
        "# model build\r\n",
        "\r\n",
        "morpheme_model = Sequential()\r\n",
        "morpheme_model.add(Dense(500, input_dim=128, activation= \"relu\"))\r\n",
        "morpheme_model.add(Dense(100, activation= \"relu\"))\r\n",
        "morpheme_model.add(Dense(50, activation= \"relu\"))\r\n",
        "morpheme_model.add(Dense(1))\r\n",
        "morpheme_model.summary() #Print model Summary\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 500)               64500     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 100)               50100     \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 50)                5050      \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 119,701\n",
            "Trainable params: 119,701\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "487/487 [==============================] - 4s 4ms/step - loss: 0.3580 - mean_squared_error: 0.3580\n",
            "Epoch 2/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.2464 - mean_squared_error: 0.2464\n",
            "Epoch 3/100\n",
            "487/487 [==============================] - 2s 3ms/step - loss: 0.2337 - mean_squared_error: 0.2337\n",
            "Epoch 4/100\n",
            "487/487 [==============================] - 2s 5ms/step - loss: 0.2236 - mean_squared_error: 0.2236\n",
            "Epoch 5/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.2186 - mean_squared_error: 0.2186\n",
            "Epoch 6/100\n",
            "487/487 [==============================] - 2s 3ms/step - loss: 0.2042 - mean_squared_error: 0.2042\n",
            "Epoch 7/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1963 - mean_squared_error: 0.1963\n",
            "Epoch 8/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1844 - mean_squared_error: 0.1844\n",
            "Epoch 9/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1746 - mean_squared_error: 0.1746\n",
            "Epoch 10/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1670 - mean_squared_error: 0.1670\n",
            "Epoch 11/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1570 - mean_squared_error: 0.1570\n",
            "Epoch 12/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1471 - mean_squared_error: 0.1471\n",
            "Epoch 13/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1396 - mean_squared_error: 0.1396\n",
            "Epoch 14/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1333 - mean_squared_error: 0.1333\n",
            "Epoch 15/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1257 - mean_squared_error: 0.1257\n",
            "Epoch 16/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1180 - mean_squared_error: 0.1180\n",
            "Epoch 17/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1105 - mean_squared_error: 0.1105\n",
            "Epoch 18/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.1028 - mean_squared_error: 0.1028\n",
            "Epoch 19/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0984 - mean_squared_error: 0.0984\n",
            "Epoch 20/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0910 - mean_squared_error: 0.0910\n",
            "Epoch 21/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0841 - mean_squared_error: 0.0841\n",
            "Epoch 22/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0825 - mean_squared_error: 0.0825\n",
            "Epoch 23/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0747 - mean_squared_error: 0.0747\n",
            "Epoch 24/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0688 - mean_squared_error: 0.0688\n",
            "Epoch 25/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0685 - mean_squared_error: 0.0685\n",
            "Epoch 26/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0627 - mean_squared_error: 0.0627\n",
            "Epoch 27/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0626 - mean_squared_error: 0.0626\n",
            "Epoch 28/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0599 - mean_squared_error: 0.0599\n",
            "Epoch 29/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0590 - mean_squared_error: 0.0590\n",
            "Epoch 30/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0555 - mean_squared_error: 0.0555\n",
            "Epoch 31/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0553 - mean_squared_error: 0.0553\n",
            "Epoch 32/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0501 - mean_squared_error: 0.0501\n",
            "Epoch 33/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0558 - mean_squared_error: 0.0558\n",
            "Epoch 34/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0495 - mean_squared_error: 0.0495\n",
            "Epoch 35/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0476 - mean_squared_error: 0.0476\n",
            "Epoch 36/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0501 - mean_squared_error: 0.0501\n",
            "Epoch 37/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0469 - mean_squared_error: 0.0469\n",
            "Epoch 38/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0442 - mean_squared_error: 0.0442\n",
            "Epoch 39/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0473 - mean_squared_error: 0.0473\n",
            "Epoch 40/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0428 - mean_squared_error: 0.0428\n",
            "Epoch 41/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0454 - mean_squared_error: 0.0454\n",
            "Epoch 42/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0484 - mean_squared_error: 0.0484\n",
            "Epoch 43/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0431 - mean_squared_error: 0.0431\n",
            "Epoch 44/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0413 - mean_squared_error: 0.0413\n",
            "Epoch 45/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0411 - mean_squared_error: 0.0411\n",
            "Epoch 46/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0397 - mean_squared_error: 0.0397\n",
            "Epoch 47/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0408 - mean_squared_error: 0.0408\n",
            "Epoch 48/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0374 - mean_squared_error: 0.0374\n",
            "Epoch 49/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0353 - mean_squared_error: 0.0353\n",
            "Epoch 50/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0358 - mean_squared_error: 0.0358\n",
            "Epoch 51/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0335 - mean_squared_error: 0.0335\n",
            "Epoch 52/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0401 - mean_squared_error: 0.0401\n",
            "Epoch 53/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0380 - mean_squared_error: 0.0380\n",
            "Epoch 54/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0374 - mean_squared_error: 0.0374\n",
            "Epoch 55/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0374 - mean_squared_error: 0.0374\n",
            "Epoch 56/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0352 - mean_squared_error: 0.0352\n",
            "Epoch 57/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0338 - mean_squared_error: 0.0338\n",
            "Epoch 58/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0369 - mean_squared_error: 0.0369\n",
            "Epoch 59/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0349 - mean_squared_error: 0.0349\n",
            "Epoch 60/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0310 - mean_squared_error: 0.0310\n",
            "Epoch 61/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0315 - mean_squared_error: 0.0315\n",
            "Epoch 62/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0337 - mean_squared_error: 0.0337\n",
            "Epoch 63/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0366 - mean_squared_error: 0.0366\n",
            "Epoch 64/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0337 - mean_squared_error: 0.0337\n",
            "Epoch 65/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0327 - mean_squared_error: 0.0327\n",
            "Epoch 66/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0308 - mean_squared_error: 0.0308\n",
            "Epoch 67/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0312 - mean_squared_error: 0.0312\n",
            "Epoch 68/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0334 - mean_squared_error: 0.0334\n",
            "Epoch 69/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0342 - mean_squared_error: 0.0342\n",
            "Epoch 70/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0314 - mean_squared_error: 0.0314\n",
            "Epoch 71/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0318 - mean_squared_error: 0.0318\n",
            "Epoch 72/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0294 - mean_squared_error: 0.0294\n",
            "Epoch 73/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0292 - mean_squared_error: 0.0292\n",
            "Epoch 74/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0274 - mean_squared_error: 0.0274\n",
            "Epoch 75/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0320 - mean_squared_error: 0.0320\n",
            "Epoch 76/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0308 - mean_squared_error: 0.0308\n",
            "Epoch 77/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0302 - mean_squared_error: 0.0302\n",
            "Epoch 78/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0265 - mean_squared_error: 0.0265\n",
            "Epoch 79/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0277 - mean_squared_error: 0.0277\n",
            "Epoch 80/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0321 - mean_squared_error: 0.0321\n",
            "Epoch 81/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0323 - mean_squared_error: 0.0323\n",
            "Epoch 82/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0294 - mean_squared_error: 0.0294\n",
            "Epoch 83/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0261 - mean_squared_error: 0.0261\n",
            "Epoch 84/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0268 - mean_squared_error: 0.0268\n",
            "Epoch 85/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0271 - mean_squared_error: 0.0271\n",
            "Epoch 86/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0278 - mean_squared_error: 0.0278\n",
            "Epoch 87/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0357 - mean_squared_error: 0.0357\n",
            "Epoch 88/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0277 - mean_squared_error: 0.0277\n",
            "Epoch 89/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0271 - mean_squared_error: 0.0271\n",
            "Epoch 90/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0264 - mean_squared_error: 0.0264\n",
            "Epoch 91/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0294 - mean_squared_error: 0.0294\n",
            "Epoch 92/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0247 - mean_squared_error: 0.0247\n",
            "Epoch 93/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0257 - mean_squared_error: 0.0257\n",
            "Epoch 94/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0296 - mean_squared_error: 0.0296\n",
            "Epoch 95/100\n",
            "487/487 [==============================] - 2s 3ms/step - loss: 0.0295 - mean_squared_error: 0.0295\n",
            "Epoch 96/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0266 - mean_squared_error: 0.0266\n",
            "Epoch 97/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0277 - mean_squared_error: 0.0277\n",
            "Epoch 98/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0300 - mean_squared_error: 0.0300\n",
            "Epoch 99/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0244 - mean_squared_error: 0.0244\n",
            "Epoch 100/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0273 - mean_squared_error: 0.0273\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3064520a58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOA1zDvoK2HE",
        "outputId": "fcb8da35-36a5-49e9-9472-d81aea6299d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "morpheme_model.compile(loss= \"mean_squared_error\" , optimizer=\"adam\", metrics=[\"mean_squared_error\"])\r\n",
        "morpheme_model.fit(X_train, y_train, epochs=100)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "487/487 [==============================] - 2s 3ms/step - loss: 0.0259 - mean_squared_error: 0.0259\n",
            "Epoch 2/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0269 - mean_squared_error: 0.0269\n",
            "Epoch 3/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0255 - mean_squared_error: 0.0255\n",
            "Epoch 4/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0261 - mean_squared_error: 0.0261\n",
            "Epoch 5/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0249 - mean_squared_error: 0.0249\n",
            "Epoch 6/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0248 - mean_squared_error: 0.0248\n",
            "Epoch 7/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0257 - mean_squared_error: 0.0257\n",
            "Epoch 8/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0243 - mean_squared_error: 0.0243\n",
            "Epoch 9/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0221 - mean_squared_error: 0.0221\n",
            "Epoch 10/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0244 - mean_squared_error: 0.0244\n",
            "Epoch 11/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0250 - mean_squared_error: 0.0250\n",
            "Epoch 12/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0238 - mean_squared_error: 0.0238\n",
            "Epoch 13/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0263 - mean_squared_error: 0.0263\n",
            "Epoch 14/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0254 - mean_squared_error: 0.0254\n",
            "Epoch 15/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0222 - mean_squared_error: 0.0222\n",
            "Epoch 16/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0255 - mean_squared_error: 0.0255\n",
            "Epoch 17/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0303 - mean_squared_error: 0.0303\n",
            "Epoch 18/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0232 - mean_squared_error: 0.0232\n",
            "Epoch 19/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0238 - mean_squared_error: 0.0238\n",
            "Epoch 20/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0217 - mean_squared_error: 0.0217\n",
            "Epoch 21/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0242 - mean_squared_error: 0.0242\n",
            "Epoch 22/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0247 - mean_squared_error: 0.0247\n",
            "Epoch 23/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0226 - mean_squared_error: 0.0226\n",
            "Epoch 24/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0211 - mean_squared_error: 0.0211\n",
            "Epoch 25/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0259 - mean_squared_error: 0.0259\n",
            "Epoch 26/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0235 - mean_squared_error: 0.0235\n",
            "Epoch 27/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0215 - mean_squared_error: 0.0215\n",
            "Epoch 28/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0234 - mean_squared_error: 0.0234\n",
            "Epoch 29/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0277 - mean_squared_error: 0.0277\n",
            "Epoch 30/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0223 - mean_squared_error: 0.0223\n",
            "Epoch 31/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0230 - mean_squared_error: 0.0230\n",
            "Epoch 32/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0240 - mean_squared_error: 0.0240\n",
            "Epoch 33/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0239 - mean_squared_error: 0.0239\n",
            "Epoch 34/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0217 - mean_squared_error: 0.0217\n",
            "Epoch 35/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0229 - mean_squared_error: 0.0229\n",
            "Epoch 36/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0252 - mean_squared_error: 0.0252\n",
            "Epoch 37/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0238 - mean_squared_error: 0.0238\n",
            "Epoch 38/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0196 - mean_squared_error: 0.0196\n",
            "Epoch 39/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0205 - mean_squared_error: 0.0205\n",
            "Epoch 40/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0260 - mean_squared_error: 0.0260\n",
            "Epoch 41/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0230 - mean_squared_error: 0.0230\n",
            "Epoch 42/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0238 - mean_squared_error: 0.0238\n",
            "Epoch 43/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0249 - mean_squared_error: 0.0249\n",
            "Epoch 44/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0244 - mean_squared_error: 0.0244\n",
            "Epoch 45/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0244 - mean_squared_error: 0.0244\n",
            "Epoch 46/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0200 - mean_squared_error: 0.0200\n",
            "Epoch 47/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0183 - mean_squared_error: 0.0183\n",
            "Epoch 48/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0205 - mean_squared_error: 0.0205\n",
            "Epoch 49/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0232 - mean_squared_error: 0.0232\n",
            "Epoch 50/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0244 - mean_squared_error: 0.0244\n",
            "Epoch 51/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0205 - mean_squared_error: 0.0205\n",
            "Epoch 52/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0251 - mean_squared_error: 0.0251\n",
            "Epoch 53/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0213 - mean_squared_error: 0.0213\n",
            "Epoch 54/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0213 - mean_squared_error: 0.0213\n",
            "Epoch 55/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0210 - mean_squared_error: 0.0210\n",
            "Epoch 56/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0230 - mean_squared_error: 0.0230\n",
            "Epoch 57/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0232 - mean_squared_error: 0.0232\n",
            "Epoch 58/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0241 - mean_squared_error: 0.0241\n",
            "Epoch 59/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0190 - mean_squared_error: 0.0190\n",
            "Epoch 60/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0202 - mean_squared_error: 0.0202\n",
            "Epoch 61/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0227 - mean_squared_error: 0.0227\n",
            "Epoch 62/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0208 - mean_squared_error: 0.0208\n",
            "Epoch 63/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0202 - mean_squared_error: 0.0202\n",
            "Epoch 64/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0199 - mean_squared_error: 0.0199\n",
            "Epoch 65/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0231 - mean_squared_error: 0.0231\n",
            "Epoch 66/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0227 - mean_squared_error: 0.0227\n",
            "Epoch 67/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0211 - mean_squared_error: 0.0211\n",
            "Epoch 68/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0234 - mean_squared_error: 0.0234\n",
            "Epoch 69/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0195 - mean_squared_error: 0.0195\n",
            "Epoch 70/100\n",
            "487/487 [==============================] - 2s 3ms/step - loss: 0.0185 - mean_squared_error: 0.0185\n",
            "Epoch 71/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0214 - mean_squared_error: 0.0214\n",
            "Epoch 72/100\n",
            "487/487 [==============================] - 2s 3ms/step - loss: 0.0215 - mean_squared_error: 0.0215\n",
            "Epoch 73/100\n",
            "487/487 [==============================] - 2s 4ms/step - loss: 0.0214 - mean_squared_error: 0.0214\n",
            "Epoch 74/100\n",
            "487/487 [==============================] - 2s 3ms/step - loss: 0.0194 - mean_squared_error: 0.0194\n",
            "Epoch 75/100\n",
            "487/487 [==============================] - 2s 4ms/step - loss: 0.0205 - mean_squared_error: 0.0205\n",
            "Epoch 76/100\n",
            "487/487 [==============================] - 2s 3ms/step - loss: 0.0215 - mean_squared_error: 0.0215\n",
            "Epoch 77/100\n",
            "487/487 [==============================] - 2s 4ms/step - loss: 0.0212 - mean_squared_error: 0.0212\n",
            "Epoch 78/100\n",
            "487/487 [==============================] - 2s 3ms/step - loss: 0.0222 - mean_squared_error: 0.0222\n",
            "Epoch 79/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0251 - mean_squared_error: 0.0251\n",
            "Epoch 80/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0214 - mean_squared_error: 0.0214\n",
            "Epoch 81/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0197 - mean_squared_error: 0.0197\n",
            "Epoch 82/100\n",
            "487/487 [==============================] - 2s 3ms/step - loss: 0.0194 - mean_squared_error: 0.0194\n",
            "Epoch 83/100\n",
            "487/487 [==============================] - 2s 4ms/step - loss: 0.0228 - mean_squared_error: 0.0228\n",
            "Epoch 84/100\n",
            "487/487 [==============================] - 2s 3ms/step - loss: 0.0203 - mean_squared_error: 0.0203\n",
            "Epoch 85/100\n",
            "487/487 [==============================] - 2s 3ms/step - loss: 0.0225 - mean_squared_error: 0.0225\n",
            "Epoch 86/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0192 - mean_squared_error: 0.0192\n",
            "Epoch 87/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0186 - mean_squared_error: 0.0186\n",
            "Epoch 88/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0226 - mean_squared_error: 0.0226\n",
            "Epoch 89/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0221 - mean_squared_error: 0.0221\n",
            "Epoch 90/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0213 - mean_squared_error: 0.0213\n",
            "Epoch 91/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0183 - mean_squared_error: 0.0183\n",
            "Epoch 92/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0193 - mean_squared_error: 0.0193\n",
            "Epoch 93/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0267 - mean_squared_error: 0.0267\n",
            "Epoch 94/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0215 - mean_squared_error: 0.0215\n",
            "Epoch 95/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0192 - mean_squared_error: 0.0192\n",
            "Epoch 96/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0182 - mean_squared_error: 0.0182\n",
            "Epoch 97/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0206 - mean_squared_error: 0.0206\n",
            "Epoch 98/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0209 - mean_squared_error: 0.0209\n",
            "Epoch 99/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0189 - mean_squared_error: 0.0189\n",
            "Epoch 100/100\n",
            "487/487 [==============================] - 1s 3ms/step - loss: 0.0175 - mean_squared_error: 0.0175\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f303efac208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvrVAccWpwAm",
        "outputId": "ed3ba855-5afc-4e27-d92b-501077c92618"
      },
      "source": [
        "# 학습결과 확인\r\n",
        "\r\n",
        "results = morpheme_model.evaluate(X_test, y_test)\r\n",
        "print(results)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "122/122 [==============================] - 0s 1ms/step - loss: 0.2499 - mean_squared_error: 0.2499\n",
            "[0.2499193251132965, 0.2499193251132965]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5i2wtQvsWyY",
        "outputId": "751233f9-f9ce-4128-89ff-c57528575c80"
      },
      "source": [
        "# 학습 결과의 확인\r\n",
        "predicts = morpheme_model.predict(X_test)\r\n",
        "predicts = np.asarray(predicts)\r\n",
        "predicts = [ 1 if x>0.5 else 0 for [x] in predicts]\r\n",
        "y = np.asarray(y_test)\r\n",
        "y = y.reshape(y.shape[0],)\r\n",
        "\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "print(classification_report(y, predicts))"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.69      0.69      1916\n",
            "           1       0.70      0.71      0.71      1974\n",
            "\n",
            "    accuracy                           0.70      3890\n",
            "   macro avg       0.70      0.70      0.70      3890\n",
            "weighted avg       0.70      0.70      0.70      3890\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdnf-b-ywwup"
      },
      "source": [
        "# 모델의 저장\r\n",
        "\r\n",
        "morpheme_model.save('morpheme_model.h5')\r\n",
        "morpheme_model.save_weights('morpheme_model.weights')"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bkJ7Abs2XG3"
      },
      "source": [
        "def morpheme_discriminator(queries):\r\n",
        "    # queries : 복수의 문장의 2차원 배열 (None,1)\r\n",
        "    # return : 결과 score 배열 (None,)\r\n",
        "    x_codes = []\r\n",
        "    for query in queries:\r\n",
        "        x_codes.append(morpheme_encode(query))\r\n",
        "    scores = morpheme_model.predict(x_codes)\r\n",
        "    return scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgjWTMGt3SC6"
      },
      "source": [
        "ko_grammar_sentences[100:102]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KU7aOemq3WqY"
      },
      "source": [
        "morpheme_discriminator(ko_grammar_sentences[100:102])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZF3lwa70lB0"
      },
      "source": [
        "## 문서 유사도 Discriminator 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKNnwvwx0kLN"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\r\n",
        "from transformers import BertTokenizer\r\n",
        "\r\n",
        "if DO_ALL:\r\n",
        "    # embedder download...\r\n",
        "    embedder = SentenceTransformer('xlm-r-large-en-ko-nli-ststb')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiICVLIm1SZv"
      },
      "source": [
        "\r\n",
        "# 생성된 문장의 원문 유사도를 측정하기 위한 함수\r\n",
        "\r\n",
        "import scipy\r\n",
        "#print(doc_emb)\r\n",
        "def similarity_discriminator(queries,org_embedding):\r\n",
        "    # queries : 복수의 문장의 2차원 배열 (None,1)\r\n",
        "    # org_embedding : 비교 대상의 원문 embedding 1차원 배열 (1,)\r\n",
        "    # return : 결과 score 배열 (None,)\r\n",
        "    total_score = 0\r\n",
        "    query_embeddings = embedder.encode(queries,show_progress_bar=False)\r\n",
        "    for query, query_embedding in zip(queries, query_embeddings):\r\n",
        "        distances = scipy.spatial.distance.cdist([query_embedding], [org_embedding], \"cosine\")[0]\r\n",
        "        results = zip(range(len(distances)), distances)\r\n",
        "        for idx, distance in results:\r\n",
        "            total_score += 1-distance\r\n",
        "    return total_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrAYyyNP4HXf"
      },
      "source": [
        "# 원문의 embedding...\r\n",
        "\r\n",
        "org_text_emb = embedder.encode([org_text],show_progress_bar=False)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXrhGn_W4VZF"
      },
      "source": [
        "org_text_emb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gypz5XEH4cUv"
      },
      "source": [
        "## 원문 base의 generator 준비 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p66D6hso4t3N"
      },
      "source": [
        "org_term_set = org_text.split(' ')\r\n",
        "\r\n",
        "_MAX_GEN_TOKEN = 40\r\n",
        "_NOISE_DIM = len(org_term_set)\r\n",
        "\r\n",
        "word_table = {}\r\n",
        "\r\n",
        "for index, word in zip(range(len(org_term_set)),org_term_set):\r\n",
        "    word_table[index] = word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rv5g266R42Y6"
      },
      "source": [
        "print('Token table of origin text')\r\n",
        "print('---------------------------------------------')\r\n",
        "print(' Code         Token      ')\r\n",
        "print('')\r\n",
        "for k in word_table.keys():\r\n",
        "  print( f'  {str(k).ljust(8)}    {word_table[k]}')\r\n",
        "print('---------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lejMbMq654is"
      },
      "source": [
        "_MAX_GEN_TOKEN = 40\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E04x981k7Gzh"
      },
      "source": [
        "noise = np.random.rand(_NOISE_DIM,)\r\n",
        "def text_gen(noise):\r\n",
        "    text = \"\"\r\n",
        "    sorted_noise = np.sort(noise)[::-1]\r\n",
        "    order = np.where(noise > sorted_noise[_MAX_GEN_TOKEN])[0][:_MAX_GEN_TOKEN]\r\n",
        "    assert len(order) == _MAX_GEN_TOKEN\r\n",
        "    for k in order:\r\n",
        "        text += word_table[k]+' '    \r\n",
        "    return text,order \r\n",
        "\r\n",
        "text,order = text_gen(noise)\r\n",
        "\r\n",
        "text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sweJG7zR_MAb"
      },
      "source": [
        "'''\r\n",
        "for i in range(10):\r\n",
        "    noise = tf.random.normal([1,_NOISE_DIM])\r\n",
        "    texts,embeddings,compratios,morpcodes = generator(noise, training=True)\r\n",
        "    p_score = 0\r\n",
        "    for txt in texts:\r\n",
        "        embedding = embedder.encode([txt.numpy().decode('utf-8')],show_progress_bar=False)\r\n",
        "        distances = scipy.spatial.distance.cdist(embedding, [org_text_emb], \"cosine\")[0]\r\n",
        "        score = 1-distances[0]\r\n",
        "        reward = p_score - score\r\n",
        "        if p_score == 0:\r\n",
        "            reward = -reward\r\n",
        "        print(f'score:{score} reward:{reward}') # 1-distance\r\n",
        "        p_score = score\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
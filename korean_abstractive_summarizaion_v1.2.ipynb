{"cells":[{"metadata":{"id":"WdM3q73ReHxs"},"cell_type":"markdown","source":"# **Korean Summarizer Using Multiple Discriminators**\n\n참조 : https://www.kaggle.com/shivamb/beginners-guide-to-text-generation-using-lstms\n\n참조 : https://github.com/williamSYSU/TextGAN-PyTorch\n\n* 2020년12월27일 v1.0 완전히 실패...\n* 2020년12월27일 오후 Generator 다시 만들고... 역시 실패 한듯... v1.2 완전히 실패...\n* 문법 Discriminator를 먼저 학습하고... transfer learning을 사용해 보자.\n* 그래도 역시 Generator는 다시 만들어야 할 듯."},{"metadata":{"id":"nNBNW5dMZ13G","trusted":true},"cell_type":"code","source":"DO_ALL = False # 전체 실행하면서 시간 걸리는 걸 Pass 하려면 이걸 False ...\n","execution_count":201,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"id":"FlvsCFJaeHxt","outputId":"f7c14366-e5d9-4da7-8fad-0dac65f47dd4"},"cell_type":"code","source":"\nif DO_ALL:\n    !pip install sentence-transformers==0.3.0\n    !pip install transformers==3.0.2\n    !pip install wikipedia\n    !pip install konlpy","execution_count":202,"outputs":[]},{"metadata":{"trusted":true,"id":"Em1oCkJceHxz"},"cell_type":"code","source":"# keras module for building LSTM \nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding, LSTM, Dense, Dropout\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nfrom tensorflow.python.framework import tensor_shape\nimport keras.utils as ku \n\n# set seeds for reproducability\nfrom tensorflow.random import set_seed\nfrom numpy.random import seed\nset_seed(2)\nseed(1)\n\nimport pandas as pd\nimport numpy as np\nimport string, os \n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(action='ignore', category=FutureWarning)","execution_count":203,"outputs":[]},{"metadata":{"id":"dFdj0QfSeHx1"},"cell_type":"markdown","source":"# 학습을 위한 데이터 준비"},{"metadata":{"id":"94NlJEeHeHx3"},"cell_type":"markdown","source":"네이버 뉴스에서 아무거나 하나 Text를 얻어옴\n\n이것을 '요약' 목표"},{"metadata":{"trusted":true,"id":"lz5XtC9MeHx5"},"cell_type":"code","source":"org_text = \"\"\"\n옛날 어느 집에 귀여운 여자 아기가 태어났어요.\n\n아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요.\n\n그러던 어느날, 소녀의 어머니가 병이들어\n\n그만 세상을 떠나고 말았어요.\n\n \n\n소녀의 아버지는 홀로 남은 소녀가 걱정되었어요.\n\n그래서 얼마 후 새어머니를 맞이했어요.\n\n새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요.\n\n그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요.\n\n새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요.\n\n \n\n그런데 이번에는 아버지마저 돌아가셨어요.\n\n소녀는 하녀처럼 하루 종일 쓸고, 닦고, 집안일을 도맡아 했어요.\n\n해도 해도 끝이 없는 집안일이 힘들어 지칠때면\n\n난롯가에 앉아서 잠시 쉬곤 했지요.\n\n \n\n\"엄마, 저애를 신데렐라라고 불러야겠어요.\"\n\n\"온통 재투성이잖아요. 호호호!\" 두 언니는 소녀를 놀려 댔어요.\n\n \n\n어느 날, 왕궁에서 무도회가 열렸어요.\n\n신데렐라의 집에도 초대장이 왔어요.\n\n \n\n새어머니는 언니들을 데리고 무도회장으로 떠났어요.\n\n신데렐라도 무도회에 가고 싶었어요.\n\n혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요.\n\n\"신데렐라, 너도 무도회에 가고 싶니?\"\n\n신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요.\n\n \n\n\"내가 너를 무도회에 보내주마\n\n호박 한개와 생쥐 두마리, 도마뱀을 구해 오렴.\"\n\n  \n\n마법사 할머니가 주문을 외웠어요.\n\n그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금 마차로 변했어요.\n\n이번에는 생쥐와 도마뱀을 건드렸어요.\n\n그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했답니다.\n\n \n\n신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요.\n\n\"신데렐라, 발을 내밀어 보거라.\"\n\n할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요\n\n\"신데렐라, 밤 열두시가 되면 모든게 처음대로 돌아간단다.\n\n황금 마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼.\n\n그러니까 바늗시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지?\"\n\n 왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요.\n\n완자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고,\n\n신데렐라하고만 춤을 추었어요.\n\n신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요.\n\n땡, 땡, 땡...... 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요.\n\n \n\n신데렐라가 허둥지둥 왕궁을 빠져나가는데,\n\n유리 구두 한 짝이 벗겨졌어요.\n\n하지만 구두를 주울 틈이 없었어요.\n\n\n신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요.\n\n왕자님은 유리 구두를 가지고 임금님께 가서 말했어요.\n\n\"이 유리 구두의 주인과 결혼하겠어요.\"\n\n그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요.\n\n 언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만\n\n한눈에 보기에도 유리 구두는 너무 작았어요.\n\n \n\n그때, 신데렐라가 조용히 다가와 말했어요.\n\n\"저도 한번 신어 볼 수 있나요?\"\n\n신데렐라는 신하게 건넨 유리 구두를 신었어요,\n\n유리 구두는 신데렐라의 발에 꼭 맞았어요.\n\n \n\n신하들은 신데렐라를 왕궁으로 데리고 갔어요.\n\n \n\n그 뒤 신데렐라는 완자님과 결혼하여 오래오래 행복하게 살았대요.\n\n\n\n\"\"\"","execution_count":256,"outputs":[]},{"metadata":{"trusted":true,"id":"qa-qIW1h3DkA","outputId":"57080934-5a4d-4ab4-d0d7-e84205455e41"},"cell_type":"code","source":"# 간단한 전처리\ndef clean_text(txt):\n    txt = txt.replace('\\n','')\n    txt = txt.replace('=','')\n    txt = txt.replace('\\\"','')   \n    txt = txt.replace('\\'','')\n    txt = txt.replace(',','')\n    txt = txt.replace('..','')\n    txt = txt.replace('.','. ')\n    txt = txt.replace('  ',' ')\n    return txt \n\norg_text = clean_text(org_text)\norg_text","execution_count":255,"outputs":[{"output_type":"execute_result","execution_count":255,"data":{"text/plain":"'옛날 어느 집에 귀여운 여자 아기가 태어났어요. 아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요. 그러던 어느날 소녀의 어머니가 병이들어그만 세상을 떠나고 말았어요.  소녀의 아버지는 홀로 남은 소녀가 걱정되었어요. 그래서 얼마 후 새어머니를 맞이했어요. 새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요. 그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요. 새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요.  그런데 이번에는 아버지마저 돌아가셨어요. 소녀는 하녀처럼 하루 종일 쓸고 닦고 집안일을 도맡아 했어요. 해도 해도 끝이 없는 집안일이 힘들어 지칠때면난롯가에 앉아서 잠시 쉬곤 했지요.  엄마 저애를 신데렐라라고 불러야겠어요. 온통 재투성이잖아요.  호호호! 두 언니는 소녀를 놀려 댔어요.  어느 날 왕궁에서 무도회가 열렸어요. 신데렐라의 집에도 초대장이 왔어요.  새어머니는 언니들을 데리고 무도회장으로 떠났어요. 신데렐라도 무도회에 가고 싶었어요. 혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요. 신데렐라 너도 무도회에 가고 싶니?신데렐라가 고개를 들어보니 마법사 할머니가 빙그레 웃고 있었어요.  내가 너를 무도회에 보내주마호박 한개와 생쥐 두마리 도마뱀을 구해 오렴.   마법사 할머니가 주문을 외웠어요. 그리고 지팡이로 호박을 건드리자 호박이 화려한 황금 마차로 변했어요. 이번에는 생쥐와 도마뱀을 건드렸어요. 그랬더니 생쥐는 흰말로 도마뱀은 멋진 마부로 변했답니다.  신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요. 신데렐라 발을 내밀어 보거라. 할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요신데렐라 밤 열두시가 되면 모든게 처음대로 돌아간단다. 황금 마차는 호박으로 흰말은 생쥐로 마부는 도마뱀으로 변하게 돼. 그러니까 바늗시 밤 열두 시가 되기 전에 돌아와야 해.  알겠지? 왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요. 완자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고신데렐라하고만 춤을 추었어요. 신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요. 땡 땡 땡 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요.  신데렐라가 허둥지둥 왕궁을 빠져나가는데유리 구두 한 짝이 벗겨졌어요. 하지만 구두를 주울 틈이 없었어요. 신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요. 왕자님은 유리 구두를 가지고 임금님께 가서 말했어요. 이 유리 구두의 주인과 결혼하겠어요. 그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요.  언니들은 발을 오므려도 보고 구두를 늘려도 보았지만한눈에 보기에도 유리 구두는 너무 작았어요.  그때 신데렐라가 조용히 다가와 말했어요. 저도 한번 신어 볼 수 있나요?\\u200b신데렐라는 신하게 건넨 유리 구두를 신었어요유리 구두는 신데렐라의 발에 꼭 맞았어요.  신하들은 신데렐라를 왕궁으로 데리고 갔어요.  그 뒤 신데렐라는 완자님과 결혼하여 오래오래 행복하게 살았대요. '"},"metadata":{}}]},{"metadata":{"id":"I-k66tHNeHx6"},"cell_type":"markdown","source":"한국어 문법체계에 따라 요약문을 생성하기 위해 한국어 문장 샘플을 준비\n\n'한글 위키백과'에서 임의의 문장을 수집 함"},{"metadata":{"trusted":true,"id":"oi6AfKSzeHx7"},"cell_type":"code","source":"#한국어 위키백과에서 스크랩핑\n\nimport wikipedia as wiki\nwiki.set_lang('ko')","execution_count":206,"outputs":[]},{"metadata":{"trusted":true,"id":"ekFwbQVxeHx7","outputId":"9b600fea-389d-4f8c-afea-3838dfad720b"},"cell_type":"code","source":"# '전래동화' 라는 keyword로 100개 page의 Text를 취득\n\ndef __search_from_wiki(question,max_rank):\n    results = wiki.search(question,results=max_rank)\n    print(results)\n    contents = []\n    for result in results:\n        try:\n            page = wiki.page(result)\n            #print(f\"Top wiki result: {page}\")\n            text = page.content\n            ln = len(text)\n            print(f'Collecting page : {page} , text length {str(ln)}')\n            #if ln < 4000:\n            #  contents.append(text)\n            #else:\n            #  contents.append(text[0:4000])\n            contents.append(text)\n        except Exception as ex:\n            print(ex)\n    return contents\n\nif DO_ALL:\n    ko_grammar_set_raw = __search_from_wiki(\"전래동화\", 100)\n\nprint(f'전체 수집한 Page Count : {len(ko_grammar_set_raw)}')","execution_count":207,"outputs":[{"output_type":"stream","text":"전체 수집한 Page Count : 197\n","name":"stdout"}]},{"metadata":{"trusted":true,"id":"wfcHLkfGeHx8","outputId":"88e3b334-e462-4002-9cc3-2cd75ace5686"},"cell_type":"code","source":"\nif DO_ALL:\n    ko_grammar_set_raw += __search_from_wiki(\"역사\", 100)\n\nprint(f'전체 수집한 Page Count : {len(ko_grammar_set_raw)}')","execution_count":208,"outputs":[{"output_type":"stream","text":"전체 수집한 Page Count : 197\n","name":"stdout"}]},{"metadata":{"trusted":true,"id":"Tu_6jZAmeHx9","outputId":"ee4ccd35-eefe-4b97-a820-a49fcc01078b"},"cell_type":"code","source":"# 간단한 전처리\ndef clean_text(txt):\n    txt = txt.replace('\\n','')\n    txt = txt.replace('=','')    \n    return txt \n\nko_grammar_set_raw = [clean_text(x) for x in ko_grammar_set_raw]\nprint('Sample text : ')\nprint('--------------------------------------------------------------------------------------------')\nprint(ko_grammar_set_raw[50])\nprint('--------------------------------------------------------------------------------------------')","execution_count":209,"outputs":[{"output_type":"stream","text":"Sample text : \n--------------------------------------------------------------------------------------------\n백조의 호수(러시아어: Лебединое озеро)는 러시아의 차이콥스키가 작곡한 발레 음악이자 이 음악에 맞춰 공연되는 발레 작품이다. 초연 때는 연출이나 무대 장치가 서툴렀기 때문에 호응을 얻지 못했으나, 차이콥스키의 음악과 프티파, 이바노프(Lev Ivanov)의 개작으로 처음으로 그 진가가 인정되고, 고전 발레를 대표하는 작품으로서 각국에서 상연되기에 이르렀다. 시나리오는 러시아 전래 동화를 기반으로 하여, 악한 마법사의 저주에 걸려 백조로 변한 오데트 공주에 대한 이야기이다. 전체에 넘쳐흐르는 감미로운 선율은 오데트 공주의 슬픈 운명을 묘사하였고, 또한 제3막의 궁정 무도회의 성격(character) 무용에서는 민족적 리듬을 지닌 소곡을 차례로 전개한다. 오데트 공주의 주제 음악은 특히 유명하다. 관습적으로 오데트 공주를 연기하는 프리마 발레리나는 지그프리드 왕자를 유혹하는 흑조 오딜도 동시에 연기한다. 줄거리  제1막 지그프리드 왕자는 교사, 친구, 신민과 함께 그의 생일을 축하한다. 여황은 왕자의 자유로운 생활을 걱정하는 지그프리드의 어머니인 여왕에 의해 곧 방해받고 만다. 여왕은 다음 저녁에 열리는 왕실 무도회에서 신부를 정해야 한다고 당부하지만, 왕자는 사랑을 위해 결혼할 수 없다는 것에 화가 난다. 왕자의 친구인 벤노와 교사는 왕자의 기분을 풀어 주려 한다.  저녁이 드리워질 때 벤노는 백조 무리가 머리 위로 지나가는 것을 보고 사냥을 가자고 제안한다. 왕자와 친구들을 석궁을 들고 백조를 쫓기 위해 떠난다. 제2막 지그프리드 왕자는 친구들과 떨어진다. 왕자가 호수 옆 빈터에 다다랐을 때 백조 무리가 근처에 앉는다. 왕자는 백조를 향해 석궁을 겨누지만 백조 무리 중 한 마리가 아름다운 아가씨 오데트로 변신하는 것을 보자 얼어붙고 만다. 오데트는 처음에 지그프리드에게 겁을 먹지만, 왕자가 그녀를 해치지 않겠다고 약속하자 자신이 백조의 여왕 오데트라고 말한다. 오데트와 다른 백조들은 부엉이 같이 생긴 사악한 마법사 로트바르트의 저주의 희생자였다. 낮 동안 그들은 백조로 변하고, 밤에만 오데트의 어머니의 눈물로 만들어진 마법의 호수 옆에서 인간의 형상으로 돌아온다. 저주는 오직 사랑을 하지 않았던 사람이 오데트를 영원히 사랑할 것을 맹세했을 때만 깨질 수 있다. 로트바르트가 갑자기 나타나자 왕자는 그를 죽이겠다고 하지만 로트바르트는 저주가 깨지기 전에 죽으면 저주는 되돌릴 수 없다고 호소한다. 로트바르트가 사라지자 백조들이 빈터를 매운다. 지그프리드는 석궁을 부수고 오데트의 신뢰를 얻기 시작한다. 그러나 아침이 되었을 때 사악한 저주는 오데트와 그녀의 시종들을 호수로 끌고가고, 그녀들은 다시 백조가 된다. 제3막 무도회를 위해 왕궁에 손님들이 도착한다. 여왕이 왕자가 신부로 선택하길 바라는 여섯 명의 공주가 왕자에게 소개된다. 그 때 로트바르트가 인간으로 변장하고 오데트와 똑같이 변신한 딸 오딜과 함께 나타난다. 공주들은 왕자의 관심을 끌기 위해 춤을 추지만 오딜을 오데트로 착각한 지그프리드 왕자는 오직 오딜만 바라보고 오딜하고만 춤을 춘다. 오데트는 환영으로 나타나 지그프리드에게 속고 있음을 경고하려고 한다. 그러나 지그프리드는 그만 오딜을 아내로 삼겠다고 맹세하고 만다. 로트바르트는 지그프리드에게 오데트의 환영을 보여주고 왕자는 실수를 깨닫는다. 지그프리드는 호수로 급히 돌아간다. 제4막 오데트는 지그프리드의 배신에 깊은 충격을 받는다. 백조들은 그녀를 안심시키려 하지만 오데트는 죽음을 결심한다. 지그프리드 왕자는 호수로 돌아와 오데트를 찾는다. 왕자는 오데트에게 진심으로 사과하고, 오데트는 그를 용서한다. 로트바르트가 나타나 지그프리드는 오딜과 결혼하겠다는 약속을 지켜야 한다고 주장하며, 오데트는 영원히 백조가 될 것이라고 말한다. 왕자는 차라리 오데트 옆에서 죽는 것을 택하고 둘은 호수로 뛰어든다. 이로 인해 로트바르트의 저주는 깨지고, 로트바르트는 힘을 잃고 죽는다. 백조 아가씨들은 지그프리드와 오데트가 사랑으로 하나가 되어 함께 천국으로 올라가는 모습을 본다. 쓰임 위 노래(백조의 호수)는 피겨스케이팅,리듬체조 등 프로그램 음악으로도 많이 쓰이며발레의 명곡으로 많은 사람들의 사랑을 받고 있다. 참고 문헌 \n--------------------------------------------------------------------------------------------\n","name":"stdout"}]},{"metadata":{"id":"lqJHLPaReHx-"},"cell_type":"markdown","source":"문장으로 잘라 낸다"},{"metadata":{"trusted":true,"id":"_b8WKqYXeHx_","outputId":"124197a7-65d4-435f-bb24-f0e80be0a50a"},"cell_type":"code","source":"import nltk\nnltk.download('punkt')","execution_count":210,"outputs":[{"output_type":"stream","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","name":"stdout"},{"output_type":"execute_result","execution_count":210,"data":{"text/plain":"True"},"metadata":{}}]},{"metadata":{"trusted":true,"id":"erJc7g-VeHyA","outputId":"b8d3072f-70d0-41cf-8a8d-78e4c109f859"},"cell_type":"code","source":"#Split the document into sentences\nko_grammar_sentences = []\nfor document in ko_grammar_set_raw:\n    ko_grammar_sentences += nltk.sent_tokenize(document)\n\nprint(\"Num sentences:\", len(ko_grammar_sentences))","execution_count":211,"outputs":[{"output_type":"stream","text":"Num sentences: 13551\n","name":"stdout"}]},{"metadata":{"trusted":true,"id":"7M2yM5kFeHyC","outputId":"76ee3f7d-994b-4193-b4a2-3d006645d35d"},"cell_type":"code","source":"ko_grammar_sentences[300]","execution_count":212,"outputs":[{"output_type":"execute_result","execution_count":212,"data":{"text/plain":"\"현재 기탄교육은 학습지로는 '기탄국어, 한글떼기, 기탄한글, 기탄수학, 기탄사고력수학, 영단어 암기 끝!, 기탄 한자 빨리따기, 기탄중국어' 등이 있다.\""},"metadata":{}}]},{"metadata":{"trusted":true,"id":"Q5K4D3QM3DkG"},"cell_type":"code","source":"f = open(\"ko_sentence_set.txt\", \"a\")\nfor t in ko_grammar_sentences:\n    f.write(t+'\\n')\nf.close()","execution_count":213,"outputs":[]},{"metadata":{"id":"fdj8wrm8eHyC"},"cell_type":"markdown","source":"형태소 분리하여 모든 문장을 형태소 Code로 변환 한다."},{"metadata":{"trusted":true,"id":"uETdqraheHyE","outputId":"29c0f783-9367-4416-e154-8133987464d0"},"cell_type":"code","source":"from konlpy.tag import Twitter\ntwitter = Twitter()\n\nprint(twitter.pos(ko_grammar_sentences[305]))","execution_count":214,"outputs":[{"output_type":"stream","text":"[('백운호수', 'Noun'), ('에서', 'Josa'), ('의왕시', 'Noun'), ('와', 'Josa'), ('의왕시', 'Noun'), ('축제', 'Noun'), ('추진', 'Noun'), ('위원회', 'Noun'), ('가', 'Josa'), ('공동', 'Noun'), ('으로', 'Josa'), ('주최', 'Noun'), ('하여', 'Verb'), ('매년', 'Noun'), ('9월', 'Number'), ('에', 'Foreign'), ('이틀', 'Noun'), ('동안', 'Noun'), ('열리는', 'Verb'), ('축제', 'Noun'), ('이다', 'Josa'), ('.', 'Punctuation')]\n","name":"stdout"}]},{"metadata":{"trusted":true,"id":"OY8VTXraeHyF"},"cell_type":"code","source":"# 형태소 Code table의 구성\n\n_MAX_MORP_LENGTH = 128\n_PADDING_CODE = 0  # padding code\n_MISMATCH_CODE = 1 # mismatch word code               ex) @@@\n_MISMATCH_WORD = '@@@' # 이거 아래에서 쓴다.\n\nmorpheme_table = {}\nmorp_code = _MISMATCH_CODE+1\nmorpheme_table['Pad'] = _PADDING_CODE \nmorpheme_table['Mst'] = _MISMATCH_CODE \nfor sentence in ko_grammar_sentences[:1000]:\n    morphemes = twitter.pos(sentence)\n    for (word,morp) in morphemes:\n        if morp in morpheme_table:\n            pass\n        else:\n            morpheme_table[morp] = morp_code\n            morp_code += 1","execution_count":215,"outputs":[]},{"metadata":{"trusted":true,"id":"asiTeu8SeHyG","outputId":"bd822172-23f6-4df1-9dcb-cc2cee1750be"},"cell_type":"code","source":"print('Korean morpheme code table')\nprint('----------------------------------------------------------')\nprint('  Morpheme        Code')\nprint('')\nfor morp in morpheme_table.keys():\n    print(f' {morp.ljust(15)}   {morpheme_table[morp]}')\nprint('----------------------------------------------------------')\n","execution_count":216,"outputs":[{"output_type":"stream","text":"Korean morpheme code table\n----------------------------------------------------------\n  Morpheme        Code\n\n Pad               0\n Mst               1\n Noun              2\n Punctuation       3\n Foreign           4\n Josa              5\n Verb              6\n Modifier          7\n Adjective         8\n Suffix            9\n Adverb            10\n Number            11\n Alpha             12\n Conjunction       13\n Determiner        14\n VerbPrefix        15\n Exclamation       16\n KoreanParticle    17\n Eomi              18\n ScreenName        19\n URL               20\n----------------------------------------------------------\n","name":"stdout"}]},{"metadata":{"trusted":true,"id":"RQfjoYbceHyH","outputId":"8439f60c-e079-445d-e2be-627088f602a7"},"cell_type":"code","source":"# morpheme 코드 변환기\ndef morpheme_encode(sentence):\n    encode=[]\n    morphemes = twitter.pos(sentence)\n    for (word,morp) in morphemes:\n        encode.append(_MISMATCH_CODE if word==_MISMATCH_WORD else morpheme_table[morp])\n    return encode\n\ncode = morpheme_encode('야당이 수단으로 야당에게는 이라며 윤석열 허물어뜨렸고 수사를 군사작전을 하는 시비를 민주당 의원이 국민적 세워 화신이 나라 법위에 우려했다 될 이라며 운영할지 바람 짓밟지만 양자역학 양자역학 걱정하느냐고 누가 청와대 한다 엄포를 양자역학 양자역학 양자역학 편을 파렴치와 난관에 풀이다 대통령이 양자역학 저지하려다가')\nprint(f'Code length : {len(code)}')","execution_count":217,"outputs":[{"output_type":"stream","text":"Code length : 67\n","name":"stdout"}]},{"metadata":{"trusted":true,"id":"BS_SnM04eHyH","outputId":"5b3b8516-b337-4aee-bce4-ba077bc11a43"},"cell_type":"code","source":"# 전체 형태소 코드로 변환\nif DO_ALL:\n    ko_grammar_set = []\n    for sentence in ko_grammar_sentences:\n        code = morpheme_encode(sentence)\n        if len(code) <= _MAX_MORP_LENGTH:\n            ko_grammar_set.append(code + [_PADDING_CODE for i in range(_MAX_MORP_LENGTH-len(code))])\n\n    ko_grammar_set = np.asarray(ko_grammar_set)\nko_grammar_set.shape","execution_count":218,"outputs":[{"output_type":"execute_result","execution_count":218,"data":{"text/plain":"(13352, 128)"},"metadata":{}}]},{"metadata":{"trusted":true,"id":"d4dpyWPU3DkJ"},"cell_type":"code","source":"from numpy import savetxt\n# save to csv file\nsavetxt('ko_morpheme_set.csv', ko_grammar_set, delimiter=',')\n","execution_count":219,"outputs":[]},{"metadata":{"id":"tNq3STdueHyI"},"cell_type":"markdown","source":"# Dataset 전체 구성"},{"metadata":{"trusted":true,"id":"PiSdmi1beHyI","outputId":"79a8547c-b57c-4b64-bbe1-f16a79094ca7"},"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nfrom transformers import BertTokenizer\n\nif DO_ALL:\n    # embedder download...\n    embedder = SentenceTransformer('xlm-r-large-en-ko-nli-ststb')","execution_count":220,"outputs":[]},{"metadata":{"trusted":true,"id":"e1FGpEuJeHyJ"},"cell_type":"code","source":"BATCH_SIZE = 64\nBATCH_COUNT = 50","execution_count":221,"outputs":[]},{"metadata":{"id":"JItg20Ukw5il","trusted":true},"cell_type":"code","source":"# 원문의 간단한 전처리\ndef clean_text(txt):\n    txt = txt.replace('\\n','')\n    txt = txt.replace('=','')    \n    txt = txt.replace('\\\"','')    \n    txt = txt.replace('\\'','')    \n    return txt \n\norg_text = clean_text(org_text)","execution_count":222,"outputs":[]},{"metadata":{"id":"owsvfOpGxYTi","outputId":"a7d86d51-76ba-49b3-9398-adc6c235a336","trusted":true},"cell_type":"code","source":"org_text","execution_count":223,"outputs":[{"output_type":"execute_result","execution_count":223,"data":{"text/plain":"'옛날 어느 집에 귀여운 여자 아기가 태어났어요.아기는 무럭무럭 자라서 예쁘고 마음씨 고운 소녀가 되었어요.그러던 어느날 소녀의 어머니가 병이들어그만 세상을 떠나고 말았어요. 소녀의 아버지는 홀로 남은 소녀가 걱정되었어요.그래서 얼마 후 새어머니를 맞이했어요.새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요.그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요.새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요. 그런데 이번에는 아버지마저 돌아가셨어요.소녀는 하녀처럼 하루 종일 쓸고 닦고 집안일을 도맡아 했어요.해도 해도 끝이 없는 집안일이 힘들어 지칠때면난롯가에 앉아서 잠시 쉬곤 했지요. 엄마 저애를 신데렐라라고 불러야겠어요.온통 재투성이잖아요. 호호호! 두 언니는 소녀를 놀려 댔어요. 어느 날 왕궁에서 무도회가 열렸어요.신데렐라의 집에도 초대장이 왔어요. 새어머니는 언니들을 데리고 무도회장으로 떠났어요.신데렐라도 무도회에 가고 싶었어요.혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요.신데렐라 너도 무도회에 가고 싶니?신데렐라가 고개를 들어보니 마법사 할머니가 빙그레 웃고 있었어요. 내가 너를 무도회에 보내주마호박 한개와 생쥐 두마리 도마뱀을 구해 오렴.  마법사 할머니가 주문을 외웠어요.그리고 지팡이로 호박을 건드리자 호박이 화려한 황금 마차로 변했어요.이번에는 생쥐와 도마뱀을 건드렸어요.그랬더니 생쥐는 흰말로 도마뱀은 멋진 마부로 변했답니다. 신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요.신데렐라 발을 내밀어 보거라.할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요신데렐라 밤 열두시가 되면 모든게 처음대로 돌아간단다.황금 마차는 호박으로 흰말은 생쥐로 마부는 도마뱀으로 변하게 돼.그러니까 바늗시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지? 왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요.완자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고신데렐라하고만 춤을 추었어요.신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요.땡 땡 땡...... 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요. 신데렐라가 허둥지둥 왕궁을 빠져나가는데유리 구두 한 짝이 벗겨졌어요.하지만 구두를 주울 틈이 없었어요.신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요.왕자님은 유리 구두를 가지고 임금님께 가서 말했어요.이 유리 구두의 주인과 결혼하겠어요.그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요. 언니들은 발을 오므려도 보고 구두를 늘려도 보았지만한눈에 보기에도 유리 구두는 너무 작았어요. 그때 신데렐라가 조용히 다가와 말했어요.저도 한번 신어 볼 수 있나요?\\u200b신데렐라는 신하게 건넨 유리 구두를 신었어요유리 구두는 신데렐라의 발에 꼭 맞았어요. 신하들은 신데렐라를 왕궁으로 데리고 갔어요. 그 뒤 신데렐라는 완자님과 결혼하여 오래오래 행복하게 살았대요.'"},"metadata":{}}]},{"metadata":{"trusted":true,"id":"bhOs5GKNeHyK","outputId":"a91f1671-ed60-424f-b65e-be36f05e3c63"},"cell_type":"code","source":"#dataset 다시 만듦\n\norg_text_emb = embedder.encode([org_text])[0]\nprint(f'Text embedding shape : {org_text_emb.shape}')\ndataset = []\nfor i in range(BATCH_COUNT):\n    emb_batch_set = []\n    cod_batch_set = []\n    for j in range(BATCH_SIZE):\n        emb_batch_set.append(org_text_emb)\n        cod_batch_set.append(ko_grammar_set[BATCH_SIZE*i+j])\n\n    emb_batch_set = np.asarray(emb_batch_set)\n    cod_batch_set = np.asarray(cod_batch_set)\n    dataset.append((emb_batch_set,cod_batch_set))\n\nprint(f'Total dataset count :{BATCH_COUNT*BATCH_SIZE}')","execution_count":224,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Batches', max=1.0, style=ProgressStyle(description_width=…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f9bdd3e443b4b63aacfd09410c95685"}},"metadata":{}},{"output_type":"stream","text":"\nText embedding shape : (1024,)\nTotal dataset count :3200\n","name":"stdout"}]},{"metadata":{"trusted":true,"id":"VxfQxFQKeHyL","outputId":"8af895aa-9a7e-4891-9967-9352c95b5a10"},"cell_type":"code","source":"# 30번째 배치의 형태소코드셋 중 20번째꺼 확인\nprint(dataset[30][1][20])\nprint(dataset[31][1][20])","execution_count":225,"outputs":[{"output_type":"stream","text":"[2 9 2 5 2 2 5 6 2 2 5 6 2 2 5 2 5 6 3 2 5 2 5 2 6 2 5 6 2 5 6 6 2 8 3 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n[ 2  2  5  2  5  2  5  2  2  2  5  2  8  3  2  5  2  6  6  2  5  2  5  3\n 11  3 11  3 11  3 11  3 11  3 11  3 11  3  2  2  5  2  5  6  2  6  2  5\n  6  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0]\n","name":"stdout"}]},{"metadata":{"id":"NlM5tdo1eHyL"},"cell_type":"markdown","source":"# Generator 구현"},{"metadata":{"trusted":true,"id":"jopXmV8DeHyM","outputId":"b45bba25-1b97-4f59-be8c-69a793ab4b1e"},"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts([org_text])\ntotal_words = len(tokenizer.word_index)\n\nprint(f'Total token count of origin text : {total_words}')","execution_count":226,"outputs":[{"output_type":"stream","text":"Total token count of origin text : 275\n","name":"stdout"}]},{"metadata":{"id":"1DT4gKRYeHyN"},"cell_type":"markdown","source":"원문을 40개의 token 으로 구성된 문장으로 요약 하는것....<br>\n향후 LSTM으로 G 구성 필요"},{"metadata":{"trusted":true,"id":"P_cpFugJeHyN"},"cell_type":"code","source":"\norg_term_set = org_text.split(' ')\n\n_MAX_GEN_TOKEN = 40\n_NOISE_DIM = len(org_term_set)\n\nword_table = {}\n\nfor index, word in zip(range(len(org_term_set)),org_term_set):\n    word_table[index] = word\n\ncurrent_token_len = len(word_table)\n\n","execution_count":227,"outputs":[]},{"metadata":{"trusted":true,"id":"Jg3YCjGJeHyO","outputId":"d6d1151c-9027-4148-f490-7097f0021fe9"},"cell_type":"code","source":"print('Token table of origin text')\nprint('---------------------------------------------')\nprint(' Code         Token      ')\nprint('')\nfor k in word_table.keys():\n  print( f'  {str(k).ljust(8)}    {word_table[k]}')\nprint('---------------------------------------------')\n","execution_count":228,"outputs":[{"output_type":"stream","text":"Token table of origin text\n---------------------------------------------\n Code         Token      \n\n  0           옛날\n  1           어느\n  2           집에\n  3           귀여운\n  4           여자\n  5           아기가\n  6           태어났어요.아기는\n  7           무럭무럭\n  8           자라서\n  9           예쁘고\n  10          마음씨\n  11          고운\n  12          소녀가\n  13          되었어요.그러던\n  14          어느날\n  15          소녀의\n  16          어머니가\n  17          병이들어그만\n  18          세상을\n  19          떠나고\n  20          말았어요.\n  21          소녀의\n  22          아버지는\n  23          홀로\n  24          남은\n  25          소녀가\n  26          걱정되었어요.그래서\n  27          얼마\n  28          후\n  29          새어머니를\n  30          맞이했어요.새어머니는\n  31          소녀보다\n  32          나이가\n  33          위인\n  34          두\n  35          딸을\n  36          데리고\n  37          왔어요.그러나\n  38          새어머니와\n  39          언니들은\n  40          성질이\n  41          고약한\n  42          심술쟁이들이었어요.새어머니는\n  43          소녀가\n  44          자기\n  45          딸들보다\n  46          예쁘고\n  47          착한\n  48          게\n  49          못마땅했어요.\n  50          그런데\n  51          이번에는\n  52          아버지마저\n  53          돌아가셨어요.소녀는\n  54          하녀처럼\n  55          하루\n  56          종일\n  57          쓸고\n  58          닦고\n  59          집안일을\n  60          도맡아\n  61          했어요.해도\n  62          해도\n  63          끝이\n  64          없는\n  65          집안일이\n  66          힘들어\n  67          지칠때면난롯가에\n  68          앉아서\n  69          잠시\n  70          쉬곤\n  71          했지요.\n  72          엄마\n  73          저애를\n  74          신데렐라라고\n  75          불러야겠어요.온통\n  76          재투성이잖아요.\n  77          호호호!\n  78          두\n  79          언니는\n  80          소녀를\n  81          놀려\n  82          댔어요.\n  83          어느\n  84          날\n  85          왕궁에서\n  86          무도회가\n  87          열렸어요.신데렐라의\n  88          집에도\n  89          초대장이\n  90          왔어요.\n  91          새어머니는\n  92          언니들을\n  93          데리고\n  94          무도회장으로\n  95          떠났어요.신데렐라도\n  96          무도회에\n  97          가고\n  98          싶었어요.혼자\n  99          남은\n  100         신데렐라는\n  101         훌쩍훌쩍\n  102         울기\n  103         시작했어요.신데렐라\n  104         너도\n  105         무도회에\n  106         가고\n  107         싶니?신데렐라가\n  108         고개를\n  109         들어보니\n  110         마법사\n  111         할머니가\n  112         빙그레\n  113         웃고\n  114         있었어요.\n  115         내가\n  116         너를\n  117         무도회에\n  118         보내주마호박\n  119         한개와\n  120         생쥐\n  121         두마리\n  122         도마뱀을\n  123         구해\n  124         오렴.\n  125         \n  126         마법사\n  127         할머니가\n  128         주문을\n  129         외웠어요.그리고\n  130         지팡이로\n  131         호박을\n  132         건드리자\n  133         호박이\n  134         화려한\n  135         황금\n  136         마차로\n  137         변했어요.이번에는\n  138         생쥐와\n  139         도마뱀을\n  140         건드렸어요.그랬더니\n  141         생쥐는\n  142         흰말로\n  143         도마뱀은\n  144         멋진\n  145         마부로\n  146         변했답니다.\n  147         신데렐라의\n  148         옷도\n  149         구슬\n  150         장식이\n  151         반짝이는\n  152         예쁜\n  153         드레스로\n  154         바뀌웠어요.신데렐라\n  155         발을\n  156         내밀어\n  157         보거라.할머니는\n  158         신데렐라에게\n  159         반짝반짝\n  160         빛나는\n  161         유리\n  162         구두를\n  163         신겨\n  164         주었어요신데렐라\n  165         밤\n  166         열두시가\n  167         되면\n  168         모든게\n  169         처음대로\n  170         돌아간단다.황금\n  171         마차는\n  172         호박으로\n  173         흰말은\n  174         생쥐로\n  175         마부는\n  176         도마뱀으로\n  177         변하게\n  178         돼.그러니까\n  179         바늗시\n  180         밤\n  181         열두\n  182         시가\n  183         되기\n  184         전에\n  185         돌아와야\n  186         해.\n  187         알겠지?\n  188         왕자님도\n  189         아름다운\n  190         신데렐라에게\n  191         마음을\n  192         빼았겼어요.완자님은\n  193         무도회장에\n  194         모인\n  195         다른\n  196         아가씨들은\n  197         쳐다보지도\n  198         않고신데렐라하고만\n  199         춤을\n  200         추었어요.신데렐라는\n  201         왕자님과\n  202         춤을\n  203         추느라\n  204         시간\n  205         가는\n  206         줄도\n  207         몰랐어요.땡\n  208         땡\n  209         땡......\n  210         벽시계가\n  211         열두\n  212         시를\n  213         알리는\n  214         소리에\n  215         신데렐라는\n  216         화들짝\n  217         놀랐어요.\n  218         신데렐라가\n  219         허둥지둥\n  220         왕궁을\n  221         빠져나가는데유리\n  222         구두\n  223         한\n  224         짝이\n  225         벗겨졌어요.하지만\n  226         구두를\n  227         주울\n  228         틈이\n  229         없었어요.신데렐라를\n  230         뛰쫓아오던\n  231         왕자님은\n  232         층계에서\n  233         유리\n  234         구두\n  235         한\n  236         짝을\n  237         주웠어요.왕자님은\n  238         유리\n  239         구두를\n  240         가지고\n  241         임금님께\n  242         가서\n  243         말했어요.이\n  244         유리\n  245         구두의\n  246         주인과\n  247         결혼하겠어요.그래서\n  248         신하들은\n  249         유리\n  250         구두의\n  251         주인을\n  252         찾아\n  253         온\n  254         나라를\n  255         돌아다녔어요.\n  256         언니들은\n  257         발을\n  258         오므려도\n  259         보고\n  260         구두를\n  261         늘려도\n  262         보았지만한눈에\n  263         보기에도\n  264         유리\n  265         구두는\n  266         너무\n  267         작았어요.\n  268         그때\n  269         신데렐라가\n  270         조용히\n  271         다가와\n  272         말했어요.저도\n  273         한번\n  274         신어\n  275         볼\n  276         수\n  277         있나요?​신데렐라는\n  278         신하게\n  279         건넨\n  280         유리\n  281         구두를\n  282         신었어요유리\n  283         구두는\n  284         신데렐라의\n  285         발에\n  286         꼭\n  287         맞았어요.\n  288         신하들은\n  289         신데렐라를\n  290         왕궁으로\n  291         데리고\n  292         갔어요.\n  293         그\n  294         뒤\n  295         신데렐라는\n  296         완자님과\n  297         결혼하여\n  298         오래오래\n  299         행복하게\n  300         살았대요.\n---------------------------------------------\n","name":"stdout"}]},{"metadata":{"trusted":true,"id":"bZgNu7SYeHyP"},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import (Input,\n                                     Dense, \n                                     BatchNormalization, \n                                     LeakyReLU,\n                                     Softmax,\n                                     Reshape, \n                                     Conv2DTranspose,\n                                     Conv2D,\n                                     Dropout,\n                                     Flatten,\n                                     Concatenate,\n                                     Lambda)\nimport matplotlib.pyplot as plt","execution_count":229,"outputs":[]},{"metadata":{"id":"k6Vj9qaaeHyQ"},"cell_type":"markdown","source":"noise를 token_table을 통해 text로 변환하는 변환기 구현"},{"metadata":{"id":"8cAv4SYIVyfL","outputId":"198eed60-122d-4ef7-976c-0ca002c26819","trusted":true},"cell_type":"code","source":"\nb = tf.constant([1,2,3,4,5,6,11,8,9,10])\nc = tf.sort(b,axis=-1,direction='DESCENDING')\nwh = tf.where(tf.math.greater_equal(b,c[5-1]))[:5]\nwh","execution_count":230,"outputs":[{"output_type":"execute_result","execution_count":230,"data":{"text/plain":"<tf.Tensor: shape=(5, 1), dtype=int64, numpy=\narray([[5],\n       [6],\n       [7],\n       [8],\n       [9]])>"},"metadata":{}}]},{"metadata":{"trusted":true,"id":"72FRz_bDeHyR"},"cell_type":"code","source":"import sys\n\ndef to_text(w):\n\n    #tf.print(w)\n    texts = []\n    try:\n        s_w = tf.sort(w,axis=-1,direction='DESCENDING')\n        #print(s_w)\n        for w_0r,s_w_0r in zip(w,s_w):\n            text = \"\"\n            wh = tf.where(tf.math.greater_equal(w_0r,s_w_0r[_MAX_GEN_TOKEN]))[:_MAX_GEN_TOKEN]\n\n            for k in tf.reshape(wh,(_MAX_GEN_TOKEN,)).numpy():\n                try:\n                    text += word_table[k]+' '\n                except Exception as ex:\n                    tf.print('to_text : word_table' + str(ex),k)\n                \n            texts.append(text)\n    except Exception as ex:\n        tf.print('to_text : ' + str(ex),sys.exc_info())\n\n    return tf.constant(texts,dtype=tf.string)","execution_count":231,"outputs":[]},{"metadata":{"trusted":true,"id":"ywMSg5noeHyS","outputId":"14f033c5-b156-4361-e3af-1e7ac354a89d"},"cell_type":"code","source":"# to_text 함수의 test\nw = tf.random.normal([3,_NOISE_DIM])\nprint(w.shape)\ne = to_text(w)\nfor t in e:\n    print(t.numpy().decode('utf-8'))","execution_count":232,"outputs":[{"output_type":"stream","text":"(3, 301)\n집에 떠나고 아버지는 소녀보다 소녀가 딸들보다 돌아가셨어요.소녀는 집안일을 쉬곤 했지요. 신데렐라라고 어느 열렸어요.신데렐라의 데리고 시작했어요.신데렐라 보내주마호박 두마리 주문을 화려한 생쥐와 마부로 신데렐라에게 반짝반짝 열두시가 돌아간단다.황금 바늗시 되기 돌아와야 무도회장에 모인 가는 빠져나가는데유리 언니들은 오므려도 구두를 보았지만한눈에 구두는 신데렐라가 수 있나요?​신데렐라는 \n떠나고 말았어요. 새어머니를 착한 앉아서 댔어요. 집에도 시작했어요.신데렐라 가고 빙그레 웃고 무도회에 주문을 도마뱀을 도마뱀은 변했답니다. 발을 반짝반짝 주었어요신데렐라 모든게 해. 알겠지? 왕자님도 춤을 왕자님과 시를 놀랐어요. 구두를 구두 주웠어요.왕자님은 말했어요.이 신하들은 유리 구두를 늘려도 보기에도 신어 꼭 신데렐라를 뒤 \n소녀가 소녀의 아버지마저 했어요.해도 끝이 호호호! 왕궁에서 집에도 떠났어요.신데렐라도 너도 내가 구해  할머니가 주문을 호박을 도마뱀을 신데렐라의 옷도 구슬 예쁜 되면 돼.그러니까 되기 쳐다보지도 신데렐라가 구두 짝이 주울 층계에서 한 신하들은 돌아다녔어요. 보기에도 건넨 구두를 신었어요유리 신데렐라의 발에 그 \n","name":"stdout"}]},{"metadata":{"id":"rGwa9iaPeHyT"},"cell_type":"markdown","source":"생성된 text의 embedding 변환기 구현<br>\nembedding은 org_text의 embedding과 비교하여 원문과 유사하게 민들기 위한 목적"},{"metadata":{"trusted":true,"id":"_D7qcRmQeHyX"},"cell_type":"code","source":"# 이거 이틀걸림...잘 몰라서 ㅈㄴ 헤맴\n\n@tf.custom_gradient\ndef to_embedding(w):\n\n    def grad(dy):\n        dy = tf.nn.batch_normalization(dy, 0, tf.math.reduce_std(dy), 0, 10, 1e-3)  # gradient 증폭 \n        dy_arr = tf.reshape(dy,(dy.shape[0],1024,1))\n        #tf.print(dy_arr)\n        dy_arr_st = tf.image.resize(dy_arr, size=(dy.shape[0],w.shape[1]),method=tf.image.ResizeMethod.BILINEAR)\n        dy_arr_st = tf.reshape(dy_arr_st,shape=(dy.shape[0],w.shape[1]))\n        return dy_arr_st\n\n    #print(w)    \n    texts = []\n    value = None\n    texts = []\n    try:\n        s_w = tf.sort(w,axis=-1,direction='DESCENDING')\n        #print(s_w)\n        for w_0r,s_w_0r in zip(w,s_w):\n            text = \"\"\n            wh = tf.where(tf.math.greater_equal(w_0r,s_w_0r[_MAX_GEN_TOKEN]))[:_MAX_GEN_TOKEN]\n            for k in tf.reshape(wh,(_MAX_GEN_TOKEN,)).numpy():\n                text += word_table[k]+' '\n                 \n            texts.append(text)                 \n        value = tf.constant(embedder.encode(texts,show_progress_bar=False),dtype=tf.float64)\n    except Exception as ex:\n        tf.print(ex,sys.exc_info())\n\n    return value, grad","execution_count":233,"outputs":[]},{"metadata":{"trusted":true,"id":"9Pu18GvweHyY","outputId":"7c10dab5-0178-4c69-becf-4e7c0cb4ae7e"},"cell_type":"code","source":"# to_embedding 함수의 test\ne = to_embedding(w)\nfor t in e:\n    print(t.numpy())","execution_count":234,"outputs":[{"output_type":"stream","text":"[ 0.20697494 -0.16078305  0.04480495 ... -0.18544388  0.67773646\n  0.60726237]\n[ 0.35158053 -0.53133172  0.05704678 ... -0.37279111  0.59129167\n  0.58416045]\n[ 0.19844982 -0.38411286  0.00347876 ... -0.39883673  0.88680857\n  0.72472489]\n","name":"stdout"}]},{"metadata":{"trusted":true,"id":"DWTe53wueHya"},"cell_type":"code","source":"# to_compression_ratio\n\n@tf.custom_gradient\ndef to_compression_ratio(w):\n    def grad(dy):\n        dy = tf.nn.batch_normalization(dy, 0, tf.math.reduce_std(dy), 0, 2, 1e-3)  # gradient 증폭, 이래도 되는 건지....\n        dy_arr = tf.reshape(dy,(dy.shape[0],1,1))\n        #tf.print(dy_arr)\n        dy_arr_st = tf.image.resize(dy_arr, size=(dy.shape[0],w.shape[1]),method=tf.image.ResizeMethod.BILINEAR)\n        dy_arr_st = tf.reshape(dy_arr_st,shape=(dy.shape[0],w.shape[1]))        \n        return dy_arr_st\n\n    const = tf.math.reduce_sum(tf.constant([x for x in range(tf.size(w[0]))],tf.int64)) \n    #print(const)\n    value = None\n    compression_ratio = [] #tf.Variable()\n    texts = []\n    try:\n        s_w = tf.sort(w,axis=-1,direction='DESCENDING')\n        #print(s_w)\n        for w_0r,s_w_0r in zip(w,s_w):\n            wh = tf.where(tf.math.greater_equal(w_0r,s_w_0r[_MAX_GEN_TOKEN]))[:_MAX_GEN_TOKEN]\n            cr = tf.math.reduce_sum(tf.reshape(wh,(_MAX_GEN_TOKEN,)))\n            compression_ratio.append(cr.numpy()/const.numpy())\n        #print(compression_ratio)\n        value = tf.constant(compression_ratio,dtype=tf.float64)\n    except Exception as ex:\n        tf.print(ex,sys.exc_info())\n\n    return value, grad\n    ","execution_count":235,"outputs":[]},{"metadata":{"trusted":true,"id":"Rk25esbweHyd","outputId":"81814565-c8c0-4bd3-c2d5-76a0243efb5c"},"cell_type":"code","source":"# to_compression_ratio 함수의 test\ne = to_compression_ratio(w)\nfor t in e:\n    print(t.numpy())","execution_count":236,"outputs":[{"output_type":"stream","text":"0.12790697674418605\n0.15193798449612403\n0.14815060908084163\n","name":"stdout"}]},{"metadata":{"id":"JxlFMo3GeHyg"},"cell_type":"markdown","source":"생성된 text의 morpheme code 변환기 구현<br>\nmorpheme code는 한국어 문장들(dataset)의 morpheme code와 비교하여 한국어 문법에 가깝게 만들기 위한 목적"},{"metadata":{"trusted":true,"id":"yhOeXv93eHyh"},"cell_type":"code","source":"\n@tf.custom_gradient\ndef to_morpcoding(w):\n\n    def grad(dy):\n        dy = tf.nn.batch_normalization(dy, 0, tf.math.reduce_std(dy), 0, 2, 1e-3)  # gradient 증폭, 이래도 되는 건지....\n        dy_arr = tf.reshape(dy,(dy.shape[0],_MAX_MORP_LENGTH,1))\n        #tf.print(dy_arr)\n        dy_arr_st = tf.image.resize(dy_arr, size=(dy.shape[0],w.shape[1]),method=tf.image.ResizeMethod.BILINEAR)\n        dy_arr_st = tf.reshape(dy_arr_st,shape=(dy.shape[0],w.shape[1]))\n        return dy_arr_st\n\n    #print(w)    \n    codes = []\n    value = None\n    texts = []\n    try:\n        s_w = tf.sort(w,axis=-1,direction='DESCENDING')\n        #print(s_w)\n        for w_0r,s_w_0r in zip(w,s_w):\n            text = \"\"\n            wh = tf.where(tf.math.greater_equal(w_0r,s_w_0r[_MAX_GEN_TOKEN]))[:_MAX_GEN_TOKEN]\n            for k in tf.reshape(wh,(_MAX_GEN_TOKEN,)).numpy():\n                text += word_table[k]+' '\n                 \n            texts.append(text)    \n            \n        for sentence in texts:\n            code = morpheme_encode(sentence)\n            if len(code) <= _MAX_MORP_LENGTH:\n                codes.append(code + [_PADDING_CODE for i in range(_MAX_MORP_LENGTH-len(code))])\n            else:\n                codes.append(code[:_MAX_MORP_LENGTH])\n        value = tf.constant(codes,dtype=tf.int32)\n    except Exception as ex:\n        tf.print('to_morpcoding :' + str(ex),sys.exc_info())\n\n    return value, grad","execution_count":237,"outputs":[]},{"metadata":{"trusted":true,"id":"S-C_pxE5eHyi","outputId":"d4799f35-633c-4ced-8cff-d7e1a1edc90b"},"cell_type":"code","source":"# to_morpcoding 함수의 test\ne = to_morpcoding(w)\nfor t in e:\n    print(t.numpy())","execution_count":238,"outputs":[{"output_type":"stream","text":"[ 2  5  6  2  5  2  5  2  5  2  9  5  6  3  2  5  2  5  6  6  3  2  5 10\n  6  3  2  5  2  5  2  6  3  2  6  2 14  2  2  5  8  2  5  2  5  2  5 10\n  2  5  6  3  2  2  6  6  2  9  5  6  6  6  2  2  9  5  6  2  5  6  2  5\n  2  5  2  5  2  8  3  4  2  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0]\n[ 6  6  3  2  5  8  6  6  3  2  5  2  6  3  2  6  2  6  2  5  2  5  2  5\n  2  5  8  3  2  5 10  6  2  2  5  2  3  6  3  2  9  5  2  5  2  9  5  8\n  6  3  2  5  2  6  3  2  9  5  2  6  3  2  2  9  5  2  2  5  6  2  5  2\n  2  2  5  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0]\n[ 2  5  2  5  2  5  6  3  2  2  5  2  2  3  2  5  2  5  6  3  2  5  2  5\n  2  5  2  2  5  2  5  2  5  2  5  2  5  2  5  2  8  6  6  3 10  6  6  2\n  5  2  2  5  6  2  5  6  2  9  5  6  3  2  5  6  2  5  6  2  2  5  2  5\n  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0]\n","name":"stdout"}]},{"metadata":{"id":"hLtq1YP_eHyj"},"cell_type":"markdown","source":"Network 구성을 위해 사용자 정의 Layer 를 구현"},{"metadata":{"trusted":true,"id":"F9EfbbSreHyk"},"cell_type":"code","source":"# 이것도 잘 몰라서 하루 걸림... ㅜㅜ\n\nfrom keras import backend as K\nfrom keras.layers import Layer\n\n#tf.executing_eagerly()\n\nclass Post_processing(Layer):\n\n    def __init__(self, output_dim, encoder_func=None,Tout=tf.float64, **kwargs):\n        self.output_dim = output_dim\n        self.encoder = encoder_func\n        self.Tout = Tout\n        super(Post_processing, self).__init__(**kwargs)\n    '''\n    def build(self, input_shape):\n        tf.print('build',input_shape)\n        # 이 레이어에 대해 학습가능한 가중치 변수를 만듭니다.\n        self.kernel = self.add_weight(name='kernel', \n                                      shape=(input_shape[1], self.output_dim),\n                                      initializer='uniform',\n                                      trainable=True)\n        super(Post_processing, self).build(input_shape)  # 끝에서 꼭 이 함수를 호출하십시오\n    '''\n    def call(self, input_data):\n        #tf.print('Post_processing : call input_data',input_data.shape)\n        value = tf.py_function(self.encoder,[input_data],Tout=self.Tout,name='encode_func')\n        #print('value.shape:',value.shape)\n        #value.set_shape((input_data.shape[0],self.output_dim))\n        if self.output_dim > 0:\n            value.set_shape((input_data.shape[0],self.output_dim))\n        else:\n            value.set_shape((input_data.shape[0],))\n        #return tf.reshape(value,[input_data.shape[0]])  \n\n        #value = tf.Variable((tf.zeros([input_data.shape[0],1024]) if self.Tout==tf.float64 else tf.zeros([input_data.shape[0],])),dtype=self.Tout,shape=( (input_data.shape[0],1024) if self.Tout==tf.float64 else (input_data.shape[0],)))\n        #tf.py_function(self.encoder,[input_data],Tout=self.Tout)\n        return value\n\n    def compute_output_shape(self, input_shape):\n        tf.print('compute_output_shape:',input_shape)\n        input_shape = tensor_shape.TensorShape(input_shape).as_list()\n        if self.output_dim > 0:\n            return tensor_shape.TensorShape([input_shape[0], self.output_dim])\n        return tensor_shape.TensorShape([input_shape[0]])","execution_count":239,"outputs":[]},{"metadata":{"trusted":true,"id":"iT--4r5BeHyk","outputId":"23713139-a7df-429e-e1ca-aadc5099daa1"},"cell_type":"code","source":"# 구성한 Layer의 test\ne = Post_processing(1024,to_embedding,Tout=tf.float64)(w)\nfor c in e:\n    print(c)","execution_count":240,"outputs":[{"output_type":"stream","text":"tf.Tensor(\n[ 0.20697494 -0.16078305  0.04480495 ... -0.18544388  0.67773646\n  0.60726237], shape=(1024,), dtype=float64)\ntf.Tensor(\n[ 0.35158053 -0.53133172  0.05704678 ... -0.37279111  0.59129167\n  0.58416045], shape=(1024,), dtype=float64)\ntf.Tensor(\n[ 0.19844982 -0.38411286  0.00347876 ... -0.39883673  0.88680857\n  0.72472489], shape=(1024,), dtype=float64)\n","name":"stdout"}]},{"metadata":{"trusted":true,"id":"X0RIQRZ4eHyl"},"cell_type":"code","source":"# 별로 중요하지는 않지만 Lambda layer를 활용하기 위한 assert 함수 구성\ndef assert_layer(input_data,out_dim=None):\n    #tf.print(input_data)\n    #print(input_data)\n    assert input_data.shape[1] == out_dim\n    return input_data","execution_count":241,"outputs":[]},{"metadata":{"trusted":true,"id":"VJazT7y-eHyl","outputId":"6af4be19-38cc-4c2c-df80-b2d54802a2b4"},"cell_type":"code","source":"# 드디어 generator 구현\n# 효과적으로 구성된 것인지는 모르겠음... 이것은 아직 많은 연구가 필요함.\n# 또한 LSTM으로 바꾸어 길이의 한게를 극복해야 할 것...\n\ndef make_generator_model(org_words):\n    input = Input(shape=(org_words,), dtype='float64') \n    x1 = Dense(org_words*2, use_bias=False)(input)\n    x1 = LeakyReLU(0.2)(x1)\n    x1 = Dense(org_words*4, use_bias=False)(x1)\n    x1 = BatchNormalization()(x1)\n \n    x1 = LeakyReLU(0.2)(x1)\n    x1 = Dense(org_words*4*2, use_bias=False)(x1)\n    x1 = BatchNormalization()(x1)\n    x1 = Dense(org_words, use_bias=False,activation='tanh')(x1)\n    x1 = Lambda(assert_layer,arguments={'out_dim':org_words})(x1)\n    #x1 = Reshape((max_length, total_words))(x1)\n    #x1 = BatchNormalization()(x1)\n    #x1 = Softmax()(x1)        \n    #x1 = MyCustomLayer(max_length*total_words)(x1)\n    txt = Post_processing(0,to_text,Tout=tf.string)(x1)\n    emb = Post_processing(1024,to_embedding,Tout=tf.float64)(x1)\n    #cmr = Post_processing(0,to_compression_ratio,Tout=tf.float64)(x1)\n    cod = Post_processing(128,to_morpcoding,Tout=tf.int32)(x1)\n    \n    #model = Model(input,[txt,emb,cmr,cod])\n    model = Model(input,[txt,emb,cod])\n    \n    model.summary()\n    return model\n\ngenerator = make_generator_model(_NOISE_DIM)","execution_count":242,"outputs":[{"output_type":"stream","text":"Model: \"functional_45\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_50 (InputLayer)           [(None, 301)]        0                                            \n__________________________________________________________________________________________________\ndense_75 (Dense)                (None, 602)          181202      input_50[0][0]                   \n__________________________________________________________________________________________________\nleaky_re_lu_47 (LeakyReLU)      (None, 602)          0           dense_75[0][0]                   \n__________________________________________________________________________________________________\ndense_76 (Dense)                (None, 1204)         724808      leaky_re_lu_47[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_10 (BatchNo (None, 1204)         4816        dense_76[0][0]                   \n__________________________________________________________________________________________________\nleaky_re_lu_48 (LeakyReLU)      (None, 1204)         0           batch_normalization_10[0][0]     \n__________________________________________________________________________________________________\ndense_77 (Dense)                (None, 2408)         2899232     leaky_re_lu_48[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_11 (BatchNo (None, 2408)         9632        dense_77[0][0]                   \n__________________________________________________________________________________________________\ndense_78 (Dense)                (None, 301)          724808      batch_normalization_11[0][0]     \n__________________________________________________________________________________________________\nlambda_7 (Lambda)               (None, 301)          0           dense_78[0][0]                   \n__________________________________________________________________________________________________\npost_processing_46 (Post_proces (None,)              0           lambda_7[0][0]                   \n__________________________________________________________________________________________________\npost_processing_47 (Post_proces (None, 1024)         0           lambda_7[0][0]                   \n__________________________________________________________________________________________________\npost_processing_48 (Post_proces (None, 128)          0           lambda_7[0][0]                   \n==================================================================================================\nTotal params: 4,544,498\nTrainable params: 4,537,274\nNon-trainable params: 7,224\n__________________________________________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true,"id":"sQya4kMheHym","outputId":"72f1d967-1190-40e7-8f62-45231715270a"},"cell_type":"code","source":"# generator의 test\n# Create a random noise and generate a sample\nnoise = tf.random.normal([3,_NOISE_DIM])\ntexts,embeddings,morpcodes = generator(noise, training=True)\nprint(texts.shape)\nfor i,txt in zip(range(len(texts)),texts):\n    print(f\" {i+1}> {txt.numpy().decode('utf-8')}\" )\nprint(embeddings.shape)\nprint(compratios.shape)\nprint(morpcodes.shape)","execution_count":243,"outputs":[{"output_type":"stream","text":"(3,)\n 1> 아기가 무럭무럭 소녀가 걱정되었어요.그래서 맞이했어요.새어머니는 데리고 고약한 그런데 집안일이 저애를 언니는 언니들을 무도회장으로 마법사 빙그레 너를 도마뱀을 구해 호박이 건드렸어요.그랬더니 구슬 바뀌웠어요.신데렐라 발을 구두를 되면 마부는 왕자님도 빼았겼어요.완자님은 추었어요.신데렐라는 놀랐어요. 신데렐라가 유리 가서 결혼하겠어요.그래서 구두를 유리 수 유리 꼭 왕궁으로 \n 2> 여자 소녀의 어머니가 세상을 아버지는 두 못마땅했어요. 종일 도맡아 지칠때면난롯가에 앉아서 엄마 무도회가 고개를 내가 보내주마호박 한개와 두마리 건드리자 마차로 멋진 변했답니다. 예쁜 유리 밤 처음대로 바늗시 해. 아가씨들은 춤을 신데렐라는 신데렐라가 구두의 돌아다녔어요. 조용히 있나요?​신데렐라는 맞았어요. 왕궁으로 뒤 오래오래 \n 3> 집에 자라서 되었어요.그러던 어느날 병이들어그만 말았어요. 나이가 새어머니와 예쁘고 이번에는 재투성이잖아요. 소녀를 놀려 떠났어요.신데렐라도 신데렐라는 시작했어요.신데렐라 할머니가 있었어요. 할머니가 화려한 황금 신데렐라의 빛나는 밤 열두시가 모든게 되기 알겠지? 왕자님과 줄도 벽시계가 왕궁을 구두를 층계에서 구두를 가서 유리 보았지만한눈에 신데렐라가 다가와 \n(3, 1024)\n(10,)\n(3, 128)\n","name":"stdout"}]},{"metadata":{"id":"41aWytGveHyo"},"cell_type":"markdown","source":"# Discriminator 구현"},{"metadata":{"id":"dtjQGj54eHyo"},"cell_type":"markdown","source":"먼저 요약을 구분하기 위한 discriminator_summ 구현"},{"metadata":{"trusted":true,"id":"Fx3GgEXEeHyo"},"cell_type":"code","source":"# 문장에 대한 embeddings를 이용하여 org_text_emb (org_text의 embedding)과의 유사도를 계산한다.\nimport scipy\n\n@tf.custom_gradient\ndef to_similarity(w):\n\n    def grad(dy):\n        dy = tf.nn.batch_normalization(dy, 0, tf.math.reduce_std(dy), 0, 10, 1e-3)  # gradient 증폭, 이래도 되는 건지....\n        dy_arr = tf.reshape(dy,(dy.shape[0],1,1))\n        #tf.print(dy_arr)\n        dy_arr_st = tf.image.resize(dy_arr, size=(dy.shape[0],1024),method=tf.image.ResizeMethod.BILINEAR)\n        dy_arr_st = tf.reshape(dy_arr_st,shape=(dy.shape[0],1024))\n        return dy_arr_st\n\n    similarities = []\n    value = None\n    try:\n        for embedding in w:\n            distances = scipy.spatial.distance.cdist([embedding], [org_text_emb], \"cosine\")[0]\n            similarities.append(distances[0])\n            \n        value = tf.constant(similarities,dtype=tf.float64)\n    except Exception as ex:\n        tf.print(ex,sys.exc_info())\n\n    return value, grad\n","execution_count":244,"outputs":[]},{"metadata":{"trusted":true,"id":"DNIQNvIveHyp","outputId":"bcd5af00-f889-4672-9323-c733169eabaf"},"cell_type":"code","source":"def make_discriminator_model():\n    input_emb = Input(shape=(1024,), dtype='float64') \n    x1 = Post_processing(0,to_similarity,Tout=tf.float64)(input_emb)\n    x1 = Reshape((1,))(x1)    \n    #x1 = Dense(1024*2)(input_emb)\n    #x1 = BatchNormalization()(x1)\n    #x1 = LeakyReLU()(x1)\n\n    \n    #input_cmp = Input(shape=(), dtype='float64') \n    imput_mrp = Input(shape=(_MAX_MORP_LENGTH,), dtype='int32')\n\n    #x2 = Reshape((1,))(input_cmp)\n    x3 = Dense(256)(imput_mrp)\n    #x3 = BatchNormalization()(x3)\n    x3 = LeakyReLU()(x3)\n\n    x3 = Dense(256*3)(x3)\n    #x3 = BatchNormalization()(x3)\n    x3 = LeakyReLU()(x3)\n\n    #concatted = Concatenate(axis=1)([x1, x2, x3])\n    concatted = Concatenate(axis=1)([x1, x3])\n    x4 = Flatten()(concatted)\n    #x4 = Dense(64, use_bias=False)(x4)\n    #x4 = BatchNormalization()(x4)\n    #x4 = LeakyReLU()(x4)\n    x4 = Dense(1,activation='sigmoid')(x4)\n    \n    #model = Model([input_emb,input_cmp,imput_mrp],x4)\n    model = Model([input_emb,imput_mrp],x4)\n    model.summary()\n    \n    return model\n\ndiscriminator = make_discriminator_model()","execution_count":245,"outputs":[{"output_type":"stream","text":"Model: \"functional_47\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_52 (InputLayer)           [(None, 128)]        0                                            \n__________________________________________________________________________________________________\ndense_79 (Dense)                (None, 256)          33024       input_52[0][0]                   \n__________________________________________________________________________________________________\ninput_51 (InputLayer)           [(None, 1024)]       0                                            \n__________________________________________________________________________________________________\nleaky_re_lu_49 (LeakyReLU)      (None, 256)          0           dense_79[0][0]                   \n__________________________________________________________________________________________________\npost_processing_49 (Post_proces (None,)              0           input_51[0][0]                   \n__________________________________________________________________________________________________\ndense_80 (Dense)                (None, 768)          197376      leaky_re_lu_49[0][0]             \n__________________________________________________________________________________________________\nreshape_26 (Reshape)            (None, 1)            0           post_processing_49[0][0]         \n__________________________________________________________________________________________________\nleaky_re_lu_50 (LeakyReLU)      (None, 768)          0           dense_80[0][0]                   \n__________________________________________________________________________________________________\nconcatenate_16 (Concatenate)    (None, 769)          0           reshape_26[0][0]                 \n                                                                 leaky_re_lu_50[0][0]             \n__________________________________________________________________________________________________\nflatten_16 (Flatten)            (None, 769)          0           concatenate_16[0][0]             \n__________________________________________________________________________________________________\ndense_81 (Dense)                (None, 1)            770         flatten_16[0][0]                 \n==================================================================================================\nTotal params: 231,170\nTrainable params: 231,170\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true,"id":"y7owqbg4eHyp","outputId":"bfe220fb-944b-4e07-ecd8-d71f3580f55b"},"cell_type":"code","source":"# discriminator test\n\npredict = discriminator([embeddings,morpcodes])\nprint(predict)","execution_count":246,"outputs":[{"output_type":"stream","text":"tf.Tensor(\n[[0.5961309 ]\n [0.78245354]\n [0.85123605]], shape=(3, 1), dtype=float32)\n","name":"stdout"}]},{"metadata":{"id":"mmvyPAVSeHyq"},"cell_type":"markdown","source":"# GAN 을 이용한 loss 의 gradient 구현 --> 빡심"},{"metadata":{"trusted":true,"id":"ECEMH-yweHyr"},"cell_type":"code","source":"# This method returns a helper function to compute cross entropy loss\ncross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\ndef discriminator_loss(real_output, fake_output):\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n    total_loss = real_loss + fake_loss\n    return total_loss\n\ndef generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), fake_output)\n\ngenerator_optimizer = tf.keras.optimizers.Adam(1e-4)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(1e-4)","execution_count":247,"outputs":[]},{"metadata":{"trusted":true,"id":"cvBC5yS6eHys"},"cell_type":"code","source":"# 디짐\n@tf.function\ndef train_step(real_embedding,real_morpcoding):\n  \n    # 1 - Create a random noise to feed it into the model\n    # for the text generation\n    noise = tf.random.normal([BATCH_SIZE, _NOISE_DIM])\n    \n    # 2 - Generate text and calculate loss values\n    # GradientTape method records operations for automatic differentiation.\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        texts,embeddings,morpcodes = generator(noise, training=True)\n        #embeddings = generator(noise, training=True)\n        # real에 가까우려면 discriminator의 학습은 real_embedding 이 zero (0)에 가깝게 학습시켜야 함.\n        # 하지만 압축율의 개념으로는 본래는 ones (1)가 맞음.\n        real_output = discriminator([real_embedding,real_morpcoding], training=True)\n        fake_output = discriminator([embeddings,morpcodes], training=True)\n\n        #tf.print('train_step : embeddings.shape=',embeddings.shape)\n        gen_loss = generator_loss(fake_output)\n        #tf.print('train_step : gen_loss=',gen_loss)\n        disc_loss = discriminator_loss(real_output, fake_output)\n        #tf.print('train_step : disc_loss=',disc_loss)\n\n    # 3 - Calculate gradients using loss values and model variables\n    # \"gradient\" method computes the gradient using \n    # operations recorded in context of this tape (gen_tape and disc_tape).\n    \n    # It accepts a target (e.g., gen_loss) variable and \n    # a source variable (e.g.,generator.trainable_variables)\n    # target --> a list or nested structure of Tensors or Variables to be differentiated.\n    # source --> a list or nested structure of Tensors or Variables.\n    # target will be differentiated against elements in sources.\n\n    # \"gradient\" method returns a list or nested structure of Tensors  \n    # (or IndexedSlices, or None), one for each element in sources. \n    # Returned structure is the same as the structure of sources.\n    \n    gradients_of_discriminator = disc_tape.gradient(disc_loss, \n                                                discriminator.trainable_variables)\n    #tf.print('train_step : gradients_of_discriminator=',gradients_of_discriminator)   \n    noise = tf.random.normal([BATCH_SIZE, _NOISE_DIM])\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        texts,embeddings,morpcodes = generator(noise, training=True)\n        #embeddings = generator(noise, training=True)\n        #real_output = discriminator(real_embedding, training=True)\n        fake_output = discriminator([embeddings,morpcodes], training=True)\n\n        #tf.print('train_step : embeddings.shape=',embeddings.shape)\n        gen_loss = generator_loss(fake_output)\n        #tf.print('train_step : gen_loss=',gen_loss)\n        #disc_loss = discriminator_loss(real_output, fake_output)\n        #tf.print('train_step : disc_loss=',disc_loss)\n\n    gradients_of_generator = gen_tape.gradient(gen_loss, \n                                               generator.trainable_variables)\n    #tf.print('train_step : gradients_of_generator=',gradients_of_generator)\n \n\n    # 4 - Process  Gradients and Run the Optimizer\n    # \"apply_gradients\" method processes aggregated gradients. \n    # ex: optimizer.apply_gradients(zip(grads, vars))\n    \"\"\"\n    Example use of apply_gradients:\n    grads = tape.gradient(loss, vars)\n    grads = tf.distribute.get_replica_context().all_reduce('sum', grads)\n    # Processing aggregated gradients.\n    optimizer.apply_gradients(zip(grads, vars), experimental_aggregate_gradients=False)\n    \"\"\"\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n    #tf.print('train_step : after discriminator_optimizer')    ","execution_count":248,"outputs":[]},{"metadata":{"trusted":true,"id":"cJs3lBpJeHys"},"cell_type":"code","source":"EPOCHS = 5\n# 요약문 생성의 확인을 위해 10개의 문장을 생성하고 train과정에서 각 epoch마다 변화를 확인한다.\nseed = tf.random.normal([10, _NOISE_DIM])","execution_count":249,"outputs":[]},{"metadata":{"trusted":true,"id":"os3b0psFeHyt","outputId":"fd68a0d6-e274-491e-bbe7-32beba7130ad"},"cell_type":"code","source":"# 생성된 문장의 원문 유사도를 측정하기 위한 함수\n\nimport scipy\n#print(doc_emb)\ndef similarity_score(queries,org_embedding):\n\n    total_score = 0\n    query_embeddings = embedder.encode(queries,show_progress_bar=False)\n    for query, query_embedding in zip(queries, query_embeddings):\n        distances = scipy.spatial.distance.cdist([query_embedding], [org_embedding], \"cosine\")[0]\n        results = zip(range(len(distances)), distances)\n        for idx, distance in results:\n            total_score += 1-distance\n    return total_score\n\nqueries = []\n\ntexts,embeddings,morpcodes = generator(seed,training=False)\n\n#count = 0\nfor t in texts:\n    summary_text = t.numpy().decode('utf-8')\n    queries.append(summary_text)\nprint('Similarity score:',str(similarity_score(queries,org_text_emb)))\n","execution_count":250,"outputs":[{"output_type":"stream","text":"Similarity score: 5.708030546274871\n","name":"stdout"}]},{"metadata":{"trusted":true,"id":"cXrsOXX4eHyt"},"cell_type":"code","source":"# Print iterations progress\nclass ProgressBar:\n    # pb = ProgressBar(total=20, prefix = 'Epoch 1')\n    def __init__(self,total=20, prefix = '', suffix = '', decimals = 1, length = 20, fill = '█', printEnd = \"\\r\"):\n        self.total = total\n        self.prefix = prefix\n        self.suffix = suffix\n        self.decimals = decimals\n        self.length = length\n        self.fill = fill\n        self.printEnd = printEnd\n        self.ite = 0\n    # pb.printProgress(1,'~~~~')\n    def printProgress(self,iteration, text):\n        self.ite += iteration\n        percent = (\"{0:.\" + str(self.decimals) + \"f}\").format(100 * (self.ite / float(self.total)))\n\n        filledLength = int(self.length * self.ite // self.total)\n        bar = self.fill * filledLength + '-' * (self.length - filledLength)\n        print(f'\\r{self.prefix} |{bar}| {percent}% {self.suffix}  {text}', end=\"\", flush=True)\n        # Print New Line on Complete\n        if self.ite == self.total: \n            print()","execution_count":251,"outputs":[]},{"metadata":{"trusted":true,"id":"dhqdaIATeHyu"},"cell_type":"code","source":"import time\nfrom IPython import display # A command shell for interactive computing in Python.\nimport re\n\ndef train(dataset, epochs):\n    print('Start with seed text.')\n    seed = tf.random.normal([10, _NOISE_DIM])\n    print('-------------------------------------------------------')\n    texts,embeddings,morpcodes = generator(seed,training=False)\n    for i,t in  zip(range(len(texts)),texts):\n        summary_text = t.numpy().decode('utf-8')\n        print(f'{i+1} > {summary_text}')        \n    print('-------------------------------------------------------')\n    \n    predict = discriminator([embeddings,morpcodes])\n    socre = 0\n    for s in predict:\n        socre += s.numpy()\n    print(f'Initial score:{socre}')\n    print(' ')\n\n    # A. For each epoch, do the following:\n    for epoch in range(epochs):\n        start = time.time()\n        pb = ProgressBar(total=BATCH_COUNT, prefix = f'Epoch:{str(epoch+1)}/{epochs}')\n        pb.printProgress(0,'Start batch.')\n        # 1 - For each batch of the epoch, \n        for batch_num,(emb_batch_set,cod_batch_set) in zip(range(len(dataset)),dataset):\n            # 1.a - run the custom \"train_step\" function\n            # we just declared above\n            #print(image_batch.shape)\n            train_step(emb_batch_set,cod_batch_set)\n            texts,embeddings,morpcodes = generator(seed,training=False)\n            predict = discriminator([embeddings,morpcodes])\n            socre = 0\n            for s in predict:\n                socre += s.numpy()\n            pb.printProgress(+1,f\"Time for batch {batch_num + 1}/{BATCH_COUNT} is {int(time.time()-start)} sec, score:{socre}, text:{texts[0].numpy().decode('utf-8')}\")\n        # 4 - Print out the completed epoch no. and the time spent\n        #print (f'Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n        #texts,embeddings,compratios,morpcodes = generator(seed,training=False)\n        count = 0\n        queries = []\n        texts,embeddings,morpcodes = generator(seed,training=False)\n        for i,t in  zip(range(len(texts)),texts):\n            summary_text = t.numpy().decode('utf-8')\n            print(f'{i+1} > {summary_text}')\n            queries.append(summary_text)\n            c = [m.start() for m in re.finditer(_MISMATCH_WORD, summary_text)]\n            count += len(c)\n        predict = discriminator([embeddings,morpcodes])\n        socre = 0\n        for s in predict:\n            socre += s.numpy()            \n        print(f'Discriminator score:{socre} Similarity score:{str(similarity_score(queries,org_text_emb))}')\n        print('')","execution_count":252,"outputs":[]},{"metadata":{"id":"RCAedaSeT6A1","trusted":true,"outputId":"4d3fc962-e36c-45a8-9516-44c6137270bb"},"cell_type":"code","source":"if DO_ALL==False:\n    EPOCHS = 30\n    train(dataset, EPOCHS)","execution_count":253,"outputs":[{"output_type":"stream","text":"Start with seed text.\n-------------------------------------------------------\n1 > 마음씨 소녀가 되었어요.그러던 말았어요. 소녀의 소녀가 걱정되었어요.그래서 데리고 소녀가 못마땅했어요. 쉬곤 했지요. 집에도 초대장이 왔어요. 데리고 무도회에 싶니?신데렐라가 빙그레 지팡이로 생쥐와 반짝반짝 열두시가 돼.그러니까 아름다운 모인 다른 추었어요.신데렐라는 추느라 땡...... 놀랐어요. 한 벗겨졌어요.하지만 유리 온 유리 그때 볼 수 있나요?​신데렐라는 \n2 > 집에 태어났어요.아기는 예쁘고 아버지는 나이가 심술쟁이들이었어요.새어머니는 게 엄마 열렸어요.신데렐라의 집에도 왔어요. 남은 싶니?신데렐라가 할머니가 무도회에 한개와 생쥐 도마뱀을 구해 건드리자 호박이 화려한 생쥐와 생쥐는 옷도 내밀어 열두시가 되면 밤 해. 아름다운 추느라 알리는 구두 결혼하겠어요.그래서 유리 보았지만한눈에 구두는 그때 한번 \n3 > 태어났어요.아기는 무럭무럭 어느날 말았어요. 아버지는 나이가 못마땅했어요. 이번에는 날 집에도 초대장이 싶니?신데렐라가 마법사 빙그레 너를 무도회에 외웠어요.그리고 호박을 화려한 변했어요.이번에는 신데렐라에게 호박으로 도마뱀으로 아름다운 다른 가는 소리에 화들짝 구두 유리 돌아다녔어요. 구두는 너무 그때 말했어요.저도 수 있나요?​신데렐라는 신하게 건넨 구두는 \n4 > 집에 예쁘고 아버지는 나이가 데리고 왔어요.그러나 게 했지요. 집에도 왔어요. 남은 울기 시작했어요.신데렐라 빙그레 보내주마호박 두마리 주문을 호박이 생쥐는 마부로 옷도 신데렐라에게 빛나는 신겨 모든게 흰말은 아름다운 마음을 아가씨들은 줄도 시를 구두 결혼하겠어요.그래서 유리 구두는 그때 수 구두는 신데렐라의 그 \n5 > 예쁘고 어느날 말았어요. 소녀의 아버지는 두 왔어요.그러나 게 저애를 재투성이잖아요. 무도회가 초대장이 왔어요. 남은 신데렐라는 마법사 할머니가 빙그레 너를 외웠어요.그리고 생쥐는 신데렐라에게 처음대로 아름다운 신데렐라에게 모인 다른 아가씨들은 추느라 빠져나가는데유리 벗겨졌어요.하지만 주울 유리 짝을 구두의 주인을 돌아다녔어요. 수 있나요?​신데렐라는 맞았어요. \n6 > 아기가 태어났어요.아기는 예쁘고 마음씨 어느날 소녀의 소녀가 맞이했어요.새어머니는 왔어요.그러나 쉬곤 저애를 집에도 언니들을 싶니?신데렐라가 있었어요. 생쥐 도마뱀을 생쥐와 마부로 옷도 예쁜 신데렐라에게 구두를 밤 열두시가 처음대로 밤 해. 아름다운 시를 주울 유리 짝을 신하들은 온 오므려도 구두는 그때 있나요?​신데렐라는 건넨 \n7 > 집에 무럭무럭 예쁘고 어느날 소녀의 아버지는 걱정되었어요.그래서 착한 게 집안일을 했어요.해도 해도 쉬곤 재투성이잖아요. 집에도 데리고 싶니?신데렐라가 생쥐 호박을 호박이 생쥐와 도마뱀은 내밀어 신데렐라에게 반짝반짝 쳐다보지도 춤을 추었어요.신데렐라는 열두 소리에 주울 층계에서 유리 짝을 유리 찾아 온 신어 있나요?​신데렐라는 발에 \n8 > 태어났어요.아기는 무럭무럭 남은 맞이했어요.새어머니는 데리고 집안일을 재투성이잖아요. 놀려 초대장이 왔어요. 남은 싶니?신데렐라가 너를 무도회에 보내주마호박 외웠어요.그리고 건드리자 호박이 신데렐라에게 구두를 신겨 모든게 생쥐로 돼.그러니까 밤 해. 아름다운 모인 않고신데렐라하고만 왕자님과 벽시계가 왕궁을 짝이 주울 유리 유리 구두의 오므려도 구두를 유리 \n9 > 태어났어요.아기는 예쁘고 아버지는 나이가 소녀가 쉬곤 재투성이잖아요. 언니는 집에도 데리고 훌쩍훌쩍 시작했어요.신데렐라 빙그레 보내주마호박 한개와 생쥐 화려한 건드렸어요.그랬더니 옷도 신겨 도마뱀으로 해. 왕자님도 아름다운 다른 춤을 벽시계가 시를 소리에 구두 주울 왕자님은 층계에서 신하들은 찾아 나라를 발을 유리 한번 있나요?​신데렐라는 \n10 > 어느날 아버지는 나이가 왔어요.그러나 돌아가셨어요.소녀는 신데렐라라고 무도회가 집에도 초대장이 왔어요. 떠났어요.신데렐라도 너도 두마리 마법사 주문을 외웠어요.그리고 지팡이로 건드리자 호박이 생쥐와 건드렸어요.그랬더니 생쥐는 옷도 신데렐라에게 구두를 신겨 모든게 마차는 밤 신데렐라에게 마음을 추었어요.신데렐라는 왕자님과 화들짝 층계에서 신하들은 온 유리 있나요?​신데렐라는 구두는 \n-------------------------------------------------------\nInitial score:[5.871543]\n \nEpoch:1/30 |--------------------| 2.0%   Time for batch 1/50 is 55 sec, score:[3.099727], text:마음씨 소녀가 되었어요.그러던 말았어요. 소녀의 소녀가 걱정되었어요.그래서 데리고 왔어요.그러나 성질이 소녀가 못마땅했어요. 쉬곤 했지요. 집에도 초대장이 왔어요. 데리고 무도회에 싶니?신데렐라가 지팡이로 생쥐와 반짝반짝 열두시가 돼.그러니까 모인 다른 추었어요.신데렐라는 추느라 땡...... 놀랐어요. 한 벗겨졌어요.하지만 유리 온 유리 그때 볼 수 있나요?​신데렐라는 ","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-253-a1dc23f1f05a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mDO_ALL\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mEPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-252-a6d45b7e3e05>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, epochs)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;31m# we just declared above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;31m#print(image_batch.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_batch_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcod_batch_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmorpcodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmorpcodes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"org_term_set = org_text.split(' ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(org_term_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 그냥 테스트....\n\ndef text_gen(w):\n    s_w = tf.sort([w],axis=-1,direction='DESCENDING')\n    #print(s_w)\n    #for w_0r,s_w_0r in zip(w,s_w):\n    w_0r = w\n    s_w_0r = s_w[0]\n    text = \"\"\n    wh = tf.where(tf.math.greater_equal(w_0r,s_w_0r[_MAX_GEN_TOKEN]))[:_MAX_GEN_TOKEN]\n    order = tf.reshape(wh,(_MAX_GEN_TOKEN,)).numpy()\n    for k in order:\n        text += word_table[k]+' '    \n    return text,order \n                \n\nnoise = tf.random.normal([_NOISE_DIM])    \ntxt, ord = text_gen(noise)\nprint(txt)\nprint(ord)\n'''\nfor i in range(10):\n    noise = tf.random.normal([1,_NOISE_DIM])\n    texts,embeddings,compratios,morpcodes = generator(noise, training=True)\n    p_score = 0\n    for txt in texts:\n        embedding = embedder.encode([txt.numpy().decode('utf-8')],show_progress_bar=False)\n        distances = scipy.spatial.distance.cdist(embedding, [org_text_emb], \"cosine\")[0]\n        score = 1-distances[0]\n        reward = p_score - score\n        if p_score == 0:\n            reward = -reward\n        print(f'score:{score} reward:{reward}') # 1-distance\n        p_score = score\n'''\npass","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}
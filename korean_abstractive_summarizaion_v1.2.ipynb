{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "korean_real_summary_v1.1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dolmani38/Summary/blob/master/korean_abstractive_summarizaion_v1.2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdM3q73ReHxs"
      },
      "source": [
        "# **Korean Summarizer Using Multiple Discriminators**\n",
        "\n",
        "참조 : https://www.kaggle.com/shivamb/beginners-guide-to-text-generation-using-lstms\n",
        "\n",
        "참조 : https://github.com/williamSYSU/TextGAN-PyTorch\n",
        "\n",
        "* 2020년12월27일 v1.0 완전히 실패...\n",
        "* Generator 새로 제작!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNBNW5dMZ13G"
      },
      "source": [
        "DO_ALL = True # 전체 실행하면서 시간 걸리는 걸 Pass 하려면 이걸 False ...\r\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "FlvsCFJaeHxt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7c14366-e5d9-4da7-8fad-0dac65f47dd4"
      },
      "source": [
        "\n",
        "if DO_ALL:\n",
        "    !pip install sentence-transformers==0.3.0\n",
        "    !pip install transformers==3.0.2\n",
        "    !pip install wikipedia\n",
        "    !pip install konlpy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers==0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/23/833e0620753a36cb2f18e2e4a4f72fd8c49c123c3f07744b69f8a592e083/sentence-transformers-0.3.0.tar.gz (61kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 3.4MB/s \n",
            "\u001b[?25hCollecting transformers>=3.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.0) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.0) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.0) (1.19.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.0) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.0) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.0) (3.2.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (20.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 37.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (2019.12.20)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 29.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.1->sentence-transformers==0.3.0) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.1->sentence-transformers==0.3.0) (3.7.4.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers==0.3.0) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers==0.3.0) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers>=3.0.2->sentence-transformers==0.3.0) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=3.0.2->sentence-transformers==0.3.0) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers, sacremoses\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.3.0-cp36-none-any.whl size=86754 sha256=ed0f7fa9fc54d6e7a87ab2e20470d19e85f28764d5d948f0f9090619332d4699\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/23/85/85d6a9a6c68f0625a1ecdaad903bb0a78df058c10cf74f9de4\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=5161918000e17ba1ddc626312490ffce2e720f02bcff10438c08929cafb54e95\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sentence-transformers sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers, sentence-transformers\n",
            "Successfully installed sacremoses-0.0.43 sentence-transformers-0.3.0 tokenizers-0.9.4 transformers-4.1.1\n",
            "Collecting transformers==3.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 5.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (1.19.4)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 37.1MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 39.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (20.8)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.0.43)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (2019.12.20)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.0.2) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (2020.12.5)\n",
            "Installing collected packages: tokenizers, sentencepiece, transformers\n",
            "  Found existing installation: tokenizers 0.9.4\n",
            "    Uninstalling tokenizers-0.9.4:\n",
            "      Successfully uninstalled tokenizers-0.9.4\n",
            "  Found existing installation: transformers 4.1.1\n",
            "    Uninstalling transformers-4.1.1:\n",
            "      Successfully uninstalled transformers-4.1.1\n",
            "Successfully installed sentencepiece-0.1.94 tokenizers-0.8.1rc1 transformers-3.0.2\n",
            "Collecting wikipedia\n",
            "  Downloading https://files.pythonhosted.org/packages/67/35/25e68fbc99e672127cc6fbb14b8ec1ba3dfef035bf1e4c90f78f24a80b7d/wikipedia-1.4.0.tar.gz\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-cp36-none-any.whl size=11686 sha256=b535d37e6fcdddebb2823a66c984b18c66ab1e84c873fc8fa5d2f8210c1db8fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/2a/18/4e471fd96d12114d16fe4a446d00c3b38fb9efcb744bd31f4a\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n",
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 72.1MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/21/9e2c0dbf9df856e6392a1aec1d18006c60b175aa4e31d351e8278a8a63c0/JPype1-1.2.0-cp36-cp36m-manylinux2010_x86_64.whl (453kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 36.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting tweepy>=3.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/67/c3/6bed87f3b1e5ed2f34bd58bf7978e308c86e255193916be76e5a5ce5dfca/tweepy-3.10.0-py2.py3-none-any.whl\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 7.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.19.4)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Installing collected packages: JPype1, tweepy, colorama, beautifulsoup4, konlpy\n",
            "  Found existing installation: tweepy 3.6.0\n",
            "    Uninstalling tweepy-3.6.0:\n",
            "      Successfully uninstalled tweepy-3.6.0\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.2.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2 tweepy-3.10.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Em1oCkJceHxz"
      },
      "source": [
        "# keras module for building LSTM \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "from tensorflow.python.framework import tensor_shape\n",
        "import keras.utils as ku \n",
        "\n",
        "# set seeds for reproducability\n",
        "from tensorflow.random import set_seed\n",
        "from numpy.random import seed\n",
        "set_seed(2)\n",
        "seed(1)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string, os \n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFdj0QfSeHx1"
      },
      "source": [
        "# 학습을 위한 데이터 준비"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94NlJEeHeHx3"
      },
      "source": [
        "네이버 뉴스에서 아무거나 하나 Text를 얻어옴\n",
        "\n",
        "이것을 '요약' 목표"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "lz5XtC9MeHx5"
      },
      "source": [
        "org_text = \"\"\"\n",
        "옛날 어느 집에 귀여운 여자 아기가 태어났어요.\n",
        "\n",
        "아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요.\n",
        "\n",
        "그러던 어느날, 소녀의 어머니가 병이들어\n",
        "\n",
        "그만 세상을 떠나고 말았어요.\n",
        "\n",
        " \n",
        "\n",
        "소녀의 아버지는 홀로 남은 소녀가 걱정되었어요.\n",
        "\n",
        "그래서 얼마 후 새어머니를 맞이했어요.\n",
        "\n",
        "새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요.\n",
        "\n",
        "그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요.\n",
        "\n",
        "새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요.\n",
        "\n",
        " \n",
        "\n",
        "그런데 이번에는 아버지마저 돌아가셨어요.\n",
        "\n",
        "소녀는 하녀처럼 하루 종일 쓸고, 닦고, 집안일을 도맡아 했어요.\n",
        "\n",
        "해도 해도 끝이 없는 집안일이 힘들어 지칠때면\n",
        "\n",
        "난롯가에 앉아서 잠시 쉬곤 했지요.\n",
        "\n",
        " \n",
        "\n",
        "\"엄마, 저애를 신데렐라라고 불러야겠어요.\"\n",
        "\n",
        "\"온통 재투성이잖아요. 호호호!\" 두 언니는 소녀를 놀려 댔어요.\n",
        "\n",
        " \n",
        "\n",
        "어느 날, 왕궁에서 무도회가 열렸어요.\n",
        "\n",
        "신데렐라의 집에도 초대장이 왔어요.\n",
        "\n",
        " \n",
        "\n",
        "새어머니는 언니들을 데리고 무도회장으로 떠났어요.\n",
        "\n",
        "신데렐라도 무도회에 가고 싶었어요.\n",
        "\n",
        "혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요.\n",
        "\n",
        "\"신데렐라, 너도 무도회에 가고 싶니?\"\n",
        "\n",
        "신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요.\n",
        "\n",
        " \n",
        "\n",
        "\"내가 너를 무도회에 보내주마\n",
        "\n",
        "호박 한개와 생쥐 두마리, 도마뱀을 구해 오렴.\"\n",
        "\n",
        "  \n",
        "\n",
        "마법사 할머니가 주문을 외웠어요.\n",
        "\n",
        "그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금 마차로 변했어요.\n",
        "\n",
        "이번에는 생쥐와 도마뱀을 건드렸어요.\n",
        "\n",
        "그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했답니다.\n",
        "\n",
        " \n",
        "\n",
        "신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요.\n",
        "\n",
        "\"신데렐라, 발을 내밀어 보거라.\"\n",
        "\n",
        "할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요\n",
        "\n",
        "\"신데렐라, 밤 열두시가 되면 모든게 처음대로 돌아간단다.\n",
        "\n",
        "황금 마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼.\n",
        "\n",
        "그러니까 바늗시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지?\"\n",
        "\n",
        " 왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요.\n",
        "\n",
        "완자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고,\n",
        "\n",
        "신데렐라하고만 춤을 추었어요.\n",
        "\n",
        "신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요.\n",
        "\n",
        "땡, 땡, 땡...... 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요.\n",
        "\n",
        " \n",
        "\n",
        "신데렐라가 허둥지둥 왕궁을 빠져나가는데,\n",
        "\n",
        "유리 구두 한 짝이 벗겨졌어요.\n",
        "\n",
        "하지만 구두를 주울 틈이 없었어요.\n",
        "\n",
        "\n",
        "신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요.\n",
        "\n",
        "왕자님은 유리 구두를 가지고 임금님께 가서 말했어요.\n",
        "\n",
        "\"이 유리 구두의 주인과 결혼하겠어요.\"\n",
        "\n",
        "그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요.\n",
        "\n",
        " 언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만\n",
        "\n",
        "한눈에 보기에도 유리 구두는 너무 작았어요.\n",
        "\n",
        " \n",
        "\n",
        "그때, 신데렐라가 조용히 다가와 말했어요.\n",
        "\n",
        "\"저도 한번 신어 볼 수 있나요?\"\n",
        "\n",
        "​신데렐라는 신하게 건넨 유리 구두를 신었어요,\n",
        "\n",
        "유리 구두는 신데렐라의 발에 꼭 맞았어요.\n",
        "\n",
        " \n",
        "\n",
        "신하들은 신데렐라를 왕궁으로 데리고 갔어요.\n",
        "\n",
        " \n",
        "\n",
        "그 뒤 신데렐라는 완자님과 결혼하여 오래오래 행복하게 살았대요.\n",
        "\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "qa-qIW1h3DkA",
        "outputId": "57080934-5a4d-4ab4-d0d7-e84205455e41"
      },
      "source": [
        "org_text"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n옛날 어느 집에 귀여운 여자 아기가 태어났어요.\\n\\n아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요.\\n\\n그러던 어느날, 소녀의 어머니가 병이들어\\n\\n그만 세상을 떠나고 말았어요.\\n\\n \\n\\n소녀의 아버지는 홀로 남은 소녀가 걱정되었어요.\\n\\n그래서 얼마 후 새어머니를 맞이했어요.\\n\\n새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요.\\n\\n그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요.\\n\\n새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요.\\n\\n \\n\\n그런데 이번에는 아버지마저 돌아가셨어요.\\n\\n소녀는 하녀처럼 하루 종일 쓸고, 닦고, 집안일을 도맡아 했어요.\\n\\n해도 해도 끝이 없는 집안일이 힘들어 지칠때면\\n\\n난롯가에 앉아서 잠시 쉬곤 했지요.\\n\\n \\n\\n\"엄마, 저애를 신데렐라라고 불러야겠어요.\"\\n\\n\"온통 재투성이잖아요. 호호호!\" 두 언니는 소녀를 놀려 댔어요.\\n\\n \\n\\n어느 날, 왕궁에서 무도회가 열렸어요.\\n\\n신데렐라의 집에도 초대장이 왔어요.\\n\\n \\n\\n새어머니는 언니들을 데리고 무도회장으로 떠났어요.\\n\\n신데렐라도 무도회에 가고 싶었어요.\\n\\n혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요.\\n\\n\"신데렐라, 너도 무도회에 가고 싶니?\"\\n\\n신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요.\\n\\n \\n\\n\"내가 너를 무도회에 보내주마\\n\\n호박 한개와 생쥐 두마리, 도마뱀을 구해 오렴.\"\\n\\n  \\n\\n마법사 할머니가 주문을 외웠어요.\\n\\n그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금 마차로 변했어요.\\n\\n이번에는 생쥐와 도마뱀을 건드렸어요.\\n\\n그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했답니다.\\n\\n \\n\\n신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요.\\n\\n\"신데렐라, 발을 내밀어 보거라.\"\\n\\n할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요\\n\\n\"신데렐라, 밤 열두시가 되면 모든게 처음대로 돌아간단다.\\n\\n황금 마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼.\\n\\n그러니까 바늗시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지?\"\\n\\n 왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요.\\n\\n완자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고,\\n\\n신데렐라하고만 춤을 추었어요.\\n\\n신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요.\\n\\n땡, 땡, 땡...... 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요.\\n\\n \\n\\n신데렐라가 허둥지둥 왕궁을 빠져나가는데,\\n\\n유리 구두 한 짝이 벗겨졌어요.\\n\\n하지만 구두를 주울 틈이 없었어요.\\n\\n\\n신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요.\\n\\n왕자님은 유리 구두를 가지고 임금님께 가서 말했어요.\\n\\n\"이 유리 구두의 주인과 결혼하겠어요.\"\\n\\n그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요.\\n\\n 언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만\\n\\n한눈에 보기에도 유리 구두는 너무 작았어요.\\n\\n \\n\\n그때, 신데렐라가 조용히 다가와 말했어요.\\n\\n\"저도 한번 신어 볼 수 있나요?\"\\n\\n\\u200b신데렐라는 신하게 건넨 유리 구두를 신었어요,\\n\\n유리 구두는 신데렐라의 발에 꼭 맞았어요.\\n\\n \\n\\n신하들은 신데렐라를 왕궁으로 데리고 갔어요.\\n\\n \\n\\n그 뒤 신데렐라는 완자님과 결혼하여 오래오래 행복하게 살았대요.\\n\\n\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-k66tHNeHx6"
      },
      "source": [
        "한국어 문법체계에 따라 요약문을 생성하기 위해 한국어 문장 샘플을 준비\n",
        "\n",
        "'한글 위키백과'에서 임의의 문장을 수집 함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "oi6AfKSzeHx7"
      },
      "source": [
        "#한국어 위키백과에서 스크랩핑\n",
        "\n",
        "import wikipedia as wiki\n",
        "wiki.set_lang('ko')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ekFwbQVxeHx7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b600fea-389d-4f8c-afea-3838dfad720b"
      },
      "source": [
        "# '전래동화' 라는 keyword로 100개 page의 Text를 취득\n",
        "\n",
        "def __search_from_wiki(question,max_rank):\n",
        "    results = wiki.search(question,results=max_rank)\n",
        "    print(results)\n",
        "    contents = []\n",
        "    for result in results:\n",
        "        try:\n",
        "            page = wiki.page(result)\n",
        "            #print(f\"Top wiki result: {page}\")\n",
        "            text = page.content\n",
        "            ln = len(text)\n",
        "            print(f'Collecting page : {page} , text length {str(ln)}')\n",
        "            #if ln < 4000:\n",
        "            #  contents.append(text)\n",
        "            #else:\n",
        "            #  contents.append(text[0:4000])\n",
        "            contents.append(text)\n",
        "        except Exception as ex:\n",
        "          print(ex)\n",
        "    return contents\n",
        "\n",
        "if DO_ALL:\n",
        "    ko_grammar_set_raw = __search_from_wiki(\"전래동화\", 100)\n",
        "\n",
        "print(f'전체 수집한 Page Count : {len(ko_grammar_set_raw)}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['동화', '신 전래동화', '꾸러기 수비대', '아동 문학', '호시조라 미유키', '한국의 사찰', '해와 달이 된 오누이', '잠자는 숲속의 미녀', '거북', '이한갈', '정은찬', '김기두 (배우)', '옛날 옛적에 (애니메이션)', '계룡선녀전 (드라마)', '동요', '이상훈 (1976년)', '최홍일', '육진수 (격투기 선수)', '제비', '장석현 (연예인)', '유다미', '밀교 (불교)', '최지웅', '박재훈 (배우)', '한다은', '재희', '정미남', '안젤리나 다닐로바', '도깨비', '기탄교육', '국지용', '의왕백운호수축제', '박민경', '정정아', '콩딱쿵! 이야기 주머니', '윤기원 (배우)', '선녀와 나무꾼', '남생이', '대장화홍련전', '콩쥐팥쥐 (동음이의)', '토끼전', '미녀와 야수', '지대한', '이서휴게소', '은비까비의 옛날옛적에', '빨간 자전거', '콩쥐팥쥐전', '이설구', '홍석연', '아시리아인', '백조의 호수', '김덕현 (배우)', '티베트', '서예', '도교', '켈트 다신교', '금도끼 은도끼', '장화, 홍련 (동음이의)', '허구 국가', '노상현', '제네시스 (밴드)', '타이완의 문화', '도깨비 (동음이의)', '골디락스', '선녀강림', '손춘익', '자와어', '안동국제탈춤페스티벌', '윌리엄 버틀러 예이츠', '성덕대왕신종', '모모타로', '고려', '한국 문학', '라푼젤 (영화)', '토비트', '조선 후기의 문학', '송월동 동화마을', '홍석천', '계룡선녀전', '이집트', '응우옌 왕조', '안지환', '아시아의 역사', '외래어', '메이플 월드', '프랑스인', '스키타이족', '인도네시아', '이탈리아', 'MBC 창작동요제', '네버랜드', '일본 제국', '프랑크인', '바이킹', '김환영 (작가)', '드랑 나흐 오스텐', '조선', '오윤 (화가)', '원나라', '앱북']\n",
            "Collecting page : <WikipediaPage '동화'> , text length 685\n",
            "Collecting page : <WikipediaPage '신 전래동화'> , text length 156\n",
            "Collecting page : <WikipediaPage '꾸러기 수비대'> , text length 1554\n",
            "Collecting page : <WikipediaPage '아동 문학'> , text length 678\n",
            "Collecting page : <WikipediaPage '호시조라 미유키'> , text length 1601\n",
            "Collecting page : <WikipediaPage '한국의 사찰'> , text length 885\n",
            "Collecting page : <WikipediaPage '해와 달이 된 오누이'> , text length 392\n",
            "Collecting page : <WikipediaPage '잠자는 숲속의 미녀'> , text length 2037\n",
            "Collecting page : <WikipediaPage '거북'> , text length 1381\n",
            "Collecting page : <WikipediaPage '이한갈'> , text length 776\n",
            "Collecting page : <WikipediaPage '정은찬'> , text length 1096\n",
            "Collecting page : <WikipediaPage '김기두 (배우)'> , text length 1767\n",
            "Collecting page : <WikipediaPage '옛날 옛적에 (애니메이션)'> , text length 1080\n",
            "Collecting page : <WikipediaPage '계룡선녀전 (드라마)'> , text length 1416\n",
            "Collecting page : <WikipediaPage '동요'> , text length 1429\n",
            "Collecting page : <WikipediaPage '이상훈 (1976년)'> , text length 831\n",
            "Collecting page : <WikipediaPage '최홍일'> , text length 1917\n",
            "Collecting page : <WikipediaPage '육진수 (격투기 선수)'> , text length 749\n",
            "Collecting page : <WikipediaPage '제비'> , text length 1125\n",
            "Collecting page : <WikipediaPage '장석현 (연예인)'> , text length 576\n",
            "Collecting page : <WikipediaPage '유다미'> , text length 409\n",
            "Collecting page : <WikipediaPage '밀교 (불교)'> , text length 4379\n",
            "Collecting page : <WikipediaPage '최지웅'> , text length 712\n",
            "Collecting page : <WikipediaPage '박재훈 (배우)'> , text length 1962\n",
            "Collecting page : <WikipediaPage '한다은'> , text length 634\n",
            "Collecting page : <WikipediaPage '재희'> , text length 1229\n",
            "Collecting page : <WikipediaPage '정미남'> , text length 970\n",
            "Collecting page : <WikipediaPage '안젤리나 다닐로바'> , text length 879\n",
            "Collecting page : <WikipediaPage '도깨비'> , text length 3171\n",
            "Collecting page : <WikipediaPage '기탄교육'> , text length 245\n",
            "Collecting page : <WikipediaPage '국지용'> , text length 420\n",
            "Collecting page : <WikipediaPage '의왕백운호수축제'> , text length 272\n",
            "Collecting page : <WikipediaPage '박민경'> , text length 885\n",
            "Collecting page : <WikipediaPage '정정아'> , text length 1119\n",
            "Collecting page : <WikipediaPage '콩딱쿵! 이야기 주머니'> , text length 122\n",
            "Collecting page : <WikipediaPage '윤기원 (배우)'> , text length 2555\n",
            "Collecting page : <WikipediaPage '선녀와 나무꾼'> , text length 173\n",
            "Collecting page : <WikipediaPage '남생이'> , text length 502\n",
            "Collecting page : <WikipediaPage '대장화홍련전'> , text length 216\n",
            "\"콩쥐팥쥐 (동음이의)\" may refer to: \n",
            "콩쥐팥쥐\n",
            "콩쥐팥쥐 (1967년 영화)\n",
            "콩쥐팥쥐 (1958년 영화)\n",
            "Collecting page : <WikipediaPage '토끼전'> , text length 3931\n",
            "Collecting page : <WikipediaPage '미녀와 야수'> , text length 2977\n",
            "Collecting page : <WikipediaPage '지대한'> , text length 3296\n",
            "Collecting page : <WikipediaPage '정안알밤휴게소'> , text length 808\n",
            "Collecting page : <WikipediaPage '은비까비의 옛날옛적에'> , text length 940\n",
            "Collecting page : <WikipediaPage '빨간 자전거'> , text length 2955\n",
            "Collecting page : <WikipediaPage '콩쥐팥쥐전'> , text length 2912\n",
            "Collecting page : <WikipediaPage '이설구'> , text length 3021\n",
            "Collecting page : <WikipediaPage '홍석연'> , text length 3008\n",
            "Collecting page : <WikipediaPage '아시리아인'> , text length 2793\n",
            "Collecting page : <WikipediaPage '백조의 호수'> , text length 2130\n",
            "Collecting page : <WikipediaPage '김덕현 (배우)'> , text length 1986\n",
            "Collecting page : <WikipediaPage '티베트'> , text length 5093\n",
            "Collecting page : <WikipediaPage '서예'> , text length 10102\n",
            "Collecting page : <WikipediaPage '도교'> , text length 6686\n",
            "Collecting page : <WikipediaPage '켈트 다신교'> , text length 4308\n",
            "Collecting page : <WikipediaPage '금도끼 은도끼'> , text length 1259\n",
            "\"장화, 홍련 (동음이의)\" may refer to: \n",
            "장화홍련전\n",
            "장화, 홍련\n",
            "장화, 홍련\n",
            "Collecting page : <WikipediaPage '허구 국가'> , text length 2702\n",
            "Collecting page : <WikipediaPage '노상현'> , text length 351\n",
            "Collecting page : <WikipediaPage '제네시스 (밴드)'> , text length 5457\n",
            "Collecting page : <WikipediaPage '타이완의 문화'> , text length 1581\n",
            "\"도깨비 (동음이의)\" may refer to: \n",
            "도깨비\n",
            "도깨비\n",
            "눈물을 마시는 새\n",
            "레인보우 식스 시즈\n",
            "도깨비가 간다\n",
            "도깨비가 간다\n",
            "Tokebi\n",
            "도깨비 (DokeV)\n",
            "Collecting page : <WikipediaPage '골디락스'> , text length 1243\n",
            "Collecting page : <WikipediaPage '선녀강림'> , text length 1856\n",
            "Collecting page : <WikipediaPage '손춘익'> , text length 1687\n",
            "Collecting page : <WikipediaPage '자와어'> , text length 3770\n",
            "Collecting page : <WikipediaPage '안동국제탈춤페스티벌'> , text length 2123\n",
            "Collecting page : <WikipediaPage '윌리엄 버틀러 예이츠'> , text length 12057\n",
            "Collecting page : <WikipediaPage '성덕대왕신종'> , text length 1934\n",
            "Collecting page : <WikipediaPage '모모타로'> , text length 1980\n",
            "Collecting page : <WikipediaPage '고려'> , text length 30322\n",
            "Collecting page : <WikipediaPage '한국 문학'> , text length 14908\n",
            "Collecting page : <WikipediaPage '라푼젤 (영화)'> , text length 9736\n",
            "Collecting page : <WikipediaPage '토비트'> , text length 9004\n",
            "Collecting page : <WikipediaPage '조선 후기의 문학'> , text length 5280\n",
            "Collecting page : <WikipediaPage '송월동 동화마을'> , text length 896\n",
            "Collecting page : <WikipediaPage '홍석천'> , text length 8717\n",
            "Collecting page : <WikipediaPage '계룡선녀전'> , text length 2268\n",
            "Collecting page : <WikipediaPage '이집트'> , text length 11797\n",
            "Collecting page : <WikipediaPage '응우옌 왕조'> , text length 14070\n",
            "Collecting page : <WikipediaPage '안지환'> , text length 12002\n",
            "Collecting page : <WikipediaPage '아시아의 역사'> , text length 11513\n",
            "Collecting page : <WikipediaPage '외래어'> , text length 5000\n",
            "Collecting page : <WikipediaPage '메이플 월드'> , text length 5895\n",
            "Collecting page : <WikipediaPage '프랑스인'> , text length 12556\n",
            "Collecting page : <WikipediaPage '스키타이족'> , text length 30924\n",
            "Collecting page : <WikipediaPage '인도네시아'> , text length 30606\n",
            "Collecting page : <WikipediaPage '이탈리아'> , text length 42736\n",
            "Collecting page : <WikipediaPage 'MBC 창작동요제'> , text length 1724\n",
            "Collecting page : <WikipediaPage '네버랜드'> , text length 8303\n",
            "Collecting page : <WikipediaPage '일본 제국'> , text length 15259\n",
            "Collecting page : <WikipediaPage '프랑크인'> , text length 23128\n",
            "Collecting page : <WikipediaPage '바이킹'> , text length 22255\n",
            "Collecting page : <WikipediaPage '김환영 (작가)'> , text length 4105\n",
            "Collecting page : <WikipediaPage '드랑 나흐 오스텐'> , text length 4331\n",
            "Collecting page : <WikipediaPage '조선'> , text length 30998\n",
            "Collecting page : <WikipediaPage '오윤 (화가)'> , text length 2772\n",
            "Collecting page : <WikipediaPage '원나라'> , text length 15537\n",
            "Collecting page : <WikipediaPage '앱북'> , text length 4534\n",
            "전체 수집한 Page Count : 97\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "wfcHLkfGeHx8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88e3b334-e462-4002-9cc3-2cd75ace5686"
      },
      "source": [
        "\n",
        "if DO_ALL:\n",
        "    ko_grammar_set_raw += __search_from_wiki(\"역사\", 100)\n",
        "\n",
        "print(f'전체 수집한 Page Count : {len(ko_grammar_set_raw)}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['역사', '한국의 역사', '불교의 역사', '대체 역사', '일본의 역사', '스위스 역사 사전', '유럽의 역사', '고려의 역사', '세계의 역사', '조선의 역사', '우즈베키스탄의 역사', '동북아역사재단', '중국의 역사', '대한민국의 역사 드라마 목록', '한국 불교의 역사', '서울역사박물관', '오키나와현의 역사', '역사주의', '역사학', '러시아의 역사', '역사적 예수', '다큐멘터리 역사를 찾아서', '소련의 역사', '조선민주주의인민공화국의 역사', '조지아의 역사', '영국의 역사', '역사철학', '프랑스의 역사', '역사신학', '애플의 역사', '인터넷의 역사', '역사저널 그날', '유엔의 역사', '역사지진', '힌두교의 역사', '역사 시대', '독일의 역사', '역사소설', '압하지야의 역사', '아일랜드의 역사', '구글의 역사', '영화의 역사', '기독교의 역사', '운영 체제의 역사', '몬테네그로의 역사', '태국의 역사', '사기 (역사서)', '파키스탄의 역사', '음소문자의 역사', '그리스의 역사', '의사 역사학', '사학자', '베트남의 역사', '기술의 역사', '역사스페셜', '안드로이드 버전 역사', '서울역', '인도의 역사', '미국 육군 역사관', '벨라루스의 역사', '역사언어학', '퀴디치의 역사', '사극', '역사적 유물론', '마리아론의 역사', '역사 (헤로도토스)', 'FC 서울의 역사', '폭력의 역사', '교육의 역사', '중화민국의 역사', '터키의 역사', '몽골의 역사', '근대 그리스의 역사', '스페인의 역사', '베트남 역사박물관', '핀란드의 역사', '우크라이나의 역사', '맨체스터 유나이티드 FC의 역사', '이란의 역사', '음악의 역사', '에스페란토의 역사', '타이완의 역사', '역사 재현', '민자역사', '크림반도의 역사', '라오스의 역사', '제2차 세계 대전 기간 미국의 군사 역사', '브리튼 제도의 역사', '이집트의 역사', '전주역사박물관', '강화역사박물관', '한국사 연표', '생물의 진화 역사', '아르메니아의 역사', '군사사', '대한민국의 역사', '건축의 역사', '예수의 역사적 실존', '홍콩의 역사', '포르투갈의 역사']\n",
            "Collecting page : <WikipediaPage '역사'> , text length 5355\n",
            "Collecting page : <WikipediaPage '한국의 역사'> , text length 35878\n",
            "Collecting page : <WikipediaPage '불교의 역사'> , text length 10999\n",
            "Collecting page : <WikipediaPage '대체 역사'> , text length 12122\n",
            "Collecting page : <WikipediaPage '일본의 역사'> , text length 18518\n",
            "Collecting page : <WikipediaPage '스위스 역사 사전'> , text length 3401\n",
            "Collecting page : <WikipediaPage '유럽의 역사'> , text length 43372\n",
            "Collecting page : <WikipediaPage '고려의 역사'> , text length 1222\n",
            "Collecting page : <WikipediaPage '세계의 역사'> , text length 12393\n",
            "Collecting page : <WikipediaPage '조선의 역사'> , text length 18208\n",
            "Collecting page : <WikipediaPage '우즈베키스탄의 역사'> , text length 2106\n",
            "Collecting page : <WikipediaPage '동북아역사재단'> , text length 2202\n",
            "Collecting page : <WikipediaPage '중국의 역사'> , text length 7922\n",
            "Collecting page : <WikipediaPage '대한민국의 역사 드라마 목록'> , text length 482\n",
            "Collecting page : <WikipediaPage '한국 불교의 역사'> , text length 46394\n",
            "Collecting page : <WikipediaPage '서울역사박물관'> , text length 2769\n",
            "Collecting page : <WikipediaPage '오키나와현의 역사'> , text length 6799\n",
            "Collecting page : <WikipediaPage '역사주의'> , text length 4674\n",
            "Collecting page : <WikipediaPage '역사학'> , text length 253\n",
            "Collecting page : <WikipediaPage '러시아의 역사'> , text length 10327\n",
            "Collecting page : <WikipediaPage '역사적 예수'> , text length 21307\n",
            "Collecting page : <WikipediaPage '다큐멘터리 역사를 찾아서'> , text length 264\n",
            "Collecting page : <WikipediaPage '소련의 역사'> , text length 199\n",
            "Collecting page : <WikipediaPage '조선민주주의인민공화국의 역사'> , text length 8071\n",
            "Collecting page : <WikipediaPage '조지아의 역사'> , text length 24119\n",
            "Collecting page : <WikipediaPage '영국의 역사'> , text length 6671\n",
            "Collecting page : <WikipediaPage '역사철학'> , text length 52\n",
            "Collecting page : <WikipediaPage '프랑스의 역사'> , text length 14913\n",
            "Collecting page : <WikipediaPage '역사신학'> , text length 926\n",
            "Collecting page : <WikipediaPage '애플의 역사'> , text length 6215\n",
            "Collecting page : <WikipediaPage '인터넷의 역사'> , text length 2171\n",
            "Collecting page : <WikipediaPage '역사저널 그날'> , text length 437\n",
            "Collecting page : <WikipediaPage '유엔의 역사'> , text length 1147\n",
            "Collecting page : <WikipediaPage '역사지진'> , text length 543\n",
            "Collecting page : <WikipediaPage '힌두교의 역사'> , text length 1246\n",
            "Collecting page : <WikipediaPage '역사 시대'> , text length 551\n",
            "Collecting page : <WikipediaPage '독일의 역사'> , text length 23772\n",
            "Collecting page : <WikipediaPage '역사소설'> , text length 1721\n",
            "Collecting page : <WikipediaPage '압하지야의 역사'> , text length 15841\n",
            "Collecting page : <WikipediaPage '아일랜드의 역사'> , text length 14632\n",
            "Collecting page : <WikipediaPage '구글의 역사'> , text length 2054\n",
            "Collecting page : <WikipediaPage '영화의 역사'> , text length 12999\n",
            "Collecting page : <WikipediaPage '기독교의 역사'> , text length 28905\n",
            "Collecting page : <WikipediaPage '운영 체제의 역사'> , text length 2631\n",
            "Collecting page : <WikipediaPage '몬테네그로의 역사'> , text length 560\n",
            "Collecting page : <WikipediaPage '태국의 역사'> , text length 3114\n",
            "Collecting page : <WikipediaPage '사기 (역사서)'> , text length 4005\n",
            "Collecting page : <WikipediaPage '파키스탄의 역사'> , text length 2372\n",
            "Collecting page : <WikipediaPage '음소문자의 역사'> , text length 2966\n",
            "Collecting page : <WikipediaPage '그리스의 역사'> , text length 12854\n",
            "Collecting page : <WikipediaPage '의사 역사학'> , text length 2414\n",
            "Collecting page : <WikipediaPage '사학자'> , text length 276\n",
            "Collecting page : <WikipediaPage '베트남의 역사'> , text length 10429\n",
            "Collecting page : <WikipediaPage '기술의 역사'> , text length 186\n",
            "Collecting page : <WikipediaPage '역사스페셜'> , text length 1448\n",
            "Collecting page : <WikipediaPage '안드로이드 버전 역사'> , text length 2279\n",
            "Collecting page : <WikipediaPage '서울역'> , text length 9036\n",
            "Collecting page : <WikipediaPage '인도의 역사'> , text length 13866\n",
            "Collecting page : <WikipediaPage '미국 육군 역사관'> , text length 397\n",
            "Collecting page : <WikipediaPage '벨라루스의 역사'> , text length 1506\n",
            "Collecting page : <WikipediaPage '역사언어학'> , text length 556\n",
            "Collecting page : <WikipediaPage '퀴디치의 역사'> , text length 92\n",
            "Collecting page : <WikipediaPage '사극'> , text length 539\n",
            "Collecting page : <WikipediaPage '역사적 유물론'> , text length 2296\n",
            "Collecting page : <WikipediaPage '마리아론의 역사'> , text length 487\n",
            "Collecting page : <WikipediaPage '역사 (헤로도토스)'> , text length 1162\n",
            "Collecting page : <WikipediaPage 'FC 서울의 역사'> , text length 11289\n",
            "Collecting page : <WikipediaPage '폭력의 역사'> , text length 1036\n",
            "Collecting page : <WikipediaPage '교육의 역사'> , text length 1504\n",
            "Collecting page : <WikipediaPage '중화민국의 역사'> , text length 28162\n",
            "Collecting page : <WikipediaPage '터키의 역사'> , text length 4319\n",
            "Collecting page : <WikipediaPage '몽골의 역사'> , text length 1154\n",
            "Collecting page : <WikipediaPage '근대 그리스의 역사'> , text length 15021\n",
            "Collecting page : <WikipediaPage '스페인의 역사'> , text length 14863\n",
            "Collecting page : <WikipediaPage '베트남 역사박물관'> , text length 179\n",
            "Collecting page : <WikipediaPage '핀란드의 역사'> , text length 6371\n",
            "Collecting page : <WikipediaPage '우크라이나의 역사'> , text length 3333\n",
            "Collecting page : <WikipediaPage '맨체스터 유나이티드 FC의 역사'> , text length 156\n",
            "Collecting page : <WikipediaPage '이란의 역사'> , text length 17702\n",
            "Collecting page : <WikipediaPage '음악의 역사'> , text length 6766\n",
            "Collecting page : <WikipediaPage '에스페란토의 역사'> , text length 12663\n",
            "Collecting page : <WikipediaPage '타이완의 역사'> , text length 6145\n",
            "Collecting page : <WikipediaPage '역사 재현'> , text length 2316\n",
            "Collecting page : <WikipediaPage '민자역사'> , text length 1029\n",
            "Collecting page : <WikipediaPage '크림반도의 역사'> , text length 2563\n",
            "Collecting page : <WikipediaPage '라오스의 역사'> , text length 2200\n",
            "Collecting page : <WikipediaPage '제2차 세계 대전 기간 미국의 군사 역사'> , text length 583\n",
            "Collecting page : <WikipediaPage '브리튼 제도의 역사'> , text length 1032\n",
            "Collecting page : <WikipediaPage '이집트의 역사'> , text length 1004\n",
            "Collecting page : <WikipediaPage '전주역사박물관'> , text length 413\n",
            "Collecting page : <WikipediaPage '강화역사박물관'> , text length 248\n",
            "Collecting page : <WikipediaPage '한국사 연표'> , text length 188\n",
            "Collecting page : <WikipediaPage '생물의 진화 역사'> , text length 895\n",
            "Collecting page : <WikipediaPage '아르메니아의 역사'> , text length 1073\n",
            "Collecting page : <WikipediaPage '군사사'> , text length 1239\n",
            "Collecting page : <WikipediaPage '대한민국의 역사'> , text length 35866\n",
            "Collecting page : <WikipediaPage '건축의 역사'> , text length 25890\n",
            "Collecting page : <WikipediaPage '예수의 역사적 실존'> , text length 574\n",
            "Collecting page : <WikipediaPage '홍콩의 역사'> , text length 2662\n",
            "Collecting page : <WikipediaPage '포르투갈의 역사'> , text length 1620\n",
            "전체 수집한 Page Count : 197\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Tu_6jZAmeHx9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee4ccd35-eefe-4b97-a820-a49fcc01078b"
      },
      "source": [
        "# 간단한 전처리\n",
        "def clean_text(txt):\n",
        "    txt = txt.replace('\\n','')\n",
        "    txt = txt.replace('=','')    \n",
        "    return txt \n",
        "\n",
        "ko_grammar_set_raw = [clean_text(x) for x in ko_grammar_set_raw]\n",
        "print('Sample text : ')\n",
        "print('--------------------------------------------------------------------------------------------')\n",
        "print(ko_grammar_set_raw[50])\n",
        "print('--------------------------------------------------------------------------------------------')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample text : \n",
            "--------------------------------------------------------------------------------------------\n",
            "김덕현(본명은 김덕구, 1967년 4월 19일 ~ )은 대한민국의 배우이다. 충청남도 보령에서 태어나 청소년기를 보냈다. 서울예술전문대학 방송연예학과를 졸업했다. 1991년 KBS 14기 공채 탤런트로 데뷔했다. 학력 서울예술전문대학 방송연예학과 졸업 출연작  드라마 KBS2 단막드라마 《TV 손자병법》(1992년)KBS2 단막드라마 《신 손자병법》(1993년)KBS2 수목드라마 《전설의 고향》(1996년 6월 26일 ~ 1996년 9월 12일) ... 구미호/망자의 소원 역KBS2 월화 미니시리즈 《거짓말》(1998년 3월 20일 ~ 1998년 6월 2일)KBS2 드라마 《부부클리닉 사랑과 전쟁》(1999년 10월 22일 ~ 2009년 4월 17일)KBS2 월화 미니시리즈 《학교》(1999년 2월 22일 ~ 1999년 4월 13일) ... 선생님 역MBC 단막극 《MBC 베스트극장》(1999년)iTV 일일드라마 《해바라기 가족》(2001년)MBC 법정드라마 《실화극장 죄와 벌》(2003년)KBS2 주말연속극 《애정의 조건》 (2004년 3월 20일 ~ 2004년 10월 10일)SBS 일일드라마 《소풍가는 여자》 (2004년 5월 3일 ~ 2004년 10월 8일)KBS1 일일연속극 《금쪽같은 내 새끼》(2004년 6월 7일 ~ 2005년 2월 11일)KBS1 대하드라마 《불멸의 이순신》(2004년 9월 4일 ~ 2005년 8월 28일) ... 언복 역KBS1 TV소설 《고향역》(2005년 8월 29일 ~ 2006년 3월 18일) ... 최 계장 역KBS1 대하드라마 《서울 1945》(2006년 1월 7일 ~ 2006년 9월 10일) - 박헌영 비서 역KBS2 아침드라마 《아줌마가 간다》(2006년 11월 13일 ~ 2007년 5월 19일) - 김성태 역KBS2 아침드라마 《사랑해도 괜찮아》(2007년 5월 21일 ~ 2007년 9월 29일)KBS2 월화 미니시리즈 《얼렁뚱땅 흥신소》(2007년 10월 8일 ~ 2007년 11월 27일) ... 경찰 역KBS1 대하드라마 《명가》 (2010년 1월 2일 ~ 2010년 2월 21일) ... 이 서방 역KBS2 아침드라마 《엄마도 예쁘다》 (2010년 4월 5일 ~ 2010년 10월 23일) ... 김 비서 역KBS1 대하드라마 《광개토태왕》(2011년 6월 4일 ~ 2012년 4월 29일) ... 거보 역KBS1 일일드라마 《힘내요 미스터 김》 (2012년 11월 5일 ~ 2013년 4월 26일) ... 김 부장 역MBN 다큐드라마 《대한민국 정치비사》(2013년 5월 12일 ~ 6월 2일) ... 노태우 역MBC 드라마넷 금토드라마 《태양의 도시》(2015년 1월 30일 ~ 2015년 4월 7일)KBS1 대하드라마 《징비록》(2015년 2월 14일 ~ 2015년 8월 2일) ... 명나라 대신 역tvN 일일드라마 《울지 않는 새》(2015년 5월 4일 ~ 2015년 10월 22일)KBS2 수목드라마 《장사의 신 - 객주 2015》(2015년 9월 23일 ~ 2016년 2월 18일)KBS2 일일드라마 《여자의 비밀》(2016년 10월 27일) ... 박 변호사 역KNN 촌티콤 《웰컴 투 가오리 시즌2》 (2017년 4월 1일 ~ 현재) ... 이상수 역MBN 수목드라마 《마녀의 사랑》(2018년 8월 1일)KBS2 단막극 《KBS 드라마 스페셜 - 그곳에 두고 온 라일락》 (2020년 11월 28일) 영화 1998년 《약속》... 야쿠자 역2002년 《피아노 치는 대통령》... 교사 역2006년 《로망스》... 하 형사 역2006년 《다세포 소녀》... 조교 역2008년 《라듸오 데이즈》... 오디션 사내 3 역2018년 《신 전래동화》... 김선달 역2019년 《유정: 스며들다》... 송 부장 역2019년 《얼굴없는 보스》... 클럽 사장 1 역 예능 KBS1 《가족오락관》(2009년 1월 10일)\n",
            "--------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqJHLPaReHx-"
      },
      "source": [
        "문장으로 잘라 낸다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "_b8WKqYXeHx_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "124197a7-65d4-435f-bb24-f0e80be0a50a"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "erJc7g-VeHyA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8d3072f-70d0-41cf-8a8d-78e4c109f859"
      },
      "source": [
        "#Split the document into sentences\n",
        "ko_grammar_sentences = []\n",
        "for document in ko_grammar_set_raw:\n",
        "    ko_grammar_sentences += nltk.sent_tokenize(document)\n",
        "\n",
        "print(\"Num sentences:\", len(ko_grammar_sentences))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num sentences: 13551\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "7M2yM5kFeHyC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "76ee3f7d-994b-4193-b4a2-3d006645d35d"
      },
      "source": [
        "ko_grammar_sentences[300]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"현재 기탄교육은 학습지로는 '기탄국어, 한글떼기, 기탄한글, 기탄수학, 기탄사고력수학, 영단어 암기 끝!, 기탄 한자 빨리따기, 기탄중국어' 등이 있다.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Q5K4D3QM3DkG"
      },
      "source": [
        "f = open(\"ko_sentence_set.txt\", \"a\")\n",
        "for t in ko_grammar_sentences:\n",
        "    f.write(t+'\\n')\n",
        "f.close()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdj8wrm8eHyC"
      },
      "source": [
        "형태소 분리하여 모든 문장을 형태소 Code로 변환 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "uETdqraheHyE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29c0f783-9367-4416-e154-8133987464d0"
      },
      "source": [
        "from konlpy.tag import Twitter\n",
        "twitter = Twitter()\n",
        "\n",
        "print(twitter.pos(ko_grammar_sentences[305]))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('백운호수', 'Noun'), ('에서', 'Josa'), ('의왕시', 'Noun'), ('와', 'Josa'), ('의왕시', 'Noun'), ('축제', 'Noun'), ('추진', 'Noun'), ('위원회', 'Noun'), ('가', 'Josa'), ('공동', 'Noun'), ('으로', 'Josa'), ('주최', 'Noun'), ('하여', 'Verb'), ('매년', 'Noun'), ('9월', 'Number'), ('에', 'Foreign'), ('이틀', 'Noun'), ('동안', 'Noun'), ('열리는', 'Verb'), ('축제', 'Noun'), ('이다', 'Josa'), ('.', 'Punctuation')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "OY8VTXraeHyF"
      },
      "source": [
        "# 형태소 Code table의 구성\n",
        "\n",
        "_MAX_MORP_LENGTH = 128\n",
        "_PADDING_CODE = 0  # padding code\n",
        "_MISMATCH_CODE = 1 # mismatch word code               ex) @@@\n",
        "_MISMATCH_WORD = '@@@' # 이거 아래에서 쓴다.\n",
        "\n",
        "morpheme_table = {}\n",
        "morp_code = _MISMATCH_CODE+1\n",
        "morpheme_table['Pad'] = _PADDING_CODE \n",
        "morpheme_table['Mst'] = _MISMATCH_CODE \n",
        "for sentence in ko_grammar_sentences[:1000]:\n",
        "    morphemes = twitter.pos(sentence)\n",
        "    for (word,morp) in morphemes:\n",
        "        if morp in morpheme_table:\n",
        "            pass\n",
        "        else:\n",
        "            morpheme_table[morp] = morp_code\n",
        "            morp_code += 1"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "asiTeu8SeHyG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd822172-23f6-4df1-9dcb-cc2cee1750be"
      },
      "source": [
        "print('Korean morpheme code table')\n",
        "print('----------------------------------------------------------')\n",
        "print('  Morpheme        Code')\n",
        "print('')\n",
        "for morp in morpheme_table.keys():\n",
        "    print(f' {morp.ljust(15)}   {morpheme_table[morp]}')\n",
        "print('----------------------------------------------------------')\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Korean morpheme code table\n",
            "----------------------------------------------------------\n",
            "  Morpheme        Code\n",
            "\n",
            " Pad               0\n",
            " Mst               1\n",
            " Noun              2\n",
            " Punctuation       3\n",
            " Foreign           4\n",
            " Josa              5\n",
            " Verb              6\n",
            " Modifier          7\n",
            " Adjective         8\n",
            " Suffix            9\n",
            " Adverb            10\n",
            " Number            11\n",
            " Alpha             12\n",
            " Conjunction       13\n",
            " Determiner        14\n",
            " VerbPrefix        15\n",
            " Exclamation       16\n",
            " KoreanParticle    17\n",
            " Eomi              18\n",
            " ScreenName        19\n",
            " URL               20\n",
            "----------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "RQfjoYbceHyH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8439f60c-e079-445d-e2be-627088f602a7"
      },
      "source": [
        "# morpheme 코드 변환기\n",
        "def morpheme_encode(sentence):\n",
        "    encode=[]\n",
        "    morphemes = twitter.pos(sentence)\n",
        "    for (word,morp) in morphemes:\n",
        "        encode.append(_MISMATCH_CODE if word==_MISMATCH_WORD else morpheme_table[morp])\n",
        "    return encode\n",
        "\n",
        "code = morpheme_encode('야당이 수단으로 야당에게는 이라며 윤석열 허물어뜨렸고 수사를 군사작전을 하는 시비를 민주당 의원이 국민적 세워 화신이 나라 법위에 우려했다 될 이라며 운영할지 바람 짓밟지만 양자역학 양자역학 걱정하느냐고 누가 청와대 한다 엄포를 양자역학 양자역학 양자역학 편을 파렴치와 난관에 풀이다 대통령이 양자역학 저지하려다가')\n",
        "print(f'Code length : {len(code)}')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Code length : 67\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "BS_SnM04eHyH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b3b8516-b337-4aee-bce4-ba077bc11a43"
      },
      "source": [
        "# 전체 형태소 코드로 변환\n",
        "if DO_ALL:\n",
        "    ko_grammar_set = []\n",
        "    for sentence in ko_grammar_sentences:\n",
        "        code = morpheme_encode(sentence)\n",
        "        if len(code) <= _MAX_MORP_LENGTH:\n",
        "            ko_grammar_set.append(code + [_PADDING_CODE for i in range(_MAX_MORP_LENGTH-len(code))])\n",
        "\n",
        "    ko_grammar_set = np.asarray(ko_grammar_set)\n",
        "ko_grammar_set.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13352, 128)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "d4dpyWPU3DkJ"
      },
      "source": [
        "from numpy import savetxt\n",
        "# save to csv file\n",
        "savetxt('ko_morpheme_set.csv', ko_grammar_set, delimiter=',')\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNq3STdueHyI"
      },
      "source": [
        "# Dataset 전체 구성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "PiSdmi1beHyI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79a8547c-b57c-4b64-bbe1-f16a79094ca7"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "if DO_ALL:\n",
        "    # embedder download...\n",
        "    embedder = SentenceTransformer('xlm-r-large-en-ko-nli-ststb')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1.80G/1.80G [01:05<00:00, 27.6MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "e1FGpEuJeHyJ"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BATCH_COUNT = 50"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JItg20Ukw5il"
      },
      "source": [
        "# 원문의 간단한 전처리\r\n",
        "def clean_text(txt):\r\n",
        "    txt = txt.replace('\\n','')\r\n",
        "    txt = txt.replace('=','')    \r\n",
        "    txt = txt.replace('\\\"','')    \r\n",
        "    txt = txt.replace('\\'','')    \r\n",
        "    return txt \r\n",
        "\r\n",
        "org_text = clean_text(org_text)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owsvfOpGxYTi",
        "outputId": "a7d86d51-76ba-49b3-9398-adc6c235a336",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "org_text"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'옛날 어느 집에 귀여운 여자 아기가 태어났어요.아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요.그러던 어느날, 소녀의 어머니가 병이들어그만 세상을 떠나고 말았어요. 소녀의 아버지는 홀로 남은 소녀가 걱정되었어요.그래서 얼마 후 새어머니를 맞이했어요.새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요.그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요.새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요. 그런데 이번에는 아버지마저 돌아가셨어요.소녀는 하녀처럼 하루 종일 쓸고, 닦고, 집안일을 도맡아 했어요.해도 해도 끝이 없는 집안일이 힘들어 지칠때면난롯가에 앉아서 잠시 쉬곤 했지요. 엄마, 저애를 신데렐라라고 불러야겠어요.온통 재투성이잖아요. 호호호! 두 언니는 소녀를 놀려 댔어요. 어느 날, 왕궁에서 무도회가 열렸어요.신데렐라의 집에도 초대장이 왔어요. 새어머니는 언니들을 데리고 무도회장으로 떠났어요.신데렐라도 무도회에 가고 싶었어요.혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요.신데렐라, 너도 무도회에 가고 싶니?신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요. 내가 너를 무도회에 보내주마호박 한개와 생쥐 두마리, 도마뱀을 구해 오렴.  마법사 할머니가 주문을 외웠어요.그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금 마차로 변했어요.이번에는 생쥐와 도마뱀을 건드렸어요.그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했답니다. 신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요.신데렐라, 발을 내밀어 보거라.할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요신데렐라, 밤 열두시가 되면 모든게 처음대로 돌아간단다.황금 마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼.그러니까 바늗시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지? 왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요.완자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고,신데렐라하고만 춤을 추었어요.신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요.땡, 땡, 땡...... 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요. 신데렐라가 허둥지둥 왕궁을 빠져나가는데,유리 구두 한 짝이 벗겨졌어요.하지만 구두를 주울 틈이 없었어요.신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요.왕자님은 유리 구두를 가지고 임금님께 가서 말했어요.이 유리 구두의 주인과 결혼하겠어요.그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요. 언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만한눈에 보기에도 유리 구두는 너무 작았어요. 그때, 신데렐라가 조용히 다가와 말했어요.저도 한번 신어 볼 수 있나요?\\u200b신데렐라는 신하게 건넨 유리 구두를 신었어요,유리 구두는 신데렐라의 발에 꼭 맞았어요. 신하들은 신데렐라를 왕궁으로 데리고 갔어요. 그 뒤 신데렐라는 완자님과 결혼하여 오래오래 행복하게 살았대요.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "bhOs5GKNeHyK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a91f1671-ed60-424f-b65e-be36f05e3c63"
      },
      "source": [
        "#dataset 다시 만듦\n",
        "\n",
        "org_text_emb = embedder.encode([org_text])[0]\n",
        "print(f'Text embedding shape : {org_text_emb.shape}')\n",
        "dataset = []\n",
        "for i in range(BATCH_COUNT):\n",
        "    emb_batch_set = []\n",
        "    cod_batch_set = []\n",
        "    for j in range(BATCH_SIZE):\n",
        "        emb_batch_set.append(org_text_emb)\n",
        "        cod_batch_set.append(ko_grammar_set[BATCH_SIZE*i+j])\n",
        "\n",
        "    emb_batch_set = np.asarray(emb_batch_set)\n",
        "    cod_batch_set = np.asarray(cod_batch_set)\n",
        "    dataset.append((emb_batch_set,cod_batch_set))\n",
        "\n",
        "print(f'Total dataset count :{BATCH_COUNT*BATCH_SIZE}')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text embedding shape : (1024,)\n",
            "Total dataset count :3200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "VxfQxFQKeHyL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8af895aa-9a7e-4891-9967-9352c95b5a10"
      },
      "source": [
        "# 30번째 배치의 형태소코드셋 중 20번째꺼 확인\n",
        "print(dataset[30][1][20])\n",
        "print(dataset[31][1][20])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2 9 2 5 2 2 5 6 2 2 5 6 2 2 5 2 5 6 3 2 5 2 5 2 6 2 5 6 2 5 6 6 2 8 3 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[ 2  2  5  2  5  2  5  2  2  2  5  2  8  3  2  5  2  6  6  2  5  2  5  3\n",
            " 11  3 11  3 11  3 11  3 11  3 11  3 11  3  2  2  5  2  5  6  2  6  2  5\n",
            "  6  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlM5tdo1eHyL"
      },
      "source": [
        "# Generator 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "jopXmV8DeHyM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b45bba25-1b97-4f59-be8c-69a793ab4b1e"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([org_text])\n",
        "total_words = len(tokenizer.word_index)\n",
        "\n",
        "print(f'Total token count of origin text : {total_words}')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total token count of origin text : 276\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DT4gKRYeHyN"
      },
      "source": [
        "원문을 40개의 token 으로 구성된 문장으로 요약 하는것....<br>\n",
        "향후 LSTM으로 G 구성 필요"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "P_cpFugJeHyN"
      },
      "source": [
        "\n",
        "_MAX_GEN_TOKEN = 40\n",
        "_NOISE_DIM = total_words\n",
        "\n",
        "word_table = {}\n",
        "\n",
        "for word,index in tokenizer.word_index.items():\n",
        "    word_table[index-1] = word\n",
        "\n",
        "current_token_len = len(word_table)\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Jg3YCjGJeHyO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6d1151c-9027-4148-f490-7097f0021fe9"
      },
      "source": [
        "print('Token table of origin text')\n",
        "print('---------------------------------------------')\n",
        "print(' Code         Token      ')\n",
        "print('')\n",
        "for k in word_table.keys():\n",
        "  print( f'  {str(k).ljust(8)}    {word_table[k]}')\n",
        "print('---------------------------------------------')\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token table of origin text\n",
            "---------------------------------------------\n",
            " Code         Token      \n",
            "\n",
            "  0           유리\n",
            "  1           구두를\n",
            "  2           신데렐라는\n",
            "  3           소녀가\n",
            "  4           새어머니는\n",
            "  5           데리고\n",
            "  6           신데렐라의\n",
            "  7           무도회에\n",
            "  8           신데렐라가\n",
            "  9           땡\n",
            "  10          어느\n",
            "  11          예쁘고\n",
            "  12          소녀의\n",
            "  13          남은\n",
            "  14          그래서\n",
            "  15          두\n",
            "  16          왔어요\n",
            "  17          언니들은\n",
            "  18          이번에는\n",
            "  19          해도\n",
            "  20          가고\n",
            "  21          신데렐라\n",
            "  22          마법사\n",
            "  23          할머니가\n",
            "  24          도마뱀을\n",
            "  25          황금\n",
            "  26          발을\n",
            "  27          신데렐라에게\n",
            "  28          밤\n",
            "  29          열두\n",
            "  30          춤을\n",
            "  31          구두\n",
            "  32          한\n",
            "  33          신데렐라를\n",
            "  34          왕자님은\n",
            "  35          말했어요\n",
            "  36          구두의\n",
            "  37          신하들은\n",
            "  38          구두는\n",
            "  39          옛날\n",
            "  40          집에\n",
            "  41          귀여운\n",
            "  42          여자\n",
            "  43          아기가\n",
            "  44          태어났어요\n",
            "  45          아기는\n",
            "  46          무럭무럭\n",
            "  47          자라서\n",
            "  48          마음씨\n",
            "  49          고운\n",
            "  50          되었어요\n",
            "  51          그러던\n",
            "  52          어느날\n",
            "  53          어머니가\n",
            "  54          병이들어그만\n",
            "  55          세상을\n",
            "  56          떠나고\n",
            "  57          말았어요\n",
            "  58          아버지는\n",
            "  59          홀로\n",
            "  60          걱정되었어요\n",
            "  61          얼마\n",
            "  62          후\n",
            "  63          새어머니를\n",
            "  64          맞이했어요\n",
            "  65          소녀보다\n",
            "  66          나이가\n",
            "  67          위인\n",
            "  68          딸을\n",
            "  69          그러나\n",
            "  70          새어머니와\n",
            "  71          성질이\n",
            "  72          고약한\n",
            "  73          심술쟁이들이었어요\n",
            "  74          자기\n",
            "  75          딸들보다\n",
            "  76          착한\n",
            "  77          게\n",
            "  78          못마땅했어요\n",
            "  79          그런데\n",
            "  80          아버지마저\n",
            "  81          돌아가셨어요\n",
            "  82          소녀는\n",
            "  83          하녀처럼\n",
            "  84          하루\n",
            "  85          종일\n",
            "  86          쓸고\n",
            "  87          닦고\n",
            "  88          집안일을\n",
            "  89          도맡아\n",
            "  90          했어요\n",
            "  91          끝이\n",
            "  92          없는\n",
            "  93          집안일이\n",
            "  94          힘들어\n",
            "  95          지칠때면난롯가에\n",
            "  96          앉아서\n",
            "  97          잠시\n",
            "  98          쉬곤\n",
            "  99          했지요\n",
            "  100         엄마\n",
            "  101         저애를\n",
            "  102         신데렐라라고\n",
            "  103         불러야겠어요\n",
            "  104         온통\n",
            "  105         재투성이잖아요\n",
            "  106         호호호\n",
            "  107         언니는\n",
            "  108         소녀를\n",
            "  109         놀려\n",
            "  110         댔어요\n",
            "  111         날\n",
            "  112         왕궁에서\n",
            "  113         무도회가\n",
            "  114         열렸어요\n",
            "  115         집에도\n",
            "  116         초대장이\n",
            "  117         언니들을\n",
            "  118         무도회장으로\n",
            "  119         떠났어요\n",
            "  120         신데렐라도\n",
            "  121         싶었어요\n",
            "  122         혼자\n",
            "  123         훌쩍훌쩍\n",
            "  124         울기\n",
            "  125         시작했어요\n",
            "  126         너도\n",
            "  127         싶니\n",
            "  128         고개를\n",
            "  129         들어보니\n",
            "  130         빙그레\n",
            "  131         웃고\n",
            "  132         있었어요\n",
            "  133         내가\n",
            "  134         너를\n",
            "  135         보내주마호박\n",
            "  136         한개와\n",
            "  137         생쥐\n",
            "  138         두마리\n",
            "  139         구해\n",
            "  140         오렴\n",
            "  141         주문을\n",
            "  142         외웠어요\n",
            "  143         그리고\n",
            "  144         지팡이로\n",
            "  145         호박을\n",
            "  146         건드리자\n",
            "  147         호박이\n",
            "  148         화려한\n",
            "  149         마차로\n",
            "  150         변했어요\n",
            "  151         생쥐와\n",
            "  152         건드렸어요\n",
            "  153         그랬더니\n",
            "  154         생쥐는\n",
            "  155         흰말로\n",
            "  156         도마뱀은\n",
            "  157         멋진\n",
            "  158         마부로\n",
            "  159         변했답니다\n",
            "  160         옷도\n",
            "  161         구슬\n",
            "  162         장식이\n",
            "  163         반짝이는\n",
            "  164         예쁜\n",
            "  165         드레스로\n",
            "  166         바뀌웠어요\n",
            "  167         내밀어\n",
            "  168         보거라\n",
            "  169         할머니는\n",
            "  170         반짝반짝\n",
            "  171         빛나는\n",
            "  172         신겨\n",
            "  173         주었어요신데렐라\n",
            "  174         열두시가\n",
            "  175         되면\n",
            "  176         모든게\n",
            "  177         처음대로\n",
            "  178         돌아간단다\n",
            "  179         마차는\n",
            "  180         호박으로\n",
            "  181         흰말은\n",
            "  182         생쥐로\n",
            "  183         마부는\n",
            "  184         도마뱀으로\n",
            "  185         변하게\n",
            "  186         돼\n",
            "  187         그러니까\n",
            "  188         바늗시\n",
            "  189         시가\n",
            "  190         되기\n",
            "  191         전에\n",
            "  192         돌아와야\n",
            "  193         해\n",
            "  194         알겠지\n",
            "  195         왕자님도\n",
            "  196         아름다운\n",
            "  197         마음을\n",
            "  198         빼았겼어요\n",
            "  199         완자님은\n",
            "  200         무도회장에\n",
            "  201         모인\n",
            "  202         다른\n",
            "  203         아가씨들은\n",
            "  204         쳐다보지도\n",
            "  205         않고\n",
            "  206         신데렐라하고만\n",
            "  207         추었어요\n",
            "  208         왕자님과\n",
            "  209         추느라\n",
            "  210         시간\n",
            "  211         가는\n",
            "  212         줄도\n",
            "  213         몰랐어요\n",
            "  214         벽시계가\n",
            "  215         시를\n",
            "  216         알리는\n",
            "  217         소리에\n",
            "  218         화들짝\n",
            "  219         놀랐어요\n",
            "  220         허둥지둥\n",
            "  221         왕궁을\n",
            "  222         빠져나가는데\n",
            "  223         짝이\n",
            "  224         벗겨졌어요\n",
            "  225         하지만\n",
            "  226         주울\n",
            "  227         틈이\n",
            "  228         없었어요\n",
            "  229         뛰쫓아오던\n",
            "  230         층계에서\n",
            "  231         짝을\n",
            "  232         주웠어요\n",
            "  233         가지고\n",
            "  234         임금님께\n",
            "  235         가서\n",
            "  236         이\n",
            "  237         주인과\n",
            "  238         결혼하겠어요\n",
            "  239         주인을\n",
            "  240         찾아\n",
            "  241         온\n",
            "  242         나라를\n",
            "  243         돌아다녔어요\n",
            "  244         오므려도\n",
            "  245         보고\n",
            "  246         늘려도\n",
            "  247         보았지만한눈에\n",
            "  248         보기에도\n",
            "  249         너무\n",
            "  250         작았어요\n",
            "  251         그때\n",
            "  252         조용히\n",
            "  253         다가와\n",
            "  254         저도\n",
            "  255         한번\n",
            "  256         신어\n",
            "  257         볼\n",
            "  258         수\n",
            "  259         있나요\n",
            "  260         ​신데렐라는\n",
            "  261         신하게\n",
            "  262         건넨\n",
            "  263         신었어요\n",
            "  264         발에\n",
            "  265         꼭\n",
            "  266         맞았어요\n",
            "  267         왕궁으로\n",
            "  268         갔어요\n",
            "  269         그\n",
            "  270         뒤\n",
            "  271         완자님과\n",
            "  272         결혼하여\n",
            "  273         오래오래\n",
            "  274         행복하게\n",
            "  275         살았대요\n",
            "---------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "bZgNu7SYeHyP"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (Input,\n",
        "                                     Dense, \n",
        "                                     BatchNormalization, \n",
        "                                     LeakyReLU,\n",
        "                                     Softmax,\n",
        "                                     Reshape, \n",
        "                                     Conv2DTranspose,\n",
        "                                     Conv2D,\n",
        "                                     Dropout,\n",
        "                                     Flatten,\n",
        "                                     Concatenate,\n",
        "                                     Lambda)\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6Vj9qaaeHyQ"
      },
      "source": [
        "noise를 token_table을 통해 text로 변환하는 변환기 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cAv4SYIVyfL",
        "outputId": "198eed60-122d-4ef7-976c-0ca002c26819"
      },
      "source": [
        "\r\n",
        "b = tf.constant([1,2,3,4,5,6,11,8,9,10])\r\n",
        "c = tf.sort(b,axis=-1,direction='DESCENDING')\r\n",
        "wh = tf.where(tf.math.greater_equal(b,c[5-1]))[:5]\r\n",
        "wh"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5, 1), dtype=int64, numpy=\n",
              "array([[5],\n",
              "       [6],\n",
              "       [7],\n",
              "       [8],\n",
              "       [9]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "72FRz_bDeHyR"
      },
      "source": [
        "import sys\n",
        "\n",
        "def to_text(w):\n",
        "\n",
        "    #tf.print(w)\n",
        "    texts = []\n",
        "    try:\n",
        "        s_w = tf.sort(w,axis=-1,direction='DESCENDING')\n",
        "        #print(s_w)\n",
        "        for w_0r,s_w_0r in zip(w,s_w):\n",
        "            text = \"\"\n",
        "            wh = tf.where(tf.math.greater_equal(w_0r,s_w_0r[_MAX_GEN_TOKEN]))[:_MAX_GEN_TOKEN]\n",
        "\n",
        "            for k in tf.reshape(wh,(_MAX_GEN_TOKEN,)).numpy():\n",
        "                try:\n",
        "                    text += word_table[k]+' '\n",
        "                except Exception as ex:\n",
        "                    tf.print('to_text : word_table' + str(ex),k)\n",
        "                \n",
        "            texts.append(text)\n",
        "    except Exception as ex:\n",
        "        tf.print('to_text : ' + str(ex),sys.exc_info())\n",
        "\n",
        "    return tf.constant(texts,dtype=tf.string)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ywMSg5noeHyS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14f033c5-b156-4361-e3af-1e7ac354a89d"
      },
      "source": [
        "# to_text 함수의 test\n",
        "w = tf.random.normal([3,_NOISE_DIM])\n",
        "print(w.shape)\n",
        "e = to_text(w)\n",
        "for t in e:\n",
        "    print(t.numpy().decode('utf-8'))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 276)\n",
            "신데렐라는 해도 마법사 구두 아기가 아기는 어머니가 홀로 새어머니와 성질이 자기 하녀처럼 닦고 집안일이 불러야겠어요 무도회장으로 싶었어요 고개를 너를 두마리 호박을 그랬더니 마부로 변했답니다 바뀌웠어요 반짝반짝 돌아간단다 마차는 마부는 변하게 돼 해 알겠지 않고 왕궁을 신어 수 ​신데렐라는 건넨 꼭 \n",
            "유리 구두를 소녀가 태어났어요 아기는 병이들어그만 고약한 소녀는 집안일이 언니는 무도회가 고개를 웃고 생쥐 두마리 외웠어요 그랬더니 예쁜 보거라 빛나는 호박으로 도마뱀으로 시가 해 가는 줄도 몰랐어요 짝이 벗겨졌어요 주울 짝을 이 주인과 나라를 그때 있나요 건넨 갔어요 오래오래 행복하게 \n",
            "땡 어느 소녀의 할머니가 말했어요 구두는 아기가 아기는 후 소녀보다 딸을 쉬곤 신데렐라라고 날 무도회가 싶니 보내주마호박 두마리 호박을 호박이 마차로 생쥐는 드레스로 주었어요신데렐라 되면 처음대로 돌아간단다 흰말은 시가 마음을 빼았겼어요 완자님은 다른 소리에 틈이 없었어요 가지고 보았지만한눈에 갔어요 결혼하여 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGwa9iaPeHyT"
      },
      "source": [
        "생성된 text의 embedding 변환기 구현<br>\n",
        "embedding은 org_text의 embedding과 비교하여 원문과 유사하게 민들기 위한 목적"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "_D7qcRmQeHyX"
      },
      "source": [
        "# 이거 이틀걸림...잘 몰라서 ㅈㄴ 헤맴\n",
        "\n",
        "@tf.custom_gradient\n",
        "def to_embedding(w):\n",
        "\n",
        "    def grad(dy):\n",
        "        dy = tf.nn.batch_normalization(dy, 0, tf.math.reduce_std(dy), 0, 10, 1e-3)  # gradient 증폭 \n",
        "        dy_arr = tf.reshape(dy,(dy.shape[0],1024,1))\n",
        "        #tf.print(dy_arr)\n",
        "        dy_arr_st = tf.image.resize(dy_arr, size=(dy.shape[0],w.shape[1]),method=tf.image.ResizeMethod.BILINEAR)\n",
        "        dy_arr_st = tf.reshape(dy_arr_st,shape=(dy.shape[0],w.shape[1]))\n",
        "        return dy_arr_st\n",
        "\n",
        "    #print(w)    \n",
        "    texts = []\n",
        "    value = None\n",
        "    texts = []\n",
        "    try:\n",
        "        s_w = tf.sort(w,axis=-1,direction='DESCENDING')\n",
        "        #print(s_w)\n",
        "        for w_0r,s_w_0r in zip(w,s_w):\n",
        "            text = \"\"\n",
        "            wh = tf.where(tf.math.greater_equal(w_0r,s_w_0r[_MAX_GEN_TOKEN]))[:_MAX_GEN_TOKEN]\n",
        "            for k in tf.reshape(wh,(_MAX_GEN_TOKEN,)).numpy():\n",
        "                text += word_table[k]+' '\n",
        "                 \n",
        "            texts.append(text)                 \n",
        "        value = tf.constant(embedder.encode(texts,show_progress_bar=False),dtype=tf.float64)\n",
        "    except Exception as ex:\n",
        "        tf.print(ex,sys.exc_info())\n",
        "\n",
        "    return value, grad"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "9Pu18GvweHyY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c10dab5-0178-4c69-becf-4e7c0cb4ae7e"
      },
      "source": [
        "# to_embedding 함수의 test\n",
        "e = to_embedding(w)\n",
        "for t in e:\n",
        "    print(t.numpy())"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.20847742 -0.24716251  0.19367166 ... -0.32198694  0.92360282\n",
            "  0.32289121]\n",
            "[-0.10688743 -0.04786511 -0.40968931 ... -0.28833583  1.11195648\n",
            "  0.21435505]\n",
            "[ 0.09529657 -0.2386384   0.08158018 ... -0.23322445  0.81238294\n",
            "  0.24843544]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "DWTe53wueHya"
      },
      "source": [
        "# to_compression_ratio\n",
        "\n",
        "@tf.custom_gradient\n",
        "def to_compression_ratio(w):\n",
        "    def grad(dy):\n",
        "        dy = tf.nn.batch_normalization(dy, 0, tf.math.reduce_std(dy), 0, 2, 1e-3)  # gradient 증폭, 이래도 되는 건지....\n",
        "        dy_arr = tf.reshape(dy,(dy.shape[0],1,1))\n",
        "        #tf.print(dy_arr)\n",
        "        dy_arr_st = tf.image.resize(dy_arr, size=(dy.shape[0],w.shape[1]),method=tf.image.ResizeMethod.BILINEAR)\n",
        "        dy_arr_st = tf.reshape(dy_arr_st,shape=(dy.shape[0],w.shape[1]))        \n",
        "        return dy_arr_st\n",
        "\n",
        "    const = tf.math.reduce_sum(tf.constant([x for x in range(tf.size(w[0]))],tf.int64)) \n",
        "    #print(const)\n",
        "    value = None\n",
        "    compression_ratio = [] #tf.Variable()\n",
        "    texts = []\n",
        "    try:\n",
        "        s_w = tf.sort(w,axis=-1,direction='DESCENDING')\n",
        "        #print(s_w)\n",
        "        for w_0r,s_w_0r in zip(w,s_w):\n",
        "            wh = tf.where(tf.math.greater_equal(w_0r,s_w_0r[_MAX_GEN_TOKEN]))[:_MAX_GEN_TOKEN]\n",
        "            cr = tf.math.reduce_sum(tf.reshape(wh,(_MAX_GEN_TOKEN,)))\n",
        "            compression_ratio.append(cr.numpy()/const.numpy())\n",
        "        #print(compression_ratio)\n",
        "        value = tf.constant(compression_ratio,dtype=tf.float64)\n",
        "    except Exception as ex:\n",
        "        tf.print(ex,sys.exc_info())\n",
        "\n",
        "    return value, grad\n",
        "    "
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Rk25esbweHyd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81814565-c8c0-4bd3-c2d5-76a0243efb5c"
      },
      "source": [
        "# to_compression_ratio 함수의 test\n",
        "e = to_compression_ratio(w)\n",
        "for t in e:\n",
        "    print(t.numpy())"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.14413702239789197\n",
            "0.17217391304347826\n",
            "0.1463768115942029\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxlFMo3GeHyg"
      },
      "source": [
        "생성된 text의 morpheme code 변환기 구현<br>\n",
        "morpheme code는 한국어 문장들(dataset)의 morpheme code와 비교하여 한국어 문법에 가깝게 만들기 위한 목적"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "yhOeXv93eHyh"
      },
      "source": [
        "\n",
        "@tf.custom_gradient\n",
        "def to_morpcoding(w):\n",
        "\n",
        "    def grad(dy):\n",
        "        dy = tf.nn.batch_normalization(dy, 0, tf.math.reduce_std(dy), 0, 2, 1e-3)  # gradient 증폭, 이래도 되는 건지....\n",
        "        dy_arr = tf.reshape(dy,(dy.shape[0],_MAX_MORP_LENGTH,1))\n",
        "        #tf.print(dy_arr)\n",
        "        dy_arr_st = tf.image.resize(dy_arr, size=(dy.shape[0],w.shape[1]),method=tf.image.ResizeMethod.BILINEAR)\n",
        "        dy_arr_st = tf.reshape(dy_arr_st,shape=(dy.shape[0],w.shape[1]))\n",
        "        return dy_arr_st\n",
        "\n",
        "    #print(w)    \n",
        "    codes = []\n",
        "    value = None\n",
        "    texts = []\n",
        "    try:\n",
        "        s_w = tf.sort(w,axis=-1,direction='DESCENDING')\n",
        "        #print(s_w)\n",
        "        for w_0r,s_w_0r in zip(w,s_w):\n",
        "            text = \"\"\n",
        "            wh = tf.where(tf.math.greater_equal(w_0r,s_w_0r[_MAX_GEN_TOKEN]))[:_MAX_GEN_TOKEN]\n",
        "            for k in tf.reshape(wh,(_MAX_GEN_TOKEN,)).numpy():\n",
        "                text += word_table[k]+' '\n",
        "                 \n",
        "            texts.append(text)    \n",
        "            \n",
        "        for sentence in texts:\n",
        "            code = morpheme_encode(sentence)\n",
        "            if len(code) <= _MAX_MORP_LENGTH:\n",
        "                codes.append(code + [_PADDING_CODE for i in range(_MAX_MORP_LENGTH-len(code))])\n",
        "            else:\n",
        "                codes.append(code[:_MAX_MORP_LENGTH])\n",
        "        value = tf.constant(codes,dtype=tf.int32)\n",
        "    except Exception as ex:\n",
        "        tf.print('to_morpcoding :' + str(ex),sys.exc_info())\n",
        "\n",
        "    return value, grad"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "S-C_pxE5eHyi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4799f35-633c-4ced-8cff-d7e1a1edc90b"
      },
      "source": [
        "# to_morpcoding 함수의 test\n",
        "e = to_morpcoding(w)\n",
        "for t in e:\n",
        "    print(t.numpy())"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2  5  2  2  2  2  5  2  5  2  5  2  2  5  2  5  2  2  5  6  2  5  6  2\n",
            "  9  5  6  2  5  2  5 14  2  2  5 13  2  5  8  6  2  9  5 10  6  2  5  2\n",
            "  5  8  6  2  6  6  2  5  2  2  4  2  5  6  2  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0]\n",
            "[ 2  2  5  2  5  6  2  5  2  5  6 10  8  2  5  2  5  2  5  2  5  2  5  6\n",
            "  2 14  2  6 13  8  6  6  2  5  2  5  2  2  6  2  5  6  2  5  6  6  2  5\n",
            "  2  2  5  2  5  2  8  6  6  2  8  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0]\n",
            "[ 2 10  2  5  2  5  2  6  2  5  2  5  2  5  2  2  5  2  5  6  2  5  2  2\n",
            "  5  6  6  2 14  2  2  5  2  5  2  5  2  5  2  5  6  2  6  2  5  6  8  2\n",
            "  2  5  6  6  2  5  2  2  5  2  5  8  2  5  6  2  5  6  2  6  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLtq1YP_eHyj"
      },
      "source": [
        "Network 구성을 위해 사용자 정의 Layer 를 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "F9EfbbSreHyk"
      },
      "source": [
        "# 이것도 잘 몰라서 하루 걸림... ㅜㅜ\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.layers import Layer\n",
        "\n",
        "#tf.executing_eagerly()\n",
        "\n",
        "class Post_processing(Layer):\n",
        "\n",
        "    def __init__(self, output_dim, encoder_func=None,Tout=tf.float64, **kwargs):\n",
        "        self.output_dim = output_dim\n",
        "        self.encoder = encoder_func\n",
        "        self.Tout = Tout\n",
        "        super(Post_processing, self).__init__(**kwargs)\n",
        "    '''\n",
        "    def build(self, input_shape):\n",
        "        tf.print('build',input_shape)\n",
        "        # 이 레이어에 대해 학습가능한 가중치 변수를 만듭니다.\n",
        "        self.kernel = self.add_weight(name='kernel', \n",
        "                                      shape=(input_shape[1], self.output_dim),\n",
        "                                      initializer='uniform',\n",
        "                                      trainable=True)\n",
        "        super(Post_processing, self).build(input_shape)  # 끝에서 꼭 이 함수를 호출하십시오\n",
        "    '''\n",
        "    def call(self, input_data):\n",
        "        #tf.print('Post_processing : call input_data',input_data.shape)\n",
        "        value = tf.py_function(self.encoder,[input_data],Tout=self.Tout,name='encode_func')\n",
        "        #print('value.shape:',value.shape)\n",
        "        #value.set_shape((input_data.shape[0],self.output_dim))\n",
        "        if self.output_dim > 0:\n",
        "            value.set_shape((input_data.shape[0],self.output_dim))\n",
        "        else:\n",
        "            value.set_shape((input_data.shape[0],))\n",
        "        #return tf.reshape(value,[input_data.shape[0]])  \n",
        "\n",
        "        #value = tf.Variable((tf.zeros([input_data.shape[0],1024]) if self.Tout==tf.float64 else tf.zeros([input_data.shape[0],])),dtype=self.Tout,shape=( (input_data.shape[0],1024) if self.Tout==tf.float64 else (input_data.shape[0],)))\n",
        "        #tf.py_function(self.encoder,[input_data],Tout=self.Tout)\n",
        "        return value\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        tf.print('compute_output_shape:',input_shape)\n",
        "        input_shape = tensor_shape.TensorShape(input_shape).as_list()\n",
        "        if self.output_dim > 0:\n",
        "            return tensor_shape.TensorShape([input_shape[0], self.output_dim])\n",
        "        return tensor_shape.TensorShape([input_shape[0]])"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "iT--4r5BeHyk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23713139-a7df-429e-e1ca-aadc5099daa1"
      },
      "source": [
        "# 구성한 Layer의 test\n",
        "e = Post_processing(1024,to_embedding,Tout=tf.float64)(w)\n",
        "for c in e:\n",
        "    print(c)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[ 0.20847742 -0.24716251  0.19367166 ... -0.32198694  0.92360282\n",
            "  0.32289121], shape=(1024,), dtype=float64)\n",
            "tf.Tensor(\n",
            "[-0.10688743 -0.04786511 -0.40968931 ... -0.28833583  1.11195648\n",
            "  0.21435505], shape=(1024,), dtype=float64)\n",
            "tf.Tensor(\n",
            "[ 0.09529657 -0.2386384   0.08158018 ... -0.23322445  0.81238294\n",
            "  0.24843544], shape=(1024,), dtype=float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "X0RIQRZ4eHyl"
      },
      "source": [
        "# 별로 중요하지는 않지만 Lambda layer를 활용하기 위한 assert 함수 구성\n",
        "def assert_layer(input_data,out_dim=None):\n",
        "    #tf.print(input_data)\n",
        "    #print(input_data)\n",
        "    assert input_data.shape[1] == out_dim\n",
        "    return input_data"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "VJazT7y-eHyl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6af4be19-38cc-4c2c-df80-b2d54802a2b4"
      },
      "source": [
        "# 드디어 generator 구현\n",
        "# 효과적으로 구성된 것인지는 모르겠음... 이것은 아직 많은 연구가 필요함.\n",
        "# 또한 LSTM으로 바꾸어 길이의 한게를 극복해야 할 것...\n",
        "\n",
        "def make_generator_model(org_words):\n",
        "    input = Input(shape=(org_words,), dtype='float64') \n",
        "    x1 = Dense(org_words*2, use_bias=False)(input)\n",
        "    #x1 = BatchNormalization()(x1)\n",
        "    x1 = LeakyReLU()(x1)\n",
        "    \n",
        "    x1 = Dense(org_words*4, use_bias=False)(x1)\n",
        "    #x1 = BatchNormalization()(x1)\n",
        "    x1 = LeakyReLU()(x1)\n",
        "    \n",
        "    #x1 = Dense(max_length*total_words, use_bias=False, activation='tanh')(x1)\n",
        "    x1 = Dense(org_words, use_bias=False)(x1)\n",
        "    x1 = Lambda(assert_layer,arguments={'out_dim':org_words})(x1)\n",
        "    #x1 = Reshape((max_length, total_words))(x1)\n",
        "    #x1 = BatchNormalization()(x1)\n",
        "    #x1 = Softmax()(x1)        \n",
        "    #x1 = MyCustomLayer(max_length*total_words)(x1)\n",
        "    txt = Post_processing(0,to_text,Tout=tf.string)(x1)\n",
        "    emb = Post_processing(1024,to_embedding,Tout=tf.float64)(x1)\n",
        "    cmr = Post_processing(0,to_compression_ratio,Tout=tf.float64)(x1)\n",
        "    cod = Post_processing(128,to_morpcoding,Tout=tf.int32)(x1)\n",
        "    \n",
        "    model = Model(input,[txt,emb,cmr,cod])\n",
        "    \n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "generator = make_generator_model(_NOISE_DIM)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 276)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 552)          152352      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)         (None, 552)          0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1104)         609408      leaky_re_lu[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)       (None, 1104)         0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 276)          304704      leaky_re_lu_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 276)          0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "post_processing_1 (Post_process (None,)              0           lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "post_processing_2 (Post_process (None, 1024)         0           lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "post_processing_3 (Post_process (None,)              0           lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "post_processing_4 (Post_process (None, 128)          0           lambda[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 1,066,464\n",
            "Trainable params: 1,066,464\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "sQya4kMheHym",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72f1d967-1190-40e7-8f62-45231715270a"
      },
      "source": [
        "# generator의 test\n",
        "# Create a random noise and generate a sample\n",
        "noise = tf.random.normal([3,_NOISE_DIM])\n",
        "texts,embeddings,compratios,morpcodes = generator(noise, training=True)\n",
        "print(texts.shape)\n",
        "for i,txt in zip(range(len(texts)),texts):\n",
        "    print(f\" {i+1}> {txt.numpy().decode('utf-8')}\" )\n",
        "print(embeddings.shape)\n",
        "print(compratios.shape)\n",
        "print(morpcodes.shape)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3,)\n",
            " 1> 유리 새어머니는 어느 열두 말했어요 신하들은 옛날 아기는 고운 세상을 못마땅했어요 하루 닦고 도맡아 불러야겠어요 온통 언니는 놀려 훌쩍훌쩍 시작했어요 있었어요 오렴 지팡이로 반짝이는 할머니는 주었어요신데렐라 되면 모든게 처음대로 마부는 도마뱀으로 마음을 벗겨졌어요 틈이 짝을 주인과 보고 보았지만한눈에 그때 조용히 \n",
            " 2> 어느 말했어요 신하들은 아기가 무럭무럭 고운 아버지는 홀로 못마땅했어요 그런데 닦고 도맡아 온통 소녀를 댔어요 왕궁에서 혼자 시작했어요 싶니 내가 그리고 모든게 생쥐로 마부는 그러니까 바늗시 시가 알겠지 빼았겼어요 완자님은 소리에 화들짝 빠져나가는데 짝이 틈이 임금님께 주인과 그때 한번 그 \n",
            " 3> 소녀가 땡 어느 해도 신데렐라에게 구두는 옛날 자라서 고약한 딸들보다 못마땅했어요 도맡아 없는 저애를 싶었어요 시작했어요 있었어요 지팡이로 마부로 할머니는 주었어요신데렐라 처음대로 생쥐로 그러니까 시가 왕자님도 마음을 아가씨들은 왕자님과 가는 몰랐어요 시를 알리는 화들짝 짝이 주인과 나라를 오므려도 늘려도 보기에도 \n",
            "(3, 1024)\n",
            "(3,)\n",
            "(3, 128)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41aWytGveHyo"
      },
      "source": [
        "# Discriminator 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtjQGj54eHyo"
      },
      "source": [
        "먼저 요약을 구분하기 위한 discriminator_summ 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Fx3GgEXEeHyo"
      },
      "source": [
        "# 문장에 대한 embeddings를 이용하여 org_text_emb (org_text의 embedding)과의 유사도를 계산한다.\n",
        "import scipy\n",
        "\n",
        "@tf.custom_gradient\n",
        "def to_similarity(w):\n",
        "\n",
        "    def grad(dy):\n",
        "        dy = tf.nn.batch_normalization(dy, 0, tf.math.reduce_std(dy), 0, 10, 1e-3)  # gradient 증폭, 이래도 되는 건지....\n",
        "        dy_arr = tf.reshape(dy,(dy.shape[0],1,1))\n",
        "        #tf.print(dy_arr)\n",
        "        dy_arr_st = tf.image.resize(dy_arr, size=(dy.shape[0],1024),method=tf.image.ResizeMethod.BILINEAR)\n",
        "        dy_arr_st = tf.reshape(dy_arr_st,shape=(dy.shape[0],1024))\n",
        "        return dy_arr_st\n",
        "\n",
        "    similarities = []\n",
        "    value = None\n",
        "    try:\n",
        "        for embedding in w:\n",
        "            distances = scipy.spatial.distance.cdist([embedding], [org_text_emb], \"cosine\")[0]\n",
        "            similarities.append(distances[0])\n",
        "            \n",
        "        value = tf.constant(similarities,dtype=tf.float64)\n",
        "    except Exception as ex:\n",
        "        tf.print(ex,sys.exc_info())\n",
        "\n",
        "    return value, grad\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "DNIQNvIveHyp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcd5af00-f889-4672-9323-c733169eabaf"
      },
      "source": [
        "def make_discriminator_model():\n",
        "    input_emb = Input(shape=(1024,), dtype='float64') \n",
        "    x1 = Post_processing(0,to_similarity,Tout=tf.float64)(input_emb)\n",
        "    x1 = Reshape((1,))(x1)    \n",
        "    #x1 = Dense(1024*2)(input_emb)\n",
        "    #x1 = BatchNormalization()(x1)\n",
        "    #x1 = LeakyReLU()(x1)\n",
        "\n",
        "    \n",
        "    input_cmp = Input(shape=(), dtype='float64') \n",
        "    imput_mrp = Input(shape=(_MAX_MORP_LENGTH,), dtype='int32')\n",
        "\n",
        "    x2 = Reshape((1,))(input_cmp)\n",
        "    x3 = Dense(256)(imput_mrp)\n",
        "    #x3 = BatchNormalization()(x3)\n",
        "    x3 = LeakyReLU()(x3)\n",
        "\n",
        "    x3 = Dense(256*3)(x3)\n",
        "    #x3 = BatchNormalization()(x3)\n",
        "    x3 = LeakyReLU()(x3)\n",
        "\n",
        "    concatted = Concatenate(axis=1)([x1, x2, x3])\n",
        "    x4 = Flatten()(concatted)\n",
        "    x4 = Dense(64, use_bias=False)(x4)\n",
        "    #x4 = BatchNormalization()(x4)\n",
        "    x4 = LeakyReLU()(x4)\n",
        "    x4 = Dense(1)(x4)\n",
        "    \n",
        "    model = Model([input_emb,input_cmp,imput_mrp],x4)\n",
        "    \n",
        "    model.summary()\n",
        "    \n",
        "    return model\n",
        "\n",
        "discriminator = make_discriminator_model()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 256)          33024       input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 1024)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)       (None, 256)          0           dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "post_processing_5 (Post_process (None,)              0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None,)]            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 768)          197376      leaky_re_lu_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, 1)            0           post_processing_5[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 1)            0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)       (None, 768)          0           dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 770)          0           reshape[0][0]                    \n",
            "                                                                 reshape_1[0][0]                  \n",
            "                                                                 leaky_re_lu_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 770)          0           concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 64)           49280       flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)       (None, 64)           0           dense_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 1)            65          leaky_re_lu_4[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 279,745\n",
            "Trainable params: 279,745\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "y7owqbg4eHyp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfe220fb-944b-4e07-ecd8-d71f3580f55b"
      },
      "source": [
        "# discriminator test\n",
        "\n",
        "predict = discriminator([embeddings,compratios,morpcodes])\n",
        "print(predict)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[-0.75045085]\n",
            " [-1.6413989 ]\n",
            " [-3.4098363 ]], shape=(3, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmvyPAVSeHyq"
      },
      "source": [
        "# GAN 을 이용한 loss 의 gradient 구현 --> 빡심"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ECEMH-yweHyr"
      },
      "source": [
        "# This method returns a helper function to compute cross entropy loss\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "cvBC5yS6eHys"
      },
      "source": [
        "# 디짐\n",
        "@tf.function\n",
        "def train_step(real_embedding,real_morpcoding):\n",
        "  \n",
        "    # 1 - Create a random noise to feed it into the model\n",
        "    # for the text generation\n",
        "    noise = tf.random.normal([BATCH_SIZE, _NOISE_DIM])\n",
        "    \n",
        "    # 2 - Generate text and calculate loss values\n",
        "    # GradientTape method records operations for automatic differentiation.\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        texts,embeddings,compratios,morpcodes = generator(noise, training=True)\n",
        "        #embeddings = generator(noise, training=True)\n",
        "        # real에 가까우려면 discriminator의 학습은 real_embedding 이 zero (0)에 가깝게 학습시켜야 함.\n",
        "        # 하지만 압축율의 개념으로는 본래는 ones (1)가 맞음.\n",
        "        real_output = discriminator([real_embedding,np.zeros(len(real_embedding)),real_morpcoding], training=True)\n",
        "        fake_output = discriminator([embeddings,compratios,morpcodes], training=True)\n",
        "\n",
        "        #tf.print('train_step : embeddings.shape=',embeddings.shape)\n",
        "        gen_loss = generator_loss(fake_output)\n",
        "        #tf.print('train_step : gen_loss=',gen_loss)\n",
        "        disc_loss = discriminator_loss(real_output, fake_output)\n",
        "        #tf.print('train_step : disc_loss=',disc_loss)\n",
        "\n",
        "    # 3 - Calculate gradients using loss values and model variables\n",
        "    # \"gradient\" method computes the gradient using \n",
        "    # operations recorded in context of this tape (gen_tape and disc_tape).\n",
        "    \n",
        "    # It accepts a target (e.g., gen_loss) variable and \n",
        "    # a source variable (e.g.,generator.trainable_variables)\n",
        "    # target --> a list or nested structure of Tensors or Variables to be differentiated.\n",
        "    # source --> a list or nested structure of Tensors or Variables.\n",
        "    # target will be differentiated against elements in sources.\n",
        "\n",
        "    # \"gradient\" method returns a list or nested structure of Tensors  \n",
        "    # (or IndexedSlices, or None), one for each element in sources. \n",
        "    # Returned structure is the same as the structure of sources.\n",
        "    \n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, \n",
        "                                                discriminator.trainable_variables)\n",
        "    #tf.print('train_step : gradients_of_discriminator=',gradients_of_discriminator)   \n",
        "    noise = tf.random.normal([BATCH_SIZE, _NOISE_DIM])\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        texts,embeddings,compratios,morpcodes = generator(noise, training=True)\n",
        "        #embeddings = generator(noise, training=True)\n",
        "        #real_output = discriminator(real_embedding, training=True)\n",
        "        fake_output = discriminator([embeddings,compratios,morpcodes], training=True)\n",
        "\n",
        "        #tf.print('train_step : embeddings.shape=',embeddings.shape)\n",
        "        gen_loss = generator_loss(fake_output)\n",
        "        #tf.print('train_step : gen_loss=',gen_loss)\n",
        "        #disc_loss = discriminator_loss(real_output, fake_output)\n",
        "        #tf.print('train_step : disc_loss=',disc_loss)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, \n",
        "                                               generator.trainable_variables)\n",
        "    #tf.print('train_step : gradients_of_generator=',gradients_of_generator)\n",
        " \n",
        "\n",
        "    # 4 - Process  Gradients and Run the Optimizer\n",
        "    # \"apply_gradients\" method processes aggregated gradients. \n",
        "    # ex: optimizer.apply_gradients(zip(grads, vars))\n",
        "    \"\"\"\n",
        "    Example use of apply_gradients:\n",
        "    grads = tape.gradient(loss, vars)\n",
        "    grads = tf.distribute.get_replica_context().all_reduce('sum', grads)\n",
        "    # Processing aggregated gradients.\n",
        "    optimizer.apply_gradients(zip(grads, vars), experimental_aggregate_gradients=False)\n",
        "    \"\"\"\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "    #tf.print('train_step : after discriminator_optimizer')    "
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "cJs3lBpJeHys"
      },
      "source": [
        "EPOCHS = 5\n",
        "# 요약문 생성의 확인을 위해 10개의 문장을 생성하고 train과정에서 각 epoch마다 변화를 확인한다.\n",
        "seed = tf.random.normal([10, _NOISE_DIM])"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "os3b0psFeHyt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd68a0d6-e274-491e-bbe7-32beba7130ad"
      },
      "source": [
        "# 생성된 문장의 원문 유사도를 측정하기 위한 함수\n",
        "\n",
        "import scipy\n",
        "#print(doc_emb)\n",
        "def similarity_score(queries,org_embedding):\n",
        "\n",
        "    total_score = 0\n",
        "    query_embeddings = embedder.encode(queries,show_progress_bar=False)\n",
        "    for query, query_embedding in zip(queries, query_embeddings):\n",
        "        distances = scipy.spatial.distance.cdist([query_embedding], [org_embedding], \"cosine\")[0]\n",
        "        results = zip(range(len(distances)), distances)\n",
        "        for idx, distance in results:\n",
        "            total_score += 1-distance\n",
        "    return total_score\n",
        "\n",
        "queries = []\n",
        "\n",
        "texts,embeddings,compratios,morpcodes = generator(seed,training=False)\n",
        "\n",
        "#count = 0\n",
        "for t in texts:\n",
        "    summary_text = t.numpy().decode('utf-8')\n",
        "    queries.append(summary_text)\n",
        "print('Similarity score:',str(similarity_score(queries,org_text_emb)))\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Similarity score: 5.63828260513954\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "cXrsOXX4eHyt"
      },
      "source": [
        "# Print iterations progress\n",
        "class ProgressBar:\n",
        "    # pb = ProgressBar(total=20, prefix = 'Epoch 1')\n",
        "    def __init__(self,total=20, prefix = '', suffix = '', decimals = 1, length = 20, fill = '█', printEnd = \"\\r\"):\n",
        "        self.total = total\n",
        "        self.prefix = prefix\n",
        "        self.suffix = suffix\n",
        "        self.decimals = decimals\n",
        "        self.length = length\n",
        "        self.fill = fill\n",
        "        self.printEnd = printEnd\n",
        "        self.ite = 0\n",
        "    # pb.printProgress(1,'~~~~')\n",
        "    def printProgress(self,iteration, text):\n",
        "        self.ite += iteration\n",
        "        percent = (\"{0:.\" + str(self.decimals) + \"f}\").format(100 * (self.ite / float(self.total)))\n",
        "\n",
        "        filledLength = int(self.length * self.ite // self.total)\n",
        "        bar = self.fill * filledLength + '-' * (self.length - filledLength)\n",
        "        print(f'\\r{self.prefix} |{bar}| {percent}% {self.suffix}  {text}', end=\"\", flush=True)\n",
        "        # Print New Line on Complete\n",
        "        if self.ite == self.total: \n",
        "            print()"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "dhqdaIATeHyu"
      },
      "source": [
        "import time\n",
        "from IPython import display # A command shell for interactive computing in Python.\n",
        "import re\n",
        "\n",
        "def train(dataset, epochs):\n",
        "    print('Start with seed text.')\n",
        "    seed = tf.random.normal([10, _NOISE_DIM])\n",
        "    print('-------------------------------------------------------')\n",
        "    texts,embeddings,compratios,morpcodes = generator(seed,training=False)\n",
        "    for i,t in  zip(range(len(texts)),texts):\n",
        "        summary_text = t.numpy().decode('utf-8')\n",
        "        print(f'{i+1} > {summary_text}')        \n",
        "    print('-------------------------------------------------------')\n",
        "    print('')\n",
        "\n",
        "    # A. For each epoch, do the following:\n",
        "    for epoch in range(epochs):\n",
        "        start = time.time()\n",
        "        pb = ProgressBar(total=BATCH_COUNT, prefix = f'Epoch:{str(epoch+1)}/{epochs}')\n",
        "        pb.printProgress(0,'Start batch.')\n",
        "        # 1 - For each batch of the epoch, \n",
        "        for batch_num,(emb_batch_set,cod_batch_set) in zip(range(len(dataset)),dataset):\n",
        "            # 1.a - run the custom \"train_step\" function\n",
        "            # we just declared above\n",
        "            #print(image_batch.shape)\n",
        "            train_step(emb_batch_set,cod_batch_set)\n",
        "            texts,embeddings,compratios,morpcodes = generator(seed,training=False)\n",
        "            pb.printProgress(+1,f\"Time for batch {batch_num + 1}/{BATCH_COUNT} is {int(time.time()-start)} sec, generated text:{texts[0].numpy().decode('utf-8')}\")\n",
        "        # 4 - Print out the completed epoch no. and the time spent\n",
        "        #print (f'Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "        #texts,embeddings,compratios,morpcodes = generator(seed,training=False)\n",
        "        count = 0\n",
        "        queries = []\n",
        "        texts,embeddings,compratios,morpcodes = generator(seed,training=False)\n",
        "        for i,t in  zip(range(len(texts)),texts):\n",
        "            summary_text = t.numpy().decode('utf-8')\n",
        "            print(f'{i+1} > {summary_text}')\n",
        "            queries.append(summary_text)\n",
        "            c = [m.start() for m in re.finditer(_MISMATCH_WORD, summary_text)]\n",
        "            count += len(c)\n",
        "        print(f'Mismatch count:{count} Similarity score:{str(similarity_score(queries,org_text_emb))}')\n",
        "        print('')"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCAedaSeT6A1",
        "trusted": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "outputId": "4d3fc962-e36c-45a8-9516-44c6137270bb"
      },
      "source": [
        "if DO_ALL:\n",
        "    EPOCHS = 30\n",
        "    train(dataset, EPOCHS)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start with seed text.\n",
            "-------------------------------------------------------\n",
            "1 > 구두를 신데렐라의 어느 남은 가고 황금 딸을 자기 딸들보다 종일 도맡아 저애를 온통 언니들을 신데렐라도 시작했어요 들어보니 주문을 호박을 반짝이는 할머니는 되면 처음대로 마부는 도마뱀으로 왕자님도 아름다운 마음을 추었어요 몰랐어요 벽시계가 시를 알리는 뛰쫓아오던 주인과 돌아다녔어요 보기에도 저도 한번 수 \n",
            "2 > 신데렐라의 소녀의 두 언니들은 황금 신데렐라를 구두는 귀여운 아기는 아버지는 고약한 딸들보다 못마땅했어요 잠시 쉬곤 놀려 댔어요 무도회장으로 시작했어요 빙그레 주문을 건드렸어요 장식이 할머니는 신겨 주었어요신데렐라 처음대로 마부는 도마뱀으로 해 아름다운 마음을 아가씨들은 시를 알리는 왕궁을 틈이 뛰쫓아오던 수 완자님과 \n",
            "3 > 신데렐라의 어느 신데렐라 도마뱀을 황금 신데렐라에게 열두 말했어요 구두는 옛날 고운 말았어요 딸들보다 못마땅했어요 지칠때면난롯가에 불러야겠어요 온통 놀려 언니들을 시작했어요 싶니 있었어요 그리고 지팡이로 생쥐는 할머니는 주었어요신데렐라 되면 생쥐로 도마뱀으로 알겠지 마음을 추었어요 허둥지둥 짝이 임금님께 한번 신하게 건넨 완자님과 \n",
            "4 > 새어머니는 데리고 무도회에 신데렐라가 언니들은 신데렐라 열두 한 여자 무럭무럭 고운 후 못마땅했어요 그런데 도맡아 불러야겠어요 놀려 신데렐라도 너도 지팡이로 마부로 장식이 할머니는 신겨 주었어요신데렐라 열두시가 모든게 마부는 도마뱀으로 바늗시 빼았겼어요 시간 시를 알리는 화들짝 벗겨졌어요 틈이 주인과 결혼하겠어요 찾아 \n",
            "5 > 신데렐라의 이번에는 신데렐라에게 신데렐라를 아기가 세상을 얼마 맞이했어요 성질이 딸들보다 못마땅했어요 그런데 종일 없는 잠시 온통 집에도 시작했어요 그리고 호박이 반짝이는 예쁜 할머니는 처음대로 돌아간단다 호박으로 흰말은 시가 왕자님도 빼았겼어요 몰랐어요 벽시계가 알리는 소리에 화들짝 틈이 임금님께 보았지만한눈에 보기에도 저도 \n",
            "6 > 신데렐라의 신데렐라 왕자님은 아기가 고운 후 맞이했어요 착한 소녀는 하루 종일 도맡아 소녀를 놀려 집에도 언니들을 싶었어요 훌쩍훌쩍 너도 내가 너를 두마리 오렴 주문을 그리고 도마뱀은 장식이 주었어요신데렐라 호박으로 생쥐로 마음을 빼았겼어요 시간 벽시계가 시를 주울 보았지만한눈에 보기에도 한번 있나요 \n",
            "7 > 신데렐라의 예쁘고 두 신데렐라에게 말했어요 신하들은 무럭무럭 고운 되었어요 말았어요 위인 고약한 게 종일 불러야겠어요 훌쩍훌쩍 시작했어요 싶니 있었어요 외웠어요 지팡이로 생쥐는 할머니는 신겨 왕자님도 아름다운 아가씨들은 않고 왕자님과 시를 소리에 화들짝 없었어요 짝을 주인과 결혼하겠어요 온 보았지만한눈에 그때 한번 \n",
            "8 > 새어머니는 어느 왔어요 황금 말했어요 구두는 자라서 후 고약한 못마땅했어요 아버지마저 종일 온통 놀려 언니들을 훌쩍훌쩍 시작했어요 들어보니 두마리 주문을 호박을 화려한 마부로 할머니는 주었어요신데렐라 바늗시 알겠지 왕자님도 아름다운 마음을 알리는 소리에 왕궁을 없었어요 임금님께 주인과 보기에도 너무 다가와 한번 \n",
            "9 > 어느 열두 말했어요 신하들은 구두는 옛날 떠나고 맞이했어요 나이가 딸들보다 그런데 하루 집안일을 지칠때면난롯가에 쉬곤 불러야겠어요 온통 집에도 무도회장으로 시작했어요 한개와 그리고 도마뱀은 구슬 처음대로 흰말은 마부는 도마뱀으로 시가 돌아와야 알겠지 왕자님도 아름다운 아가씨들은 왕자님과 시를 소리에 틈이 짝을 임금님께 \n",
            "10 > 새어머니는 신데렐라의 신데렐라 도마뱀을 신하들은 구두는 옛날 고운 걱정되었어요 딸을 못마땅했어요 소녀는 하루 온통 놀려 댔어요 무도회가 있었어요 두마리 그리고 건드렸어요 마부로 예쁜 할머니는 되면 처음대로 모인 추었어요 시간 시를 알리는 화들짝 허둥지둥 틈이 주인과 보았지만한눈에 보기에도 저도 왕궁으로 그 \n",
            "-------------------------------------------------------\n",
            "\n",
            "Epoch:1/30 |--------------------| 0.0%   Start batch."
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-8a3811f0ae15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mDO_ALL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mEPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-52-b91b429fa6db>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, epochs)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m# we just declared above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m#print(image_batch.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_batch_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcod_batch_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcompratios\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmorpcodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintProgress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34mf\"Time for batch {batch_num + 1}/{BATCH_COUNT} is {int(time.time()-start)} sec, generated text:{texts[0].numpy().decode('utf-8')}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}
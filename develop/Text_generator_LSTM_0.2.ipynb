{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dolmani38/Summary/blob/master/Text_generator_LSTM_0.2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtzolBfwXm42"
      },
      "source": [
        "## Text Generation With LSTM Recurrent Neural Networks in Python with Keras\n",
        "\n",
        "https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVJ-l2pVqHzF",
        "outputId": "66ed645b-21ad-4a39-b25b-f51d38d44de0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.6/dist-packages (0.5.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from konlpy) (0.4.4)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.19.4)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.2.0)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.6.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikZWgwpkXs-Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e7d4cf8-0245-4b31-bfb9-72239866188c"
      },
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "\n",
        "import urllib.request\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from konlpy.tag import Okt\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HnUWvkCpvyd"
      },
      "source": [
        "# 간단한 전처리\r\n",
        "def clean_text(txt):\r\n",
        "    txt = txt.replace('\\n',' ')\r\n",
        "    txt = txt.replace('\\r',' ')    \r\n",
        "    txt = txt.replace('=','')\r\n",
        "    txt = txt.replace('\\\"','')   \r\n",
        "    txt = txt.replace('\\'','')\r\n",
        "    txt = txt.replace(',','')\r\n",
        "    txt = txt.replace('..','')\r\n",
        "    txt = txt.replace('.','. ')\r\n",
        "    txt = txt.replace('  ',' ')\r\n",
        "    txt = txt.replace('  ',' ')    \r\n",
        "    txt = txt.replace('  ',' ')   \r\n",
        "    txt = txt.replace('  ',' ')           \r\n",
        "    return txt "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IGtTyGCXzzV"
      },
      "source": [
        "# load ascii text and covert to lowercase\n",
        "\n",
        "def get_sample_text(target_url):\n",
        "    raw_text = urllib.request.urlopen(target_url).read().decode('utf-8')\n",
        "    #raw_text = raw_text.lower()\n",
        "    return nltk.sent_tokenize(clean_text(raw_text))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FcVZ6WDp70K"
      },
      "source": [
        "ko_sentences_dataset = []"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KScTVrDGf0CN"
      },
      "source": [
        "ko_sentences_dataset += get_sample_text(\"https://raw.githubusercontent.com/dolmani38/Summary/master/data/korean_sample.txt\")\n",
        "# 여기서 계속 수집...\n",
        "ko_sentences_dataset += get_sample_text(\"https://github.com/dolmani38/Summary/blob/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-ABC%20%EC%82%B4%EC%9D%B8%EC%82%AC%EA%B1%B4.txt\")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeA_STVgGYC7"
      },
      "source": [
        "org_text = \"\"\"\r\n",
        "옛날 어느 집에 귀여운 여자 아기가 태어났어요.\r\n",
        "아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요.\r\n",
        "그러던 어느날, 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요.\r\n",
        "소녀의 아버지는 홀로 남은 소녀가 걱정되었어요.\r\n",
        "그래서 얼마 후 새어머니를 맞이했어요.\r\n",
        "새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요.\r\n",
        "그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요.\r\n",
        "새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요.\r\n",
        "그런데 이번에는 아버지마저 돌아가셨어요.\r\n",
        "소녀는 하녀처럼 하루 종일 쓸고, 닦고, 집안일을 도맡아 했어요.\r\n",
        "해도 해도 끝이 없는 집안일이 힘들어 지칠때면\r\n",
        "난롯가에 앉아서 잠시 쉬곤 했지요.\r\n",
        "\"엄마, 저애를 신데렐라라고 불러야겠어요.\"\r\n",
        "\"온통 재투성이잖아요. 호호호!\" 두 언니는 소녀를 놀려 댔어요.\r\n",
        "어느 날, 왕궁에서 무도회가 열렸어요.\r\n",
        "신데렐라의 집에도 초대장이 왔어요.\r\n",
        "새어머니는 언니들을 데리고 무도회장으로 떠났어요.\r\n",
        "신데렐라도 무도회에 가고 싶었어요.\r\n",
        "혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요.\r\n",
        "\"신데렐라, 너도 무도회에 가고 싶니?\"\r\n",
        "신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요.\r\n",
        "\"내가 너를 무도회에 보내주마 호박 한개와 생쥐 두마리, 도마뱀을 구해 오렴.\"\r\n",
        "마법사 할머니가 주문을 외웠어요.\r\n",
        "그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금 마차로 변했어요.\r\n",
        "이번에는 생쥐와 도마뱀을 건드렸어요.\r\n",
        "그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했답니다.\r\n",
        "신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요.\r\n",
        "\"신데렐라, 발을 내밀어 보거라.\"\r\n",
        "할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요.\r\n",
        "\"신데렐라, 밤 열두시가 되면 모든게 처음대로 돌아간단다. 황금 마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼. 그러니까 반드시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지?\"\r\n",
        "왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요.\r\n",
        "왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고,신데렐라하고만 춤을 추었어요.\r\n",
        "신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요.\r\n",
        "땡, 땡, 땡...... 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요.\r\n",
        "신데렐라가 허둥지둥 왕궁을 빠져나가는데,\r\n",
        "유리 구두 한 짝이 벗겨졌어요.\r\n",
        "하지만 구두를 주울 틈이 없었어요.\r\n",
        "신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요.\r\n",
        "왕자님은 유리 구두를 가지고 임금님께 가서 말했어요.\r\n",
        "\"이 유리 구두의 주인과 결혼하겠어요.\"\r\n",
        "그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요.\r\n",
        "언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만 한눈에 보기에도 유리 구두는 너무 작았어요.\r\n",
        "그때, 신데렐라가 조용히 다가와 말했어요.\r\n",
        "\"저도 한번 신어 볼 수 있나요?\"\r\n",
        "신데렐라는 신하게 건넨 유리 구두를 신었어요,\r\n",
        "유리 구두는 신데렐라의 발에 꼭 맞았어요.\r\n",
        "신하들은 신데렐라를 왕궁으로 데리고 갔어요.\r\n",
        "그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요.\r\n",
        "\"\"\""
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IJZ7zpnGe32"
      },
      "source": [
        "ko_sentences_dataset += [clean_text(org_text)]"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fGeeH8UGrAB",
        "outputId": "15ae6d1e-eb18-435a-b244-ef536e86c617",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(ko_sentences_dataset)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OR8DiPNT-38w"
      },
      "source": [
        "text = ' '.join(ko_sentences_dataset)\r\n",
        "text = text.strip()"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLcAbrJ-AK4D",
        "outputId": "cb1b2561-ec8b-4546-c886-75fa523538a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(text)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1516"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NN0fohhAviv"
      },
      "source": [
        "text = clean_text(text)"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnClYQSL-1La",
        "outputId": "1088fb8f-166c-4af2-a9a2-3161cda97f58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Corpus length:\", len(text))\r\n",
        "\r\n",
        "chars = sorted(list(set(text)))\r\n",
        "print(\"Total chars:\", len(chars))\r\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\r\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\r\n",
        "\r\n",
        "# cut the text in semi-redundant sequences of maxlen characters\r\n",
        "maxlen = 40\r\n",
        "step = 3\r\n",
        "sentences = []\r\n",
        "next_chars = []\r\n",
        "for i in range(0, len(text) - maxlen, step):\r\n",
        "    sentences.append(text[i : i + maxlen])\r\n",
        "    next_chars.append(text[i + maxlen])\r\n",
        "print(\"Number of sequences:\", len(sentences))\r\n",
        "\r\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\r\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\r\n",
        "for i, sentence in enumerate(sentences):\r\n",
        "    for t, char in enumerate(sentence):\r\n",
        "        x[i, t, char_indices[char]] = 1\r\n",
        "    y[i, char_indices[next_chars[i]]] = 1\r\n"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corpus length: 2946\n",
            "Total chars: 256\n",
            "Number of sequences: 969\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWo3MvxHBAdm",
        "outputId": "c371133a-1e0a-406d-befe-46a1e8009e96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from tensorflow import keras\r\n",
        "from tensorflow.keras import layers\r\n",
        "\r\n",
        "import random\r\n",
        "import io\r\n",
        "\r\n",
        "model = keras.Sequential(\r\n",
        "    [\r\n",
        "        keras.Input(shape=(maxlen, len(chars))),\r\n",
        "        layers.LSTM(256,return_sequences=True),\r\n",
        "        layers.Dropout(0.2),\r\n",
        "        #layers.LSTM(256),\r\n",
        "        #layers.Dropout(0.2),\r\n",
        "        layers.LSTM(128),\r\n",
        "        layers.Dense(len(chars), activation=\"softmax\"),\r\n",
        "    ]\r\n",
        ")\r\n",
        "model.summary()\r\n",
        "optimizer = keras.optimizers.RMSprop(learning_rate=0.01)\r\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_20\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_35 (LSTM)               (None, 40, 256)           525312    \n",
            "_________________________________________________________________\n",
            "dropout_30 (Dropout)         (None, 40, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_36 (LSTM)               (None, 128)               197120    \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 256)               33024     \n",
            "=================================================================\n",
            "Total params: 755,456\n",
            "Trainable params: 755,456\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_DW18Q_BdLS"
      },
      "source": [
        "def sample(preds, temperature=1.0):\r\n",
        "    # helper function to sample an index from a probability array\r\n",
        "    preds = np.asarray(preds).astype(\"float64\")\r\n",
        "    preds = np.log(preds) / temperature\r\n",
        "    exp_preds = np.exp(preds)\r\n",
        "    preds = exp_preds / np.sum(exp_preds)\r\n",
        "    probas = np.random.multinomial(1, preds, 1)\r\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdluUe5nBgjE",
        "outputId": "08b1bc24-a8b9-417d-800f-c5c46da44cb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epochs = 40\r\n",
        "batch_size = 128\r\n",
        "\r\n",
        "for epoch in range(epochs):\r\n",
        "    model.fit(x, y, batch_size=batch_size, epochs=1000,verbose=0)\r\n",
        "    print()\r\n",
        "    print(\"Generating text after epoch: %d\" % epoch)\r\n",
        "\r\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\r\n",
        "    for diversity in [0.2, 0.5, 1.0, 1.2]:\r\n",
        "        print(\"...Diversity:\", diversity)\r\n",
        "\r\n",
        "        generated = \"\"\r\n",
        "        sentence = text[start_index : start_index + maxlen]\r\n",
        "        print('...Generating with seed: \"' + sentence + '\"')\r\n",
        "\r\n",
        "        for i in range(400):\r\n",
        "            x_pred = np.zeros((1, maxlen, len(chars)))\r\n",
        "            for t, char in enumerate(sentence):\r\n",
        "                x_pred[0, t, char_indices[char]] = 1.0\r\n",
        "            preds = model.predict(x_pred, verbose=0)[0]\r\n",
        "            next_index = sample(preds, diversity)\r\n",
        "            next_char = indices_char[next_index]\r\n",
        "            sentence = sentence[1:] + next_char\r\n",
        "            generated += next_char\r\n",
        "\r\n",
        "        print(\"...Generated: \", generated)\r\n",
        "        print()"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Generating text after epoch: 0\n",
            "...Diversity: 0.2\n",
            "...Generating with seed: \"싶니? 신데렐라가 고개를 들어보니 마법사 할머니가 빙그레 웃고 있었어요.\"\n",
            "...Generated:   소녀에 신데 한 신씨사라이마하밤오 화개 시생 신두렐라 밤 뱀두렸 신데렐라도마말에 신아히  춤개 할혼게 예랬마은궁허리 시았어요. 그 뛰 에게 데 다 구두 시가 알었님나 신녀사이이 무계요. 변음 새 무도님장마지지했어요. 그 뛰궁보럼빠는 아느 시개개로 한리 구두 지아요 어머니가 놀아아요 울에 한고 신 들술 남아지보음회나 구두 오 뛰 아게 남에 소음초라 주나에요지 왕마에은이바바고라주 는 놀두간 뛰했자기. 돼번 무말에나 신아를 의 딸 하틈만라생마을까 아났렐라초게 데리로 늘날 왕아안에 구두 시두 보지지들셨 오두 주고고 잠들보까 왕돌님로 신하렐라하너행 신었간요뛰궁음뛰  아났오 말렸라부너 모이이오 울행틈어요 시두루 아혼로 왕했땡데 금 않마어느돼이렸도 왕했어요. 할진 다 짝짝를!변음뛰은 주녀어요. 신데들럼 유박 지두와 유\n",
            "\n",
            "...Diversity: 0.5\n",
            "...Generating with seed: \"싶니? 신데렐라가 고개를 들어보니 마법사 할머니가 빙그레 웃고 있었어요.\"\n",
            "...Generated:   소녀에 신데 한 신씨보라이마하밤오 화개 시생 신두렐라 밤 뱀두 구두 오에 마말 시고 호두 춤에 와고 갔느회까렸변음님과 뛰두 오느하 뛰고 뛰두를 유박 보음 밤렸요가 을 그에보.지데마왕도 아름로까요. 빙혼났까어기게 보도하재박 호두를 울 들에 말두 시아니은 유들 구두 !에 오에이 말렸럼도 유박 구밤 구두 구고 지두 틈궁보가 무도하바고 신두들라의 지한하로 왕했도 소녀가 되느뛰한 너났 갔짝는 유리 구두일까 왕했라요. 그 왕혼하두 구두 홀성 말음지잠 신두는 무이땅도 신고 라. 주마하니. 소번가 어요. 그데 다 홀나 주이 시갔 어기. 그 시두 구두를 무혼로 신니계 되느 오도 쳐들 호두 댔음와궁 힘녀새 과 되궁보요고 뛰두들 어하 왕렸님지여지홀여 추여보. 뛰이계구는 추데마 아났렐럼신녀만 되금뛰바도 무여 왕아요. 그리 내두\n",
            "\n",
            "...Diversity: 1.0\n",
            "...Generating with seed: \"싶니? 신데렐라가 고개를 들어보니 마법사 할머니가 빙그레 웃고 있었어요.\"\n",
            "...Generated:   소녀에 신데 한도회울보는셨주데렐오고 힘에 생의 갔 신 가 뱀가운구 울박에 성말 시고 호두 주녀는 신았만라에라 말리 내. 왕두 오 왕음들요  말렐라요. 허두렸럼요너 마부계 왕마에지 호나 어 과에 데에자돼주데렐라도 왕은 울박를 갔 왕두머오고 그행 바말 밤개 밤 구두지구고 무도회에 어머 말  아름보. 주마뛰요. 소번가 짝 렐라. 갔고 데 왕댔어요. 추들되 호  남들 쉬 새을머게. 신니하 이이하두 오. 주 신여렐라의 호두 바웠렸까 문고고 유들 구두 과에 소혼났 할혼틈 예계 남에 구두 내웠렸라가 뛰아님은 무에하구셨간 뛰아요지 쳐리이 유들 오셨그 놀번고 뛰리 나 한리 구두 구에 지혼하 하리들 주 알라요까 주바들보 말초로 무리로 주두났 신고 라고 지에 어혼. 신데렐라는 신데들댔를짝바하게 바에 볼두하게을요는 왕너에 주 \n",
            "\n",
            "...Diversity: 1.2\n",
            "...Generating with seed: \"싶니? 신데렐라가 고개를 들어보니 마법사 할머니가 빙그레 웃고 있었어요.\"\n",
            "...Generated:   소녀에 신데 한느로 보이게 주일보요화개 아요. 맞두님 가 뱀부는 신데렐라도마을 시았 왕말하오 너났 소녀가 빙라님 왕쉬님났진 유았 왕아 과 신데 니고 변음히가 무혼하두를셨유나 신두들 호행틈요.셨신도하다모요. 온 왕았어요. 저했났 어났 오뛰보 왕날 시았계회. 알날 시두 오아요. 호두을구히말 쉬었는 이들는이회말렸 가때 변했어요. 그금 바음라자 행날렐럼.요신아히  무차오 할바 와 구두 구았어도 말렸 도번는 짝 한돌지난 만 과 왕두 볼 가 허 왕짝님도 한았간 왕리 구두  고안오고이 왕했어요. 그아을 뛰는는 아름지모틈게 호 히났 주틈을간 데 소부신오을까주행마일은 소번가 되렸 건가 신아계짝님. 소궁는 신었에  고 홀두 내에 모안바은 신데렐라의 왕아님은 왕두렐오호박에로 유리 구두 모에 다안 아느렐 신었계니. 놀 맞라에 \n",
            "\n",
            "\n",
            "Generating text after epoch: 1\n",
            "...Diversity: 0.2\n",
            "...Generating with seed: \"니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요. 신데렐라 \"\n",
            "...Generated:  밤 왕두니오 지했어요. 이리보 아름하리고 갔름왕왕금를리 볼날 왕두계오에지모가렐라의  아잠과 무랬와은 가 신혼렐 의 딸들하반보 는는 행차을  말 구두  두 돼가이아기들 반니들 왕시에 지두들셨셨요. 신데렐기생 아느 밤고 데에지갔 왕온 추차는 소녀하여을 지박 뛰궁 왕여 왕두로 신기들게 지말 구았지주. 틈지 볼두어니홀마혼났  아름로 신었을게 데고 시두 오두 주아간 왕아을과 주아는 뛰기 시여 예들 돼아 왕여 다  쳐보보허뛰박을로 말리 모차로 아너리 신금렐라. 그이이벗왕왕가여 아름구기 추 한리로 아하 다 신데렐라의 빙리에 그여 무도회진 왕틈하니들 아기하 왕 하마만 되기계 소녀뱀니놀 맞 다 지도번가 어느니진 틈개일구고 추두일모여 무 신데루라 발에 왕두루볼들도 그 구안 남에 소안가게스 아간 구고 구생 볼에로 유났 신두을라\n",
            "\n",
            "...Diversity: 0.5\n",
            "...Generating with seed: \"니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요. 신데렐라 \"\n",
            "...Generated:  밤 왕두니오 지했어요. 이리보 아름하리고 갔름왕.고를리땡 울 왕시신데 말고 데고 오 딸 를울요와 신녀되라 오 댔두로 하박을보신데렐 여틈허. 구두 구았 홀박들오 보박 반댔 성 한계셨이이로 왕혼사  왕계어요. 왕박간 뛰고 왕두 내궁  유리 구이고 주마만 신데렐라하너까 주틈만 울박을 밤 리리 모에로 왕생 저아을고 난뱀회을 왕에 구여 추랬어 주짝 유계 유리를구았 과 지에 틈기로 되느 쉬안요은 알고렐건신게 구두를 을 저아들도 신씨와 아요  말짝어요. 보박박박신데루럼는 데 시음간어요. 신었는라빼들 왕술들리 구두 구이를 무 왕름님. 신데렐라에 궁음 데 ! 왕시땡 한데 모고셨오 건드고 유와 볼기어요. 호박 구여 말 땡 신데렐럼의 허았 볼고 신부를 왕마머 저아는 는 새라머게. 건아났 아신데 시았 내두 내아는는 왕했어보. 뛰\n",
            "\n",
            "...Diversity: 1.0\n",
            "...Generating with seed: \"니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요. 신데렐라 \"\n",
            "...Generated:  밤 왕두니오 지했어요. 이리보 아름하리고 갔름왕.고를리땡 울 왕시신데 말고 데고 오 딸 새울요와 신녀되라 오 댔두로 하박을보신데렐 여틈허. 구두 구았어요. 예두 구두 왕두성모어요.이이데 소혼울  시계라가 짝두박건 데고 왕두회오이게고고 뛰여틈은 신했렐오와한계니에 왕궁보허 호박이로옷 내에로 가고 데루와 너는 신기는 왕너하오자녀날 소말가틈 너두 왕두와 여 왕여 유  반야 그 빙 왕시어요. 빙혼돌 성리 구두 짝빙를뛰여틈뛰들어 주행났어하  댔님도 맞느여 허리 쉬두지볼 고 신데렐라이게 사리 화금춤도 을셨보 혼박이이아했어요지밤 추리이 뛰아는 시에 구두 마음짝개 지짝는알 아녀어 어어 와 울마를 주말들 신마리리 오궁보지건한라이고 난혼박 이열럼어요. 남리 내들 다루보 말 빼두 은 새훌보  아름 주 와 없어 추진로 호두 구고 \n",
            "\n",
            "...Diversity: 1.2\n",
            "...Generating with seed: \"니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요. 신데렐라 \"\n",
            "...Generated:  밤 왕두니오 지했어요. 이리보 아름하리고 갔름왕.고를마땡 울 왕시신오 말고 데고 오 딸 를울일와 신녀되게 오 댔박힘아고박어요신데 반여을보시무 소녀에 보쉬훌도셨요. 신녀히라 너는 예이이로이가 식 울게 도 소녀하음들니에 지 한이 내두 과에 마아 은 신이계라 너  행개어요. 그니계마무게셨어지 아 와 왕아요계 맞박운 유니 시. 어요. 저술만 놀박들요  바볼 어느 볼아알 주틈되요. 그리들도 소궁보  유렸 오개스 초때 무날는까는는 재리는 돼음를을요. 무도 쳐궁 신데렐 도 하두자 .게신데신녀가 그 그 신데렐라도 울씨들!유께 소기을 왕두고 유들 오고고 유했 신두로 소번에 왕아요은 소녀어요계 아름계생 없여 갔 저마고 추 왕궁하마 시돌 신데렐라의 추에니. 마두 볼 한아어기주 재리 저나박  아일틈 오궁보허 왕씨계오 내두 지두박\n",
            "\n",
            "\n",
            "Generating text after epoch: 2\n",
            "...Diversity: 0.2\n",
            "...Generating with seed: \"요. 해도 해도 끝이 없는 집안일이 힘들어 지칠때면 난롯가에 앉아서 잠시\"\n",
            "...Generated:  로 어났 밤렸니두 마기하게스난신데와라이게 반드 신럼렐라의 모에로돼부이이허 되기는 갔 사라 바고 반드 왕았어요. 왕리에 시아 과두렐 지하 다 너두 빼두는 . 할에 짝두 볼에님 왕일은 가고 말궁럼가 아라은 아혼하게빙요가 되라렐 유박 구이이라는 지아짝로 니들 오 그 저아님돼 니들 어요. 왕번님은 놀름저남고 와음한부 아기이 아름럭자훌도 알개되 왕 소않에 놀 을라안아니벗벗아개고 아녀히 아기날 주녀가 주데렐 변음씨까 하니계 신데렐라도 쳐름 왕두보. 되기을 되기날 소녀와 지리에오 지했어요. 소녀에 신데 니여 아녀을 호박를빼가 추셨를 니나 그 신라렐럼 아마새시무니홀마지발에 다두 남가 다밤 소혼가 호두 시두 구두 왕두 오 남느 쉬음어요. 신데렐 의 신고 다들지없에 온부스데보럼갔 그고 주데렐라어요지 이 새 유계 구두 내에 오\n",
            "\n",
            "...Diversity: 0.5\n",
            "...Generating with seed: \"요. 해도 해도 끝이 없는 집안일이 힘들어 지칠때면 난롯가에 앉아서 잠시\"\n",
            "...Generated:  로 성금 어요. 온마저마는 난았어요. 그느 시야 그요계 온 남재로벗이는 호고 그 새날요. 바아훌라 너났 울박들번지이허 소번에  아머니계 왕리는 난너도 유리 구박짝모가 다 를이어요. 안아와게 홀안님도 돼생 소녀하게 시에 허도 말고 시이 온 소두보 주마에 신기들럼 시신 들두 구았에어. 변음렐 의 구두 댔음와 지일에  말을 지왕마않고 빠음 없루로고 왕말로 도지않은 맞고 갔이이. 유 를라도 왕들한 틈안 뱀에로 주마만 뱀두 구하박들 주니가 뱀두렸 갔 저아님보 했가 벗두짝 신었히지 말렸어요. 뛰고 오두오 성름 겨 신아보라의 돼아 과 신녀히라이. 그 한 아 새 무리로 무안뛰은고무아간금 왕마한 여 왕아간은 왕마님과 무도회가 되기 시가 신데렐볼신금니는 돼데 남차오 건났님틈돼하니여 무아아볼 유데 구두를 음 왕시신데는 그밤 주\n",
            "\n",
            "...Diversity: 1.0\n",
            "...Generating with seed: \"요. 해도 해도 끝이 없는 집안일이 힘들어 지칠때면 난롯가에 앉아서 잠시\"\n",
            "...Generated:  로 어났 밤렸니두 마녀짝애스난신데와라어요 반한리 는 바았어요. 그게이셨음하리들게 갔 딸은 바궁이이반발. 그 새라머게 틈부 짝가 쉬 무도 했어요. 신데렐라의 구았 짝두로 그 오 홀 한 생 짝개아오이 신녀들 허 잠일 과 왕았어요. 그 이이스데 재두자 아느다어기이이씨들 마에로 무날 이이로 아녀틈 일은 소번을 와초고라신데고 되두 볼고 와개하오만 무녀님니를 신었님돼마짝렐라니게 안아들는 주마어요. 빼데는럼요밤 무혼일  볼마한름보왕시가 왕혼이 어요. 신데렐라도너박 울박고 신데렐라의 볼에니는 그박 시가 허두를모어요. 왕박에로 놀 니계 유했어요. 신데렐라는 지름애도 소음와  말고 구두 오두를 말하벗고 밤 저아스 놀 왕시머오어요 아요는 신했들볼 유들 건아스자요날 그 오 너리 알이로궁이벗여님허 새 가 되기렐 왕마바는 저데 다가\n",
            "\n",
            "...Diversity: 1.2\n",
            "...Generating with seed: \"요. 해도 해도 끝이 없는 집안일이 힘들어 지칠때면 난롯가에 앉아서 잠시\"\n",
            "...Generated:  로 성금 소기에 힘마하로 난난니은 신데렐라 발야 는 무니온 남재 위이이보하고까 유나 지부스셨무 단루계밤니. 신데렐 신었어요 밤 지고고 아데와 끝옷 구도 구두 구았 볼가 더두 구에 시았 왕두차 아간렐 주너뱀 주발들보. 그 한고 갔고 왕는는 도이뛰고 지궁음 여 뛰두님돼 데리을 너리 신데렐라도 왕했뱀게. 행녀땡 호들되게 유아 답녀어기는 아름 더여 빙랬한 시았 한들를 잠 오계이쁘로 밤 바웠 저두로 주마새라부도 아 왕라의  유궁 왕 새술보이이자자무투에에 지행짝은 바고 소녀뱀 어요. 그행틈어지도 하게을 주 예를 렴차로 이리 구아이오니짝 왕니에  외에 쉬기도마뱀마 황에 그 를리이게 성술님행이와리 데 할머니는 맞두렐 호박렐요. 그혼오 주마을도 해사 시두고 생 와 는 신데렐 생밤 반들라은 웠름오. 소녀에 지리고 내가렸힘궁박\n",
            "\n",
            "\n",
            "Generating text after epoch: 3\n",
            "...Diversity: 0.2\n",
            "...Generating with seed: \" 않고신데렐라하고만 춤을 추었어요. 신데렐라는 왕자님과 춤을 추느라 시간\"\n",
            "...Generated:   구두 구두 오두 알아한  무 회에 왕았어요. 되름 아여빼에 왕두에궁 데고 갔고 뛰두을라빠 행에 다 하데렐어하 새라무말로 밤느로 지가 갔 신데렐럼의 모리로라 한럭기지지저댔 오았어요. 왕 님은 지말지 유났 힘. 왕았어요. 신데렐 의 왕가 주는이자셨 새 과두  아니  아아저아는 오 알틈히  유에 구두 내가 다두를 걱 되 왕라아요 돼가 소녀가 는 그리자빠 짝에 저아을고 뛰도 다들 뛰음보보 하두들 주마왕도 니나 온 그리 구고 오부지궁는 신댔님라 밤 오리 발에 없혼로 지말에 니했어요. 소번에  댔에는 주 에 왕이 한여 없안을 신말 댔어요  아느과 잠느 쉬 홀안를이이이지 밤 내두를 잠 딸들 주고 시두 이개를 소녀가 로이이지 유들 신두와 할났은 왕름뛰보이이들셨 지을 오음뛰은 무날니에 온 하진와 저한럭 왕라하오 이니짝 신\n",
            "\n",
            "...Diversity: 0.5\n",
            "...Generating with seed: \" 않고신데렐라하고만 춤을 추었어요. 신데렐라는 왕자님과 춤을 추느라 시간\"\n",
            "...Generated:   구두 구두 오두 알여 다 구두 오에 쳐여 열두로 아아알알 밤 지에보허데고 뛰고 추두를마변했났니. 그데 말두로라신게로라에 모돼는 갔랬 반여 내았어요. 왕마뛰보 도마이름오도 쳐났 갔 왕에님하 지니계 부. 왕 는 왕도 새 과 틈혼로 호박 와아훌니개렸훌. 모두지다혼 아름볼반아 짝고 열두  에 모아짝모어요. 그들들 고 씨 래박 신데 볼어님아아는 돼박이로이생데 지두렐 돼이 잠 오고 바혼를 어요. 그 왕라는 쟁아를라부지왕웃와하진들 신아뛰라 밤 이두 추음짝짝어보 일날 구두를 반열렸어요. 아아하 들일스오 왕리어요. 빙진뛰 아게이이 너계 유고 구두 구아 새 무리님. 이나어게 행말을 시유 모안님가 그 와두를 저아와  데고 딸 왕에 신아렐라와 마음님나 주궁보도 울계 한도 반음라보 바궁들어 신혼들셨 주두어요. 왕들에 시아 새랬머\n",
            "\n",
            "...Diversity: 1.0\n",
            "...Generating with seed: \" 않고신데렐라하고만 춤을 추었어요. 신데렐라는 왕자님과 춤을 추느라 시간\"\n",
            "...Generated:   구두 구두 오못어알여 층계라잠 과에 구두 모가 왕아아알여 므 지에보허데 한고 구데 예음 는 되두  이 아음간 뛰말 다두 돼가 그 딸 주말 밤고 맡 한 신이와럼 너나 갔 모한님로 데리로 내렸라어요. 그들들 떠났 갔 신아 볼 왕마땅도 수 힘걱 께아히 구았 구도 한에 !혼하게계럼에훌이. 왕두로 는 뛰니스 고데라답빼님아님은 신했간 약한 말 어어요. 그리 난에님모졌어요. 그름로 너계 신데렐라의 왕에 데 한  단 딸 회박 다리 오두 데 새날요도 아하렐 잠요. 놀 맞여 말 쳐  반궁 도셨머데  만 자혼뛰 신아 볼어요. 무날이까여밀벗고 밤 구두를라도 그 과 왕혼계 어요. 저리에 돼는 새날머볼들쥐와!딸 왕박 내고 갔두 내들게 아여하어느 언나계저바리 왕두 다에 생는이변들들셨 주두 시. 힘데 왕혼사 돼났 있지어요 빠는 신녀렐\n",
            "\n",
            "...Diversity: 1.2\n",
            "...Generating with seed: \" 않고신데렐라하고만 춤을 추었어요. 신데렐라는 왕자님과 춤을 추느라 시간\"\n",
            "...Generated:   구두 구두 오부어었어요 아차를 호박 구여 열두를 아아알알 밤 지에보허데고 뛰고 추두는 주차를 되게 보두 쁜고 볼고게라. 뛰금훌댔 갔 주말 밤 너리 왕두 오부를 런사 돼두 내외에에 빠는 그 오리어요. 정웠를 도랬머니계 되라날 바아보고 할요. 주마님지지없안 도 자안쟁 든요로나무. 온 홀름 말 어느주마팡 모머니. 빙리에 아요로 유들고 신데 마부하 어요 지에 다  남뒤지지데 라끝게 호두 구두 모안하하궁 주왕어요. 아혼렐 놀 한말 구고 오두 아고 다두 추웠스고 뛰박라랐.요. 을느 울 다 어요. 왕마님건 너자 울랬사 아요은 소나계너리 문 놀 놀박 한에 왕아님은 울박을 데 다 를고 했어요. 왕니계니가 빙라가 벗때고로 왕날님모고 다니 왕임한어요. 그렸 호 새녀머녀하밤들구 너렴 아요로 왕하짝라 아니 다부밤 하두를 신혼들 \n",
            "\n",
            "\n",
            "Generating text after epoch: 4\n",
            "...Diversity: 0.2\n",
            "...Generating with seed: \" 했어요. 해도 해도 끝이 없는 집안일이 힘들어 지칠때면 난롯가에 앉아서\"\n",
            "...Generated:   잠일로 반금 어요. 그마 주애스난신데와라고 모에 볼신는 오고 지녀재게로이이박  반는 새날에니. 새을이게 왕쳐들 여 신음는럼 열렸자요에 저니는 신마렐 도 한에 모말 과 왕 와럼여 예금들어요. 신했렐 아금 지었는요 왕에고 유들 구두렐 주아아요하 들 딸이이는 새날머니. 놀두되럼 발차간어요. 왕아님벗고 뛰여를을느로 무혼게 빼진 모에 모생애오고벗데 왕고 말고 울두를 가 왕댔님리 보에지모에 어느하마뛰랬 내고 볼이에  았여회에 구두 구갔보 아녀하이 라에 모아니계 신말렐라의 왕에는는 갔 과 왕기하틈고 뛰고 힘 짝 할요. 왕말렐라의 소녀에 놀 맞행틈지게 힘. 구두를 무짝댔리로 데박이이유 !일여 신데 내에 마말고 구두 짝아님은 유고 구두 볼이이로 아았어요. 그두 볼에로 왕도렐오. 신데렐라이이로 니궁 신데렐라는 와고 남느 새\n",
            "\n",
            "...Diversity: 0.5\n",
            "...Generating with seed: \" 했어요. 해도 해도 끝이 없는 집안일이 힘들어 지칠때면 난롯가에 앉아서\"\n",
            "...Generated:   잠일로 반금 어요. 그마 주애스난신데와라태발새 주마신럼스오고 지리 추개이이식 왕리는 주마에 다장를을이게 왕았어요. 왕리을 시아게자요. 신녀.라와마할가고 데리을 지아애도 한와 모고 새녀머데렐라에게 집두게 호박 성쉬 반드 어요. 신었히요 이들 볼들머 왕두 마생 은 지했을요. 소마님고 유두 구두를 유너 구두 추진는 는셨신데렐라. 그리  아마에 오았에요 너도 뛰고 모두 저부는 뛰두 구두 구두 구두 오도 쳐들 추음지자자히지 빙 내리 댔고 뛰두는 신었을라 아고고 뛰고 추두자 가렸훌어요. 그고 아두를 아게이오요. 갔 왕두 다에 댔음너리 왕두 데고 온 한리 오두 댔가로모.님되럼와렸에박. 주행뛰도 말이이 갔리 틈갔 쳐  말 구두자라갔이이리 데박 빼두 구두를 가을로 유났 와음 힘여짝까. 을 구두간 갔날 힘난와 아니. 그데간\n",
            "\n",
            "...Diversity: 1.0\n",
            "...Generating with seed: \" 했어요. 해도 해도 끝이 없는 집안일이 힘들어 지칠때면 난롯가에 앉아서\"\n",
            "...Generated:   잠일로 반열들어요. 그마 주애스난신데와라래발 반났칠신빠계라. 온녀님리 이셨 홀두는 질느 쉬도 장를을이벗상마쳐들 보았어요계마왕럭들과  아마다로 할났 심렸고고 지이계과도 겨 와두 벗돌하과 왕후회했어요. 아녀님볼훌요에 쫓이 마아와 호아거요아 고금 반진 시두박 가 되데 새날머볼가마 는고 말두을 갔셨님들밤 모가 추두는 왕리고오.려임 한자빼신 왕라생럼 데 한두 구고 추. 다고 겨고 남두라열두상라뱀일 데 한음고 시가 어느 한고 구고고 주녀꼭 소애뱀게로 고고뛰 생라들  나에계 아났궁궁음아에 지가 오고 댔뱀를모성데고 이두님 부너 볼기롯 무 볼가 구두을이신차들라 건아 자안뱀이이 두고 신녀을 뛰아땅로 는는 홀들들는 신데렐라도 왕말고 시두 모날 오고 지갔 왕두성 리고 갔 과 딸 딸계이살에 구두 다가 왕음보허옛지레 자롯너났 힘행\n",
            "\n",
            "...Diversity: 1.2\n",
            "...Generating with seed: \" 했어요. 해도 해도 끝이 없는 집안일이 힘들어 지칠때면 난롯가에 앉아서\"\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "...Generated:   잠를 쉬들해들 구두를 하마들애스난신데와라이게 반났 신음렐라의 모에로추발이이보 무혼땅셨간요. 저혼계라는 왕했어요. 그했는 왕마않은 맞렐 지하 은 할녀 볼에루 데 오두는 그 모가 잠느지쉬 면 딸에 오 니들 놀 저오께니유 개는는지얼났 자혼갔 소말렐 답이이지 바는는 려말 오 구고고운빠는이꼭 왕여님가 주마을고 아웠계 녔은 무 와아짝 쩍부에 구두 오 너리 투두와라가 딸 한말을오 너리 구두렐라니너 모꼭 오고 아말고 뛰말 구두를 신데들  밤셨어요 아스지 말고 갔여 소니머라놀밤 댔두를 뱀리로구건아라. 왕았않요 갔 새날님추게 아벽댔 힘아머니잖던 벗두이이갔 왕있못요. 그 오에 왕아님은 왕말님다 그리 구이간라돼게 왕두 므루와외웠행라세게 무도회자라지모하게 생가 다 딸도 말고 난데 왕두짝모뛰요 멋들 나요어 그 볼머데로 주데렐럼돼게\n",
            "\n",
            "\n",
            "Generating text after epoch: 5\n",
            "...Diversity: 0.2\n",
            "...Generating with seed: \"보다 나이가 위인 두 딸을 데리고 왔어요. 그러나 새어머니와 언니들은 성\"\n",
            "...Generated:   힘 가 한가 술지뛰도들한 신진 마도 울박들도 행에 볼생님들 왕아머니고 뛰데렐 가 온고이아간허고 왕두스오고 유했 벗했어요지는는 아개을 돼생 뱀리로 갔 한에 왕마님하두을 지데때 주마저잠. 잠느 반밤 돼두회에 왕리 오고 왔어요. 주마만 무고 한아요계 을셨는이이이계지유리 왕두뛰데 행고로자갔너 마일 오 을고 반리 구두를 무녀에 오 와마 아하로모지울리 추생이하루. 신데렐라. 그  말 한 갔 어기. 자혼하들고 의박 데 왕리 구고 오두 왕두니은 무도회에  아 힘개가 되요고 어요계 맞두 구았어요. 그두자갔갔행모어요. 그랬 말 되기지데씨들 나 왕리를 밤 그고 데 새날요도 뛰날렸힘생 새날요도 왕아님은 뛰아계춤. 그두를 보말렐 잠너과 왕마고 내리 구두 볼에 모에 모가로 니리 되두훌요도번뛰름 반드 이셨빙 가 오 왕아간고 뛰는는 \n",
            "\n",
            "...Diversity: 0.5\n",
            "...Generating with seed: \"보다 나이가 위인 두 딸을 데리고 왔어요. 그러나 새어머니와 언니들은 성\"\n",
            "...Generated:   힘 가 한가 술지뛰도들한 신말 마도 울박들도 아머 되게 반하 은 니고 신데렐라의 온고고 랐머니. 왕아머오. 왕마고 되기생 하게 구혼계 소녀뱀 로 놀 맞 지너너 놀생 과두을 생박에 틈혼 오운 졌에요 건가 소니에  없고 벗들  아요계벗초리 아갔 추기를 가 어요. 뛰데렐라도 온니홀두렐 주마을 왕빠 추가 층아요로 되웠 구두보 유리 구두 볼았어요. 지말 !여를지고 되틈하 아한되 되!하추므요고 뛰에 구두간 뛰는는 든두  아요 시가 왕댔님까개일볼 한궁 힘두일 주마뱀리 구고 뛰데 반부는 할안 가 되궁허 행틈이 모혼루는 유났라벗갔 소부에 신데 댔고로 왕개아오 신데렐라의 왕두렸마짝버과 어느. 저음에 힘니에 지녀엄니는이뱀하여을요. 왕녀계라에 모부뛰 이이어요지 새혼보셨 말고 소두에 주아소부와 무도회가 모고로 한말 오 신데와 되\n",
            "\n",
            "...Diversity: 1.0\n",
            "...Generating with seed: \"보다 나이가 위인 두 딸을 데리고 왔어요. 그러나 새어머니와 언니들은 성\"\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-124-2aa445275ed6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mx_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mnext_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mnext_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1627\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1629\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    860\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Axm42LLAWbV",
        "outputId": "b38e9b75-6a1d-4487-985c-fd511c7b3585",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "XXXXXX"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-c67da9f6f5b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mXXXXXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'XXXXXX' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etc64YJzqge6"
      },
      "source": [
        "okt = Okt()\r\n",
        "word_set = []\r\n",
        "\r\n",
        "for sentence in ko_sentences_dataset:\r\n",
        "    word_set += [w for (w,m) in okt.pos(sentence)]"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_3l5oeROFph"
      },
      "source": [
        "word_set = (' ' + clean_text(org_text) + ' ').split(' ')"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bc7vK4Z_sgVK"
      },
      "source": [
        "# create mapping of unique chars to integers\r\n",
        "word_to_int = dict((c, i) for i, c in enumerate(word_set))\r\n",
        "word_to_int = dict((c, i) for i, c in enumerate(word_to_int))\r\n",
        "int_to_word = dict((i, c) for i, c in enumerate(word_to_int))"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4R_SXvsLYnhb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "195c2546-058d-4185-abba-09df494bb023"
      },
      "source": [
        "n_words = len(word_set)\n",
        "n_vocab = len(word_to_int)\n",
        "print(\"Total Words: \", n_words)\n",
        "print(\"Total Vocab: \", n_vocab)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Words:  342\n",
            "Total Vocab:  277\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxeisNAkY90x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab570708-cfae-46d7-cbca-e4e0b7af91ff"
      },
      "source": [
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 10\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_words - seq_length, 1):\n",
        "\tseq_in = word_set[i:i + seq_length]\n",
        "\tseq_out = word_set[i + seq_length]\n",
        "\tdataX.append([word_to_int[word] for word in seq_in])\n",
        "\tdataY.append(word_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns:  332\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNQfHjPkZErL"
      },
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXML8aBwZIvv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "436118b7-11d1-42cf-bbbe-569e00287f1a"
      },
      "source": [
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256*4, input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_22 (LSTM)               (None, 1024)              4202496   \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 277)               283925    \n",
            "=================================================================\n",
            "Total params: 4,486,421\n",
            "Trainable params: 4,486,421\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2tNIrt8mO3_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "661b7da4-5cfa-4e38-9818-4624d4c39350"
      },
      "source": [
        "#Larger LSTM Recurrent Neural Network\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(256*2, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_33 (LSTM)               (None, 10, 512)           1052672   \n",
            "_________________________________________________________________\n",
            "dropout_28 (Dropout)         (None, 10, 512)           0         \n",
            "_________________________________________________________________\n",
            "lstm_34 (LSTM)               (None, 256)               787456    \n",
            "_________________________________________________________________\n",
            "dropout_29 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 277)               71189     \n",
            "=================================================================\n",
            "Total params: 1,911,317\n",
            "Trainable params: 1,911,317\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsgD0BbIZPWe"
      },
      "source": [
        "# define the checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okEFOijaZcse",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9be9a561-1baa-43a9-83a7-0a3f5a263a4a"
      },
      "source": [
        "model.fit(X, y, epochs=1000, batch_size=128, callbacks=callbacks_list,verbose=0)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00001: loss did not improve from 0.88210\n",
            "\n",
            "Epoch 00002: loss did not improve from 0.88210\n",
            "\n",
            "Epoch 00003: loss did not improve from 0.88210\n",
            "\n",
            "Epoch 00004: loss did not improve from 0.88210\n",
            "\n",
            "Epoch 00005: loss did not improve from 0.88210\n",
            "\n",
            "Epoch 00006: loss did not improve from 0.88210\n",
            "\n",
            "Epoch 00007: loss did not improve from 0.88210\n",
            "\n",
            "Epoch 00008: loss did not improve from 0.88210\n",
            "\n",
            "Epoch 00009: loss did not improve from 0.88210\n",
            "\n",
            "Epoch 00010: loss did not improve from 0.88210\n",
            "\n",
            "Epoch 00011: loss did not improve from 0.88210\n",
            "\n",
            "Epoch 00012: loss did not improve from 0.88210\n",
            "\n",
            "Epoch 00013: loss did not improve from 0.88210\n",
            "\n",
            "Epoch 00014: loss did not improve from 0.88210\n",
            "\n",
            "Epoch 00015: loss did not improve from 0.88210\n",
            "\n",
            "Epoch 00016: loss did not improve from 0.88210\n",
            "\n",
            "Epoch 00017: loss did not improve from 0.88210\n",
            "\n",
            "Epoch 00018: loss did not improve from 0.88210\n",
            "\n",
            "Epoch 00019: loss did not improve from 0.88210\n",
            "\n",
            "Epoch 00020: loss did not improve from 0.88210\n",
            "\n",
            "Epoch 00021: loss did not improve from 0.88210\n",
            "\n",
            "Epoch 00022: loss did not improve from 0.88210\n",
            "\n",
            "Epoch 00023: loss did not improve from 0.88210\n",
            "\n",
            "Epoch 00024: loss did not improve from 0.88210\n",
            "\n",
            "Epoch 00025: loss did not improve from 0.88210\n",
            "\n",
            "Epoch 00026: loss did not improve from 0.88210\n",
            "\n",
            "Epoch 00027: loss did not improve from 0.88210\n",
            "\n",
            "Epoch 00028: loss did not improve from 0.88210\n",
            "\n",
            "Epoch 00029: loss did not improve from 0.88210\n",
            "\n",
            "Epoch 00030: loss did not improve from 0.88210\n",
            "\n",
            "Epoch 00031: loss did not improve from 0.88210\n",
            "\n",
            "Epoch 00032: loss did not improve from 0.88210\n",
            "\n",
            "Epoch 00033: loss did not improve from 0.88210\n",
            "\n",
            "Epoch 00034: loss did not improve from 0.88210\n",
            "\n",
            "Epoch 00035: loss did not improve from 0.88210\n",
            "\n",
            "Epoch 00036: loss did not improve from 0.88210\n",
            "\n",
            "Epoch 00037: loss did not improve from 0.88210\n",
            "\n",
            "Epoch 00038: loss did not improve from 0.88210\n",
            "\n",
            "Epoch 00039: loss did not improve from 0.88210\n",
            "\n",
            "Epoch 00040: loss did not improve from 0.88210\n",
            "\n",
            "Epoch 00041: loss did not improve from 0.88210\n",
            "\n",
            "Epoch 00042: loss did not improve from 0.88210\n",
            "\n",
            "Epoch 00043: loss did not improve from 0.88210\n",
            "\n",
            "Epoch 00044: loss improved from 0.88210 to 0.84758, saving model to weights-improvement-44-0.8476.hdf5\n",
            "\n",
            "Epoch 00045: loss improved from 0.84758 to 0.82595, saving model to weights-improvement-45-0.8259.hdf5\n",
            "\n",
            "Epoch 00046: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00047: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00048: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00049: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00050: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00051: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00052: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00053: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00054: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00055: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00056: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00057: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00058: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00059: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00060: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00061: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00062: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00063: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00064: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00065: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00066: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00067: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00068: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00069: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00070: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00071: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00072: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00073: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00074: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00075: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00076: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00077: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00078: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00079: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00080: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00081: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00082: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00083: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00084: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00085: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00086: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00087: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00088: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00089: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00090: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00091: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00092: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00093: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00094: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00095: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00096: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00097: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00098: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00099: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00100: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00101: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00102: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00103: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00104: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00105: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00106: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00107: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00108: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00109: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00110: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00111: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00112: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00113: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00114: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00115: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00116: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00117: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00118: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00119: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00120: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00121: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00122: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00123: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00124: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00125: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00126: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00127: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00128: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00129: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00130: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00131: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00132: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00133: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00134: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00135: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00136: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00137: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00138: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00139: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00140: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00141: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00142: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00143: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00144: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00145: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00146: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00147: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00148: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00149: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00150: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00151: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00152: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00153: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00154: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00155: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00156: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00157: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00158: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00159: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00160: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00161: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00162: loss did not improve from 0.82595\n",
            "\n",
            "Epoch 00163: loss improved from 0.82595 to 0.80558, saving model to weights-improvement-163-0.8056.hdf5\n",
            "\n",
            "Epoch 00164: loss did not improve from 0.80558\n",
            "\n",
            "Epoch 00165: loss did not improve from 0.80558\n",
            "\n",
            "Epoch 00166: loss did not improve from 0.80558\n",
            "\n",
            "Epoch 00167: loss did not improve from 0.80558\n",
            "\n",
            "Epoch 00168: loss did not improve from 0.80558\n",
            "\n",
            "Epoch 00169: loss did not improve from 0.80558\n",
            "\n",
            "Epoch 00170: loss did not improve from 0.80558\n",
            "\n",
            "Epoch 00171: loss did not improve from 0.80558\n",
            "\n",
            "Epoch 00172: loss improved from 0.80558 to 0.79349, saving model to weights-improvement-172-0.7935.hdf5\n",
            "\n",
            "Epoch 00173: loss did not improve from 0.79349\n",
            "\n",
            "Epoch 00174: loss did not improve from 0.79349\n",
            "\n",
            "Epoch 00175: loss did not improve from 0.79349\n",
            "\n",
            "Epoch 00176: loss did not improve from 0.79349\n",
            "\n",
            "Epoch 00177: loss did not improve from 0.79349\n",
            "\n",
            "Epoch 00178: loss did not improve from 0.79349\n",
            "\n",
            "Epoch 00179: loss did not improve from 0.79349\n",
            "\n",
            "Epoch 00180: loss did not improve from 0.79349\n",
            "\n",
            "Epoch 00181: loss did not improve from 0.79349\n",
            "\n",
            "Epoch 00182: loss did not improve from 0.79349\n",
            "\n",
            "Epoch 00183: loss did not improve from 0.79349\n",
            "\n",
            "Epoch 00184: loss did not improve from 0.79349\n",
            "\n",
            "Epoch 00185: loss did not improve from 0.79349\n",
            "\n",
            "Epoch 00186: loss did not improve from 0.79349\n",
            "\n",
            "Epoch 00187: loss did not improve from 0.79349\n",
            "\n",
            "Epoch 00188: loss did not improve from 0.79349\n",
            "\n",
            "Epoch 00189: loss did not improve from 0.79349\n",
            "\n",
            "Epoch 00190: loss did not improve from 0.79349\n",
            "\n",
            "Epoch 00191: loss improved from 0.79349 to 0.78041, saving model to weights-improvement-191-0.7804.hdf5\n",
            "\n",
            "Epoch 00192: loss improved from 0.78041 to 0.75798, saving model to weights-improvement-192-0.7580.hdf5\n",
            "\n",
            "Epoch 00193: loss improved from 0.75798 to 0.73519, saving model to weights-improvement-193-0.7352.hdf5\n",
            "\n",
            "Epoch 00194: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00195: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00196: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00197: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00198: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00199: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00200: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00201: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00202: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00203: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00204: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00205: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00206: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00207: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00208: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00209: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00210: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00211: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00212: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00213: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00214: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00215: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00216: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00217: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00218: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00219: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00220: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00221: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00222: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00223: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00224: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00225: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00226: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00227: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00228: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00229: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00230: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00231: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00232: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00233: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00234: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00235: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00236: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00237: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00238: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00239: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00240: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00241: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00242: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00243: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00244: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00245: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00246: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00247: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00248: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00249: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00250: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00251: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00252: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00253: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00254: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00255: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00256: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00257: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00258: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00259: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00260: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00261: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00262: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00263: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00264: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00265: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00266: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00267: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00268: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00269: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00270: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00271: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00272: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00273: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00274: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00275: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00276: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00277: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00278: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00279: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00280: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00281: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00282: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00283: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00284: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00285: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00286: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00287: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00288: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00289: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00290: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00291: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00292: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00293: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00294: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00295: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00296: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00297: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00298: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00299: loss did not improve from 0.73519\n",
            "\n",
            "Epoch 00300: loss improved from 0.73519 to 0.72113, saving model to weights-improvement-300-0.7211.hdf5\n",
            "\n",
            "Epoch 00301: loss did not improve from 0.72113\n",
            "\n",
            "Epoch 00302: loss did not improve from 0.72113\n",
            "\n",
            "Epoch 00303: loss did not improve from 0.72113\n",
            "\n",
            "Epoch 00304: loss improved from 0.72113 to 0.71553, saving model to weights-improvement-304-0.7155.hdf5\n",
            "\n",
            "Epoch 00305: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00306: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00307: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00308: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00309: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00310: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00311: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00312: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00313: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00314: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00315: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00316: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00317: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00318: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00319: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00320: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00321: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00322: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00323: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00324: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00325: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00326: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00327: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00328: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00329: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00330: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00331: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00332: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00333: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00334: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00335: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00336: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00337: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00338: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00339: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00340: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00341: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00342: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00343: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00344: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00345: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00346: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00347: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00348: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00349: loss did not improve from 0.71553\n",
            "\n",
            "Epoch 00350: loss improved from 0.71553 to 0.70738, saving model to weights-improvement-350-0.7074.hdf5\n",
            "\n",
            "Epoch 00351: loss did not improve from 0.70738\n",
            "\n",
            "Epoch 00352: loss improved from 0.70738 to 0.70600, saving model to weights-improvement-352-0.7060.hdf5\n",
            "\n",
            "Epoch 00353: loss did not improve from 0.70600\n",
            "\n",
            "Epoch 00354: loss improved from 0.70600 to 0.68386, saving model to weights-improvement-354-0.6839.hdf5\n",
            "\n",
            "Epoch 00355: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00356: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00357: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00358: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00359: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00360: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00361: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00362: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00363: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00364: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00365: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00366: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00367: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00368: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00369: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00370: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00371: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00372: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00373: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00374: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00375: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00376: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00377: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00378: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00379: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00380: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00381: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00382: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00383: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00384: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00385: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00386: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00387: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00388: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00389: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00390: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00391: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00392: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00393: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00394: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00395: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00396: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00397: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00398: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00399: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00400: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00401: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00402: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00403: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00404: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00405: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00406: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00407: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00408: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00409: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00410: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00411: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00412: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00413: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00414: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00415: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00416: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00417: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00418: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00419: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00420: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00421: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00422: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00423: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00424: loss did not improve from 0.68386\n",
            "\n",
            "Epoch 00425: loss improved from 0.68386 to 0.67590, saving model to weights-improvement-425-0.6759.hdf5\n",
            "\n",
            "Epoch 00426: loss improved from 0.67590 to 0.66428, saving model to weights-improvement-426-0.6643.hdf5\n",
            "\n",
            "Epoch 00427: loss did not improve from 0.66428\n",
            "\n",
            "Epoch 00428: loss did not improve from 0.66428\n",
            "\n",
            "Epoch 00429: loss did not improve from 0.66428\n",
            "\n",
            "Epoch 00430: loss did not improve from 0.66428\n",
            "\n",
            "Epoch 00431: loss did not improve from 0.66428\n",
            "\n",
            "Epoch 00432: loss did not improve from 0.66428\n",
            "\n",
            "Epoch 00433: loss did not improve from 0.66428\n",
            "\n",
            "Epoch 00434: loss did not improve from 0.66428\n",
            "\n",
            "Epoch 00435: loss did not improve from 0.66428\n",
            "\n",
            "Epoch 00436: loss did not improve from 0.66428\n",
            "\n",
            "Epoch 00437: loss did not improve from 0.66428\n",
            "\n",
            "Epoch 00438: loss did not improve from 0.66428\n",
            "\n",
            "Epoch 00439: loss did not improve from 0.66428\n",
            "\n",
            "Epoch 00440: loss did not improve from 0.66428\n",
            "\n",
            "Epoch 00441: loss did not improve from 0.66428\n",
            "\n",
            "Epoch 00442: loss did not improve from 0.66428\n",
            "\n",
            "Epoch 00443: loss did not improve from 0.66428\n",
            "\n",
            "Epoch 00444: loss did not improve from 0.66428\n",
            "\n",
            "Epoch 00445: loss did not improve from 0.66428\n",
            "\n",
            "Epoch 00446: loss did not improve from 0.66428\n",
            "\n",
            "Epoch 00447: loss did not improve from 0.66428\n",
            "\n",
            "Epoch 00448: loss did not improve from 0.66428\n",
            "\n",
            "Epoch 00449: loss did not improve from 0.66428\n",
            "\n",
            "Epoch 00450: loss did not improve from 0.66428\n",
            "\n",
            "Epoch 00451: loss did not improve from 0.66428\n",
            "\n",
            "Epoch 00452: loss did not improve from 0.66428\n",
            "\n",
            "Epoch 00453: loss did not improve from 0.66428\n",
            "\n",
            "Epoch 00454: loss did not improve from 0.66428\n",
            "\n",
            "Epoch 00455: loss did not improve from 0.66428\n",
            "\n",
            "Epoch 00456: loss did not improve from 0.66428\n",
            "\n",
            "Epoch 00457: loss did not improve from 0.66428\n",
            "\n",
            "Epoch 00458: loss did not improve from 0.66428\n",
            "\n",
            "Epoch 00459: loss did not improve from 0.66428\n",
            "\n",
            "Epoch 00460: loss improved from 0.66428 to 0.63770, saving model to weights-improvement-460-0.6377.hdf5\n",
            "\n",
            "Epoch 00461: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00462: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00463: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00464: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00465: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00466: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00467: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00468: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00469: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00470: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00471: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00472: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00473: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00474: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00475: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00476: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00477: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00478: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00479: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00480: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00481: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00482: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00483: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00484: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00485: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00486: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00487: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00488: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00489: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00490: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00491: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00492: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00493: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00494: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00495: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00496: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00497: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00498: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00499: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00500: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00501: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00502: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00503: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00504: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00505: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00506: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00507: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00508: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00509: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00510: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00511: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00512: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00513: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00514: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00515: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00516: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00517: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00518: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00519: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00520: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00521: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00522: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00523: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00524: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00525: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00526: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00527: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00528: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00529: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00530: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00531: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00532: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00533: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00534: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00535: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00536: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00537: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00538: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00539: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00540: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00541: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00542: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00543: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00544: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00545: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00546: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00547: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00548: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00549: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00550: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00551: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00552: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00553: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00554: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00555: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00556: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00557: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00558: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00559: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00560: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00561: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00562: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00563: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00564: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00565: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00566: loss did not improve from 0.63770\n",
            "\n",
            "Epoch 00567: loss improved from 0.63770 to 0.61164, saving model to weights-improvement-567-0.6116.hdf5\n",
            "\n",
            "Epoch 00568: loss did not improve from 0.61164\n",
            "\n",
            "Epoch 00569: loss did not improve from 0.61164\n",
            "\n",
            "Epoch 00570: loss did not improve from 0.61164\n",
            "\n",
            "Epoch 00571: loss improved from 0.61164 to 0.59807, saving model to weights-improvement-571-0.5981.hdf5\n",
            "\n",
            "Epoch 00572: loss did not improve from 0.59807\n",
            "\n",
            "Epoch 00573: loss did not improve from 0.59807\n",
            "\n",
            "Epoch 00574: loss did not improve from 0.59807\n",
            "\n",
            "Epoch 00575: loss did not improve from 0.59807\n",
            "\n",
            "Epoch 00576: loss did not improve from 0.59807\n",
            "\n",
            "Epoch 00577: loss did not improve from 0.59807\n",
            "\n",
            "Epoch 00578: loss did not improve from 0.59807\n",
            "\n",
            "Epoch 00579: loss did not improve from 0.59807\n",
            "\n",
            "Epoch 00580: loss did not improve from 0.59807\n",
            "\n",
            "Epoch 00581: loss did not improve from 0.59807\n",
            "\n",
            "Epoch 00582: loss did not improve from 0.59807\n",
            "\n",
            "Epoch 00583: loss did not improve from 0.59807\n",
            "\n",
            "Epoch 00584: loss did not improve from 0.59807\n",
            "\n",
            "Epoch 00585: loss did not improve from 0.59807\n",
            "\n",
            "Epoch 00586: loss did not improve from 0.59807\n",
            "\n",
            "Epoch 00587: loss did not improve from 0.59807\n",
            "\n",
            "Epoch 00588: loss did not improve from 0.59807\n",
            "\n",
            "Epoch 00589: loss did not improve from 0.59807\n",
            "\n",
            "Epoch 00590: loss did not improve from 0.59807\n",
            "\n",
            "Epoch 00591: loss did not improve from 0.59807\n",
            "\n",
            "Epoch 00592: loss did not improve from 0.59807\n",
            "\n",
            "Epoch 00593: loss did not improve from 0.59807\n",
            "\n",
            "Epoch 00594: loss did not improve from 0.59807\n",
            "\n",
            "Epoch 00595: loss did not improve from 0.59807\n",
            "\n",
            "Epoch 00596: loss did not improve from 0.59807\n",
            "\n",
            "Epoch 00597: loss did not improve from 0.59807\n",
            "\n",
            "Epoch 00598: loss did not improve from 0.59807\n",
            "\n",
            "Epoch 00599: loss did not improve from 0.59807\n",
            "\n",
            "Epoch 00600: loss did not improve from 0.59807\n",
            "\n",
            "Epoch 00601: loss did not improve from 0.59807\n",
            "\n",
            "Epoch 00602: loss did not improve from 0.59807\n",
            "\n",
            "Epoch 00603: loss did not improve from 0.59807\n",
            "\n",
            "Epoch 00604: loss did not improve from 0.59807\n",
            "\n",
            "Epoch 00605: loss did not improve from 0.59807\n",
            "\n",
            "Epoch 00606: loss did not improve from 0.59807\n",
            "\n",
            "Epoch 00607: loss did not improve from 0.59807\n",
            "\n",
            "Epoch 00608: loss did not improve from 0.59807\n",
            "\n",
            "Epoch 00609: loss did not improve from 0.59807\n",
            "\n",
            "Epoch 00610: loss did not improve from 0.59807\n",
            "\n",
            "Epoch 00611: loss improved from 0.59807 to 0.58192, saving model to weights-improvement-611-0.5819.hdf5\n",
            "\n",
            "Epoch 00612: loss did not improve from 0.58192\n",
            "\n",
            "Epoch 00613: loss did not improve from 0.58192\n",
            "\n",
            "Epoch 00614: loss did not improve from 0.58192\n",
            "\n",
            "Epoch 00615: loss did not improve from 0.58192\n",
            "\n",
            "Epoch 00616: loss improved from 0.58192 to 0.58125, saving model to weights-improvement-616-0.5812.hdf5\n",
            "\n",
            "Epoch 00617: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00618: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00619: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00620: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00621: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00622: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00623: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00624: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00625: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00626: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00627: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00628: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00629: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00630: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00631: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00632: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00633: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00634: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00635: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00636: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00637: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00638: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00639: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00640: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00641: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00642: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00643: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00644: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00645: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00646: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00647: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00648: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00649: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00650: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00651: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00652: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00653: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00654: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00655: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00656: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00657: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00658: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00659: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00660: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00661: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00662: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00663: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00664: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00665: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00666: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00667: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00668: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00669: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00670: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00671: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00672: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00673: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00674: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00675: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00676: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00677: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00678: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00679: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00680: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00681: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00682: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00683: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00684: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00685: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00686: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00687: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00688: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00689: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00690: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00691: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00692: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00693: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00694: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00695: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00696: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00697: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00698: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00699: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00700: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00701: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00702: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00703: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00704: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00705: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00706: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00707: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00708: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00709: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00710: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00711: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00712: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00713: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00714: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00715: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00716: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00717: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00718: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00719: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00720: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00721: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00722: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00723: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00724: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00725: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00726: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00727: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00728: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00729: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00730: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00731: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00732: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00733: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00734: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00735: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00736: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00737: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00738: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00739: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00740: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00741: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00742: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00743: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00744: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00745: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00746: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00747: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00748: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00749: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00750: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00751: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00752: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00753: loss did not improve from 0.58125\n",
            "\n",
            "Epoch 00754: loss improved from 0.58125 to 0.56542, saving model to weights-improvement-754-0.5654.hdf5\n",
            "\n",
            "Epoch 00755: loss improved from 0.56542 to 0.56539, saving model to weights-improvement-755-0.5654.hdf5\n",
            "\n",
            "Epoch 00756: loss did not improve from 0.56539\n",
            "\n",
            "Epoch 00757: loss improved from 0.56539 to 0.55638, saving model to weights-improvement-757-0.5564.hdf5\n",
            "\n",
            "Epoch 00758: loss improved from 0.55638 to 0.54151, saving model to weights-improvement-758-0.5415.hdf5\n",
            "\n",
            "Epoch 00759: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00760: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00761: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00762: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00763: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00764: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00765: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00766: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00767: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00768: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00769: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00770: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00771: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00772: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00773: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00774: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00775: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00776: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00777: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00778: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00779: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00780: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00781: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00782: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00783: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00784: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00785: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00786: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00787: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00788: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00789: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00790: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00791: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00792: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00793: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00794: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00795: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00796: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00797: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00798: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00799: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00800: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00801: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00802: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00803: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00804: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00805: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00806: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00807: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00808: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00809: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00810: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00811: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00812: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00813: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00814: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00815: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00816: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00817: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00818: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00819: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00820: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00821: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00822: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00823: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00824: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00825: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00826: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00827: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00828: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00829: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00830: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00831: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00832: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00833: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00834: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00835: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00836: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00837: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00838: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00839: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00840: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00841: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00842: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00843: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00844: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00845: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00846: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00847: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00848: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00849: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00850: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00851: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00852: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00853: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00854: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00855: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00856: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00857: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00858: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00859: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00860: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00861: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00862: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00863: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00864: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00865: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00866: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00867: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00868: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00869: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00870: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00871: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00872: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00873: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00874: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00875: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00876: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00877: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00878: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00879: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00880: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00881: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00882: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00883: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00884: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00885: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00886: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00887: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00888: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00889: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00890: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00891: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00892: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00893: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00894: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00895: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00896: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00897: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00898: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00899: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00900: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00901: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00902: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00903: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00904: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00905: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00906: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00907: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00908: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00909: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00910: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00911: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00912: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00913: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00914: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00915: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00916: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00917: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00918: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00919: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00920: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00921: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00922: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00923: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00924: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00925: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00926: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00927: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00928: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00929: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00930: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00931: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00932: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00933: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00934: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00935: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00936: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00937: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00938: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00939: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00940: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00941: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00942: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00943: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00944: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00945: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00946: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00947: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00948: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00949: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00950: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00951: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00952: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00953: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00954: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00955: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00956: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00957: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00958: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00959: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00960: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00961: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00962: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00963: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00964: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00965: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00966: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00967: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00968: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00969: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00970: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00971: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00972: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00973: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00974: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00975: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00976: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00977: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00978: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00979: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00980: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00981: loss did not improve from 0.54151\n",
            "\n",
            "Epoch 00982: loss improved from 0.54151 to 0.53205, saving model to weights-improvement-982-0.5320.hdf5\n",
            "\n",
            "Epoch 00983: loss did not improve from 0.53205\n",
            "\n",
            "Epoch 00984: loss did not improve from 0.53205\n",
            "\n",
            "Epoch 00985: loss did not improve from 0.53205\n",
            "\n",
            "Epoch 00986: loss did not improve from 0.53205\n",
            "\n",
            "Epoch 00987: loss did not improve from 0.53205\n",
            "\n",
            "Epoch 00988: loss did not improve from 0.53205\n",
            "\n",
            "Epoch 00989: loss did not improve from 0.53205\n",
            "\n",
            "Epoch 00990: loss did not improve from 0.53205\n",
            "\n",
            "Epoch 00991: loss did not improve from 0.53205\n",
            "\n",
            "Epoch 00992: loss did not improve from 0.53205\n",
            "\n",
            "Epoch 00993: loss did not improve from 0.53205\n",
            "\n",
            "Epoch 00994: loss did not improve from 0.53205\n",
            "\n",
            "Epoch 00995: loss did not improve from 0.53205\n",
            "\n",
            "Epoch 00996: loss did not improve from 0.53205\n",
            "\n",
            "Epoch 00997: loss did not improve from 0.53205\n",
            "\n",
            "Epoch 00998: loss did not improve from 0.53205\n",
            "\n",
            "Epoch 00999: loss did not improve from 0.53205\n",
            "\n",
            "Epoch 01000: loss did not improve from 0.53205\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb81c6b4e48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqEiWjNDdt68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d3fae78-b486-4401-cafa-1f8535e41f5d"
      },
      "source": [
        "import numpy as np\n",
        "# pick a random seed\n",
        "start = np.random.randint(0, len(dataX)-1)\n",
        "print(start)\n",
        "print(np.array(dataX).shape)\n",
        "pattern = dataX[start]\n",
        "print(pattern)\n",
        "print(\"Seed:\")\n",
        "print(' '.join([int_to_word[value] for value in pattern]))"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "42\n",
            "(332,)\n",
            "[39, 40, 41, 42, 43, 44, 45, 46, 47, 34]\n",
            "Seed:\n",
            "딸을 데리고 왔어요. 그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요. 새어머니는\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOwXQ0NAk6c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a3ed27a-3d71-4dcc-a808-d6ff902c765a"
      },
      "source": [
        "import sys\n",
        "# generate characters\n",
        "for i in range(10):\n",
        "\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "    #index = sample(prediction)\n",
        "\tindex = np.argmax(prediction)\n",
        "\tresult = int_to_word[index]\n",
        "\tseq_in = [int_to_word[value] for value in pattern]\n",
        "\tsys.stdout.write(result+ ' ')\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone.\")"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "없는 없는 없는 집안일이 집안일이 집안일이 힘들어 힘들어 힘들어 지칠때면 \n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOxnBEd5OBTDJayvTN2zPLo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dolmani38/Summary/blob/master/LeakGAN-PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYKbnN3Xrc0K",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "00f50a42-d4a7-46ed-8599-7c46edede442"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2703ef94-9fa3-4191-b167-532bbd743657\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2703ef94-9fa3-4191-b167-532bbd743657\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving convert.py to convert.py\n",
            "Saving data_iter.py to data_iter.py\n",
            "Saving Discriminator.py to Discriminator.py\n",
            "Saving encode.py to encode.py\n",
            "Saving eval_bleu.py to eval_bleu.py\n",
            "Saving Generator.py to Generator.py\n",
            "Saving main.py to main.py\n",
            "Saving model_params.json to model_params.json\n",
            "Saving README.md to README.md\n",
            "Saving target_lstm.py to target_lstm.py\n",
            "Saving train.py to train.py\n",
            "Saving utils.py to utils.py\n",
            "User uploaded file \"convert.py\" with length 695 bytes\n",
            "User uploaded file \"data_iter.py\" with length 1702 bytes\n",
            "User uploaded file \"Discriminator.py\" with length 4701 bytes\n",
            "User uploaded file \"encode.py\" with length 1647 bytes\n",
            "User uploaded file \"eval_bleu.py\" with length 1782 bytes\n",
            "User uploaded file \"Generator.py\" with length 4847 bytes\n",
            "User uploaded file \"main.py\" with length 20963 bytes\n",
            "User uploaded file \"model_params.json\" with length 801 bytes\n",
            "User uploaded file \"README.md\" with length 1264 bytes\n",
            "User uploaded file \"target_lstm.py\" with length 3131 bytes\n",
            "User uploaded file \"train.py\" with length 1145 bytes\n",
            "User uploaded file \"utils.py\" with length 21785 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lD6J-Oh82SoB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsBlnczVsLP_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "e7fd4b24-843f-4b3c-b1af-8b2682916009"
      },
      "source": [
        "!mkdir data\n",
        "%cd data\n",
        "!pwd"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/data\n",
            "/content/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3muW_XNsTFk",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "c7dc4af4-e4cb-46cf-8d26-9231a27ba06e"
      },
      "source": [
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4ce3950e-beec-4a52-be87-3331671bb049\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4ce3950e-beec-4a52-be87-3331671bb049\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving chars.pkl to chars.pkl\n",
            "Saving corpus.npy to corpus.npy\n",
            "Saving eval_corpus.npy to eval_corpus.npy\n",
            "Saving gen_corpus.npy to gen_corpus.npy\n",
            "Saving test_corpus.npy to test_corpus.npy\n",
            "Saving train_corpus.npy to train_corpus.npy\n",
            "User uploaded file \"chars.pkl\" with length 67607 bytes\n",
            "User uploaded file \"corpus.npy\" with length 1803920 bytes\n",
            "User uploaded file \"eval_corpus.npy\" with length 203920 bytes\n",
            "User uploaded file \"gen_corpus.npy\" with length 1597568 bytes\n",
            "User uploaded file \"test_corpus.npy\" with length 20560 bytes\n",
            "User uploaded file \"train_corpus.npy\" with length 1600080 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEjlfBWGsv7X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "8cc8bca3-3b54-4b49-ba33-7c6d54991efa"
      },
      "source": [
        "!mkdir params\n",
        "%cd params\n",
        "!pwd"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/params\n",
            "/content/params\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gx2NvdcwtJMa",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "outputId": "09ec685d-b1f4-49fd-bd94-5c0c6262ed04"
      },
      "source": [
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7261693d-b10d-4771-ad66-5aac2020527b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7261693d-b10d-4771-ad66-5aac2020527b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving dis_data_params.json to dis_data_params.json\n",
            "Saving leak_gan_params.json to leak_gan_params.json\n",
            "Saving real_data_params.json to real_data_params.json\n",
            "Saving target_params.json to target_params.json\n",
            "Saving train_params.json to train_params.json\n",
            "User uploaded file \"dis_data_params.json\" with length 183 bytes\n",
            "User uploaded file \"leak_gan_params.json\" with length 797 bytes\n",
            "User uploaded file \"real_data_params.json\" with length 132 bytes\n",
            "User uploaded file \"target_params.json\" with length 127 bytes\n",
            "User uploaded file \"train_params.json\" with length 473 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRNdgkHAtKyN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "f7292371-eeb2-40d4-f103-4e4fea78c3db"
      },
      "source": [
        "%cd .."
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqCLiy-ntUa4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1d8b704f-2c1c-4118-fd91-e73d838c8996"
      },
      "source": [
        "import argparse\n",
        "import pickle as pkl\n",
        "import numpy as np\n",
        "import json \n",
        "import glob\n",
        "import os #for checkpoint management\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "from data_iter import real_data_loader, dis_data_loader\n",
        "from utils import recurrent_func, loss_func, get_sample, get_rewards\n",
        "from Discriminator import Discriminator\n",
        "from Generator import Generator\n",
        "from target_lstm import TargetLSTM\n",
        "\n",
        "#Arguments\n",
        "parser = argparse.ArgumentParser(description=\"LeakGAN\")\n",
        "parser.add_argument(\"--hpc\", action=\"store_true\", default=False)\n",
        "parser.add_argument(\"--data_path\", type=str, default=\"/save_files/\", metavar=\"PATH\", \n",
        "                    help=\"Data path to save files (default: /save_files/)\")\n",
        "parser.add_argument(\"--rounds\", type=int, default=150, metavar=\"N\",\n",
        "                    help=\"Rounds of adversarial training (default:150)\")\n",
        "parser.add_argument(\"--g_pretrain_steps\", type=int, default=120, metavar=\"N\",\n",
        "                    help=\"Steps of pre-training generator (defaul: 120)\")                    \n",
        "parser.add_argument(\"--d_pretrain_steps\", type=int, default=50, metavar=\"N\",\n",
        "                    help=\"Steps of pre-training discriminator (defaul: 50)\")    \n",
        "parser.add_argument(\"--g_steps\", type=int, default=1, metavar=\"N\", \n",
        "                    help=\"Steps of generator updates in one round of adversarial training (defaul: 1)\") #gen_train_num\n",
        "parser.add_argument(\"--d_steps\", type=int, default=3, metavar=\"N\",\n",
        "                    help=\"Steps of discriminator updates in one round of adversarial training (defaul: 3)\")      \n",
        "parser.add_argument(\"--gk_epochs\", type=int, default=1, metavar=\"N\",\n",
        "                    help=\"Epochs of generator updates in one step of generate update (defaul: 1)\")        \n",
        "parser.add_argument(\"--dk_epochs\", type=int, default=3, metavar=\"N\",\n",
        "                    help=\"Epochs of discriminator updates in one step of generate update (defaul: 3)\")  \n",
        "parser.add_argument(\"--update_rate\", type=float, default=0.8, metavar=\"UR\",\n",
        "                    help=\"Update rate of rollout model (defaul: 0.8)\")\n",
        "parser.add_argument(\"--n_rollout\", type=int, default=16, metavar=\"N\",\n",
        "                    help=\"Number of rollouts (defaul: 16)\") #rollout_num\n",
        "parser.add_argument(\"--vocab_size\", type=int, default=10, metavar=\"N\",\n",
        "                    help=\"Vocabulary size (defaul: 10)\")\n",
        "parser.add_argument(\"--batch_size\", type=int, default=64, metavar=\"N\",\n",
        "                    help=\"Batch size(defaul: 64)\")\n",
        "parser.add_argument(\"--n_samples\", type=int, default=6400, metavar=\"N\",\n",
        "                    help=\"Number of samples generated per time(defaul: 6400)\")\n",
        "parser.add_argument(\"--gen_lr\", type=float, default=1e-3, metavar=\"LR\",\n",
        "                    help=\"Learning Rate of generator optimizer (defaul: 1e-3)\")\n",
        "parser.add_argument(\"--dis_lr\", type=float, default=1e-3, metavar=\"LR\",\n",
        "                    help=\"Learning Rate of discriminator optimizer (defaul: 1e-3)\")\n",
        "parser.add_argument(\"--no_cuda\", action=\"store_true\", default=False,\n",
        "                    help=\"Disable CUDA training (defaul: False)\")                                                                        \n",
        "parser.add_argument(\"--seed\", type=int, default=1, metavar=\"S\",\n",
        "                    help=\"Random seed (defaul: 1)\")\n",
        "\n",
        "#Files\n",
        "POSITIVE_FILE = \"real.data\"\n",
        "NEGATIVE_FILE = \"gene.data\"\n",
        "\n",
        "# Genrator Parameters\n",
        "g_embed_dim = 32\n",
        "g_hidden_dim = 32\n",
        "g_seq_len = 20\n",
        "#   MANAGER:\n",
        "g_m_batch_size = 64\n",
        "g_m_hidden_dim = 32\n",
        "g_m_goal_out_size = 0\n",
        "#   WORKER:\n",
        "g_w_batch_size = 64\n",
        "g_w_vocab_size = 5258\n",
        "g_w_embed_dim = 32\n",
        "g_w_hidden_dim = 32\n",
        "g_w_goal_out_size = 0\n",
        "g_w_goal_size = 16\n",
        "\n",
        "g_step_size = 5\n",
        "# Discriminator Parameters\n",
        "d_seq_len = 20\n",
        "d_num_classes = 2\n",
        "d_vocab_size = 5258\n",
        "d_dis_emb_dim = 64\n",
        "d_filter_sizes = [1,2,3,4,5,6,7,8,9,10,15,20],\n",
        "d_num_filters = [100,200,200,200,200,100,100,100,100,100,160,160],\n",
        "d_start_token = 0\n",
        "d_goal_out_size = 0\n",
        "d_step_size = 5\n",
        "d_dropout_prob = 0.2\n",
        "d_l2_reg_lambda = 0.2\n",
        "\n",
        "def get_params(filePath):\n",
        "    with open(filePath, 'r') as f:\n",
        "        params = json.load(f)\n",
        "    f.close()\n",
        "    return params\n",
        "\n",
        "def get_arguments():\n",
        "    train_params = get_params(\"./params/train_params.json\")\n",
        "    leak_gan_params = get_params(\"./params/leak_gan_params.json\")\n",
        "    target_params = get_params(\"./params/target_params.json\")\n",
        "    dis_data_params = get_params(\"./params/dis_data_params.json\")\n",
        "    real_data_params = get_params(\"./params/real_data_params.json\")\n",
        "    return {\n",
        "        \"train_params\": train_params,\n",
        "        \"leak_gan_params\": leak_gan_params,\n",
        "        \"target_params\": target_params,\n",
        "        \"dis_data_params\": dis_data_params,\n",
        "        \"real_data_params\" : real_data_params\n",
        "    }\n",
        "#List of models\n",
        "def prepare_model_dict(use_cuda=False):\n",
        "    f = open(\"./params/leak_gan_params.json\")\n",
        "    params = json.load(f)\n",
        "    f.close()\n",
        "    discriminator_params = params[\"discriminator_params\"]\n",
        "    generator_params = params[\"generator_params\"]\n",
        "    worker_params = generator_params[\"worker_params\"]\n",
        "    manager_params = generator_params[\"manager_params\"]\n",
        "    discriminator_params[\"goal_out_size\"] = sum(\n",
        "        discriminator_params[\"num_filters\"]\n",
        "    )\n",
        "    worker_params[\"goal_out_size\"] = discriminator_params[\"goal_out_size\"]\n",
        "    manager_params[\"goal_out_size\"] = discriminator_params[\"goal_out_size\"]\n",
        "    discriminator = Discriminator(**discriminator_params)\n",
        "    generator = Generator(worker_params, manager_params,\n",
        "                          generator_params[\"step_size\"])\n",
        "    if use_cuda:\n",
        "        generator = generator.cuda()\n",
        "        discriminator = discriminator.cuda()\n",
        "    model_dict = {\"generator\": generator, \"discriminator\": discriminator}\n",
        "    return model_dict\n",
        "\n",
        "#List of optimizers\n",
        "def prepare_optimizer_dict(model_dict, lr_dict): #lr_dict = learning rate \n",
        "    generator = model_dict[\"generator\"]\n",
        "    discriminator = model_dict[\"discriminator\"]\n",
        "    worker = generator.worker\n",
        "    manager = generator.manager\n",
        "\n",
        "    m_lr = lr_dict[\"manager\"]\n",
        "    w_lr = lr_dict[\"worker\"]\n",
        "    d_lr = lr_dict[\"discriminator\"]\n",
        "\n",
        "    w_optimizer = optim.Adam(worker.parameters(), lr=w_lr)\n",
        "    m_optimizer = optim.Adam(manager.parameters(), lr=m_lr)\n",
        "    d_optimizer = optim.Adam(discriminator.parameters(), lr=d_lr)\n",
        "\n",
        "    return {\"worker\": w_optimizer, \"manager\": m_optimizer,\n",
        "            \"discriminator\": d_optimizer}\n",
        "\n",
        "#List of Learning rate Schedulers\n",
        "def prepare_scheduler_dict(optmizer_dict, step_size=200, gamma=0.99):\n",
        "    w_optimizer = optmizer_dict[\"worker\"]\n",
        "    m_optimizer = optmizer_dict[\"manager\"]\n",
        "    d_optimizer = optmizer_dict[\"discriminator\"]\n",
        "\n",
        "    w_scheduler = optim.lr_scheduler.StepLR(w_optimizer, step_size=step_size,\n",
        "                                            gamma=gamma)\n",
        "    m_scheduler = optim.lr_scheduler.StepLR(m_optimizer, step_size=step_size,\n",
        "                                            gamma=gamma)\n",
        "    d_scheduler = optim.lr_scheduler.StepLR(d_optimizer, step_size=step_size,\n",
        "                                            gamma=gamma)\n",
        "    return {\"worker\": w_scheduler, \"manager\": m_scheduler,\n",
        "            \"discriminator\": d_scheduler}\n",
        "\n",
        "#Pretraining the Generator\n",
        "def pretrain_generator(model_dict, optimizer_dict, scheduler_dict, dataloader, vocab_size, max_norm=5.0, use_cuda=False, epoch=1, tot_epochs=100):\n",
        "    #get the models of generator\n",
        "    generator = model_dict[\"generator\"]\n",
        "    worker = generator.worker\n",
        "    manager = generator.manager\n",
        "    #get the optimizers\n",
        "    m_optimizer = optimizer_dict[\"manager\"]\n",
        "    w_optimizer = optimizer_dict[\"worker\"]\n",
        "    \n",
        "    m_optimizer.zero_grad()\n",
        "    w_optimizer.zero_grad()\n",
        "\n",
        "    m_lr_scheduler = scheduler_dict[\"manager\"]\n",
        "    w_lr_scheduler = scheduler_dict[\"worker\"]\n",
        "    \"\"\"\n",
        "     Perform pretrain step for real data\n",
        "    \"\"\"\n",
        "    \n",
        "    for i, sample in enumerate(dataloader):\n",
        "        #print(\"DataLoader: {}\".format(dataloader))\n",
        "        m_lr_scheduler.step()\n",
        "        w_lr_scheduler.step()\n",
        "\n",
        "        sample = Variable(sample)\n",
        "        if use_cuda:\n",
        "            sample = sample.cuda(async=True)\n",
        "        \n",
        "        # Calculate pretrain loss\n",
        "        if (sample.size() == torch.zeros([64, 20]).size()): #sometimes smaller than 64 (16) is passed, so this if statement disables it\n",
        "            #print(\"Sample size: {}\".format(sample.size()))\n",
        "            pre_rets = recurrent_func(\"pre\")(model_dict, sample, use_cuda)\n",
        "            real_goal = pre_rets[\"real_goal\"]\n",
        "            prediction = pre_rets[\"prediction\"]\n",
        "            delta_feature = pre_rets[\"delta_feature\"]\n",
        "\n",
        "            m_loss = loss_func(\"pre_manager\")(real_goal, delta_feature)\n",
        "            torch.autograd.grad(m_loss, manager.parameters())\n",
        "            clip_grad_norm_(manager.parameters(), max_norm=max_norm)\n",
        "            m_optimizer.step()\n",
        "            m_optimizer.zero_grad()\n",
        "            \n",
        "            w_loss = loss_func(\"pre_worker\")(sample, prediction, vocab_size, use_cuda)\n",
        "            torch.autograd.grad(w_loss, worker.parameters())\n",
        "            clip_grad_norm_(worker.parameters(), max_norm=max_norm)\n",
        "            w_optimizer.step()\n",
        "            w_optimizer.zero_grad()\n",
        "            if i == 63:\n",
        "                print(\"Pre-Manager Loss: {:.5f}, Pre-Worker Loss: {:.5f}\\n\".format(m_loss, w_loss))\n",
        "    \"\"\"\n",
        "    Update model_dict, optimizer_dict, and scheduler_dict\n",
        "    \"\"\"\n",
        "\n",
        "    generator.woroker = worker\n",
        "    generator.manager = manager\n",
        "    model_dict[\"generator\"] = generator\n",
        "\n",
        "    optimizer_dict[\"manager\"] = m_optimizer\n",
        "    optimizer_dict[\"worker\"] = w_optimizer\n",
        "\n",
        "    scheduler_dict[\"manager\"] = m_lr_scheduler\n",
        "    scheduler_dict[\"worker\"] = w_lr_scheduler\n",
        "\n",
        "    return model_dict, optimizer_dict, scheduler_dict\n",
        "\n",
        "def generate_samples(model_dict, negative_file, batch_size,\n",
        "                     use_cuda=False, temperature=1.0):\n",
        "    neg_data = []\n",
        "    for _ in range(batch_size):\n",
        "        sample = get_sample(model_dict, use_cuda, temperature)\n",
        "        sample = sample.cpu()\n",
        "        neg_data.append(sample.data.numpy())\n",
        "    neg_data = np.concatenate(neg_data, axis=0)\n",
        "    np.save(negative_file, neg_data)\n",
        "\n",
        "def pretrain_discriminator(model_dict, optimizer_dict, scheduler_dict,\n",
        "                           dis_dataloader_params, vocab_size, positive_file,\n",
        "                           negative_file, batch_size, epochs, use_cuda=False, temperature=1.0):\n",
        "    discriminator = model_dict[\"discriminator\"]\n",
        "\n",
        "    d_optimizer = optimizer_dict[\"discriminator\"]\n",
        "    d_lr_scheduler = scheduler_dict[\"discriminator\"]\n",
        "\n",
        "    generate_samples(model_dict, negative_file, batch_size, use_cuda, temperature)\n",
        "    dis_dataloader_params[\"positive_filepath\"] = positive_file\n",
        "    dis_dataloader_params[\"negative_filepath\"] = negative_file\n",
        "    #print(dis_dataloader_params)\n",
        "    dataloader = dis_data_loader(**dis_dataloader_params) #this is where data iterator is used\n",
        "\n",
        "    cross_entropy = nn.CrossEntropyLoss() #this one is similar to NLL (negative log likelihood)\n",
        "    if use_cuda:\n",
        "        cross_entropy = cross_entropy.cuda()\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        for i, sample in enumerate(dataloader):\n",
        "            d_optimizer.zero_grad()\n",
        "            data, label = sample[\"data\"], sample[\"label\"] #initialize sample variables\n",
        "            data = Variable(data)\n",
        "            label = Variable(label)\n",
        "            if use_cuda:\n",
        "                data = data.cuda()\n",
        "                label = label.cuda()\n",
        "            outs = discriminator(data)\n",
        "            loss = cross_entropy(outs[\"score\"], label.view(-1)) + discriminator.l2_loss()\n",
        "            d_lr_scheduler.step()\n",
        "            loss.backward()\n",
        "            d_optimizer.step()\n",
        "            if i == 63:\n",
        "                print(\"Pre-Discriminator loss: {:.5f}\".format(loss))\n",
        "    \n",
        "    model_dict[\"discriminator\"] = discriminator\n",
        "    optimizer_dict[\"discriminator\"] = d_optimizer\n",
        "    scheduler_dict[\"discriminator\"] = d_lr_scheduler\n",
        "    return model_dict, optimizer_dict, scheduler_dict\n",
        "\n",
        "#Adversarial training \n",
        "def adversarial_train(model_dict, optimizer_dict, scheduler_dict, dis_dataloader_params,\n",
        "                      vocab_size, pos_file, neg_file, batch_size, gen_train_num=1,\n",
        "                      dis_train_epoch=5, dis_train_num=3, max_norm=5.0,\n",
        "                      rollout_num=4, use_cuda=False, temperature=1.0, epoch=1, tot_epoch=100):\n",
        "    \"\"\"\n",
        "        Get all the models, optimizer and schedulers\n",
        "    \"\"\"                     \n",
        "    generator = model_dict[\"generator\"]\n",
        "    discriminator = model_dict [\"discriminator\"]\n",
        "    worker = generator.worker\n",
        "    manager = generator.manager\n",
        "\n",
        "    m_optimizer = optimizer_dict[\"manager\"]\n",
        "    w_optimizer = optimizer_dict[\"worker\"]\n",
        "    d_optimizer = optimizer_dict[\"discriminator\"]\n",
        "\n",
        "    #Why zero grad only m and w?\n",
        "    m_optimizer.zero_grad()\n",
        "    w_optimizer.zero_grad()\n",
        "\n",
        "    m_lr_scheduler = scheduler_dict[\"manager\"]\n",
        "    w_lr_scheduler = scheduler_dict[\"worker\"]\n",
        "    d_lr_scheduler = scheduler_dict[\"discriminator\"]\n",
        "\n",
        "    #Adversarial training for generator\n",
        "    for _ in range(gen_train_num):\n",
        "        m_lr_scheduler.step()\n",
        "        w_lr_scheduler.step()\n",
        "\n",
        "        m_optimizer.zero_grad()\n",
        "        w_optimizer.zero_grad()\n",
        "\n",
        "        #get all the return values\n",
        "        adv_rets = recurrent_func(\"adv\")(model_dict, use_cuda)\n",
        "        real_goal = adv_rets[\"real_goal\"]\n",
        "        all_goal = adv_rets[\"all_goal\"]\n",
        "        prediction = adv_rets[\"prediction\"]\n",
        "        delta_feature = adv_rets[\"delta_feature\"]\n",
        "        delta_feature_for_worker = adv_rets[\"delta_feature_for_worker\"]\n",
        "        gen_token = adv_rets[\"gen_token\"]\n",
        "\n",
        "        rewards = get_rewards(model_dict, gen_token, rollout_num, use_cuda)\n",
        "        m_loss = loss_func(\"adv_manager\")(rewards, real_goal, delta_feature)\n",
        "        w_loss = loss_func(\"adv_worker\")(all_goal, delta_feature_for_worker, gen_token, prediction, vocab_size, use_cuda)\n",
        "\n",
        "        torch.autograd.grad(m_loss, manager.parameters()) #based on loss improve the parameters\n",
        "        torch.autograd.grad(w_loss, worker.parameters())\n",
        "        clip_grad_norm_(manager.parameters(), max_norm)\n",
        "        clip_grad_norm_(worker.parameters(), max_norm)\n",
        "        m_optimizer.step()\n",
        "        w_optimizer.step()\n",
        "        print(\"Adv-Manager loss: {:.5f} Adv-Worker loss: {:.5f}\".format(m_loss, w_loss))\n",
        "    \n",
        "    del adv_rets\n",
        "    del real_goal\n",
        "    del all_goal\n",
        "    del prediction\n",
        "    del delta_feature\n",
        "    del delta_feature_for_worker\n",
        "    del gen_token\n",
        "    del rewards\n",
        "\n",
        "    #Adversarial training for discriminator\n",
        "    for n in range(dis_train_epoch):\n",
        "        generate_samples(model_dict, neg_file, batch_size, use_cuda, temperature)\n",
        "        dis_dataloader_params[\"positive_filepath\"] = pos_file\n",
        "        dis_dataloader_params[\"negative_filepath\"] = neg_file\n",
        "        dataloader = dis_data_loader(**dis_dataloader_params)\n",
        "\n",
        "        cross_entropy = nn.CrossEntropyLoss()\n",
        "        if use_cuda:\n",
        "            cross_entropy = cross_entropy.cuda()\n",
        "        \"\"\"\n",
        "        for d-steps do\n",
        "            Use current G, m,w to generate negative examples and combine with given positive examples S \n",
        "            Train discriminator D for k epochs by Eq. (2)\n",
        "        end for\n",
        "        \"\"\"\n",
        "        for _ in range(dis_train_num): \n",
        "            for i, sample in enumerate(dataloader):\n",
        "                data, label = sample[\"data\"], sample[\"label\"]\n",
        "                data = Variable(data)\n",
        "                label = Variable(label)\n",
        "                if use_cuda:\n",
        "                    data = data.cuda(async=True)\n",
        "                    label = label.cuda(async=True)\n",
        "                outs = discriminator(data)\n",
        "                loss = cross_entropy(outs[\"score\"], label.view(-1)) + discriminator.l2_loss()\n",
        "                d_optimizer.zero_grad()\n",
        "                d_lr_scheduler.step()\n",
        "                loss.backward()\n",
        "                d_optimizer.step()\n",
        "        print(\"{}/{} Adv-Discriminator Loss: {:.5f}\".format(n, range(dis_train_epoch),loss))\n",
        "    #Save all changes\n",
        "    model_dict[\"discriminator\"] = discriminator\n",
        "    generator.worker = worker\n",
        "    generator.manager = manager\n",
        "    model_dict[\"generator\"] = generator\n",
        "\n",
        "    optimizer_dict[\"manager\"] = m_optimizer\n",
        "    optimizer_dict[\"worker\"] = w_optimizer\n",
        "    optimizer_dict[\"discriminator\"] = d_optimizer\n",
        "\n",
        "    scheduler_dict[\"manager\"] = m_lr_scheduler\n",
        "    scheduler_dict[\"worker\"] = w_lr_scheduler\n",
        "    scheduler_dict[\"disciminator\"] = d_lr_scheduler\n",
        "\n",
        "    return model_dict, optimizer_dict, scheduler_dict\n",
        "\n",
        "\n",
        "def save_checkpoint(model_dict, optimizer_dict, scheduler_dict, ckpt_num, replace=False):\n",
        "    file_name = \"checkpoint\" + str(ckpt_num) + \".pth.tar\"\n",
        "    torch.save({\"model_dict\": model_dict, \"optimizer_dict\": optimizer_dict, \"scheduler_dict\": scheduler_dict, \"ckpt_num\": ckpt_num}, file_name)\n",
        "    if replace:\n",
        "        ckpts = glob.glob(\"checkpoint*\")\n",
        "        ckpt_nums = [int(x.split('.')[0][10:]) for x in ckpts]\n",
        "        oldest_ckpt = \"checkpoint\" + str(min(ckpt_nums)) + \".pth.tar\"\n",
        "        os.remove(oldest_ckpt)\n",
        "\n",
        "def restore_checkpoint(ckpt_path):\n",
        "    checkpoint = torch.load(ckpt_path)\n",
        "    return checkpoint\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Get all parameters\n",
        "    \"\"\"\n",
        "    param_dict = get_arguments()\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    #Random seed\n",
        "    torch.manual_seed(param_dict[\"train_params\"][\"seed\"])\n",
        "    #Pretrain step\n",
        "    checkpoint_path = param_dict[\"train_params\"][\"checkpoint_path\"]\n",
        "    if checkpoint_path is not None:\n",
        "        checkpoint = restore_checkpoint(checkpoint_path)\n",
        "        model_dict = checkpoint[\"model_dict\"]\n",
        "        optimizer_dict = checkpoint[\"optimizer_dict\"]\n",
        "        scheduler_dict = checkpoint[\"scheduler_dict\"]\n",
        "        ckpt_num = checkpoint[\"ckpt_num\"]\n",
        "    else:\n",
        "        model_dict = prepare_model_dict(use_cuda)\n",
        "        lr_dict = param_dict[\"train_params\"][\"lr_dict\"]\n",
        "        optimizer_dict = prepare_optimizer_dict(model_dict, lr_dict)\n",
        "        gamma = param_dict[\"train_params\"][\"decay_rate\"]\n",
        "        step_size = param_dict[\"train_params\"][\"decay_step_size\"]\n",
        "        scheduler_dict = prepare_scheduler_dict(optimizer_dict, gamma=gamma, step_size=step_size)\n",
        "    #Pretrain discriminator\n",
        "    print (\"#########################################################################\")\n",
        "    print (\"Start Pretraining Discriminator...\")\n",
        "    with open(\"./params/dis_data_params.json\", 'r') as f:\n",
        "        dis_data_params = json.load(f)\n",
        "    if use_cuda:\n",
        "        dis_data_params[\"pin_memory\"] = True\n",
        "    f.close()\n",
        "    pos_file = dis_data_params[\"positive_filepath\"]\n",
        "    neg_file = dis_data_params[\"negative_filepath\"]\n",
        "    batch_size = param_dict[\"train_params\"][\"generated_num\"]\n",
        "    vocab_size = param_dict[\"leak_gan_params\"][\"discriminator_params\"][\"vocab_size\"]\n",
        "    for i in range(param_dict[\"train_params\"][\"pre_dis_epoch_num\"]):\n",
        "        print(\"Epoch: {}/{}  Pre-Discriminator\".format(i, param_dict[\"train_params\"][\"pre_dis_epoch_num\"]))\n",
        "        model_dict, optimizer_dict, scheduler_dict = pretrain_discriminator(model_dict, optimizer_dict, scheduler_dict, dis_data_params, vocab_size=vocab_size, positive_file=pos_file, negative_file=neg_file, batch_size=batch_size, epochs=1, use_cuda=use_cuda)\n",
        "    ckpt_num = 0\n",
        "    save_checkpoint(model_dict, optimizer_dict, scheduler_dict, ckpt_num)\n",
        "\n",
        "    #Pretrain generator \n",
        "    print (\"#########################################################################\")\n",
        "    print (\"Start Pretraining Generator...\")\n",
        "    real_data_params = param_dict[\"real_data_params\"]\n",
        "    if use_cuda:\n",
        "        real_data_params[\"pin_memory\"] = True\n",
        "    r_dataloader = real_data_loader(**real_data_params)\n",
        "    for epoch in range(param_dict[\"train_params\"][\"pre_gen_epoch_num\"]):\n",
        "        print(\"Epoch: {}/{}  Pre-Generator\".format(epoch, param_dict[\"train_params\"][\"pre_gen_epoch_num\"]))\n",
        "        model_dict, optimizer_dict, scheduler_dict = pretrain_generator(model_dict, optimizer_dict, scheduler_dict, r_dataloader, vocab_size=vocab_size, use_cuda=use_cuda, epoch=epoch, tot_epochs=range(param_dict[\"train_params\"][\"pre_gen_epoch_num\"]))\n",
        "    #Finish pretrain and save the checkpoint\n",
        "    save_checkpoint(model_dict, optimizer_dict, scheduler_dict, ckpt_num)\n",
        "    \n",
        "    \n",
        "    ckpt_num = 1\n",
        "    #Adversarial train of D and G\n",
        "    print (\"#########################################################################\")\n",
        "    print (\"Start Adversarial Training...\")\n",
        "    vocab_size = param_dict[\"leak_gan_params\"][\"discriminator_params\"][\"vocab_size\"]\n",
        "    save_num = param_dict[\"train_params\"][\"save_num\"] #save checkpoint after this number of repetitions\n",
        "    replace_num = param_dict[\"train_params\"][\"replace_num\"]\n",
        "\n",
        "    for epoch in range(param_dict[\"train_params\"][\"total_epoch\"]):\n",
        "        print(\"Epoch: {}/{}  Adv\".format(epoch, param_dict[\"train_params\"][\"total_epoch\"]))\n",
        "        model_dict, optimizer_dict, scheduler_dict = adversarial_train(model_dict, optimizer_dict, scheduler_dict, dis_data_params, vocab_size=vocab_size, pos_file=pos_file, neg_file=neg_file, batch_size=batch_size, use_cuda=use_cuda, epoch=epoch, tot_epoch=param_dict[\"train_params\"][\"total_epoch\"])\n",
        "        if (epoch + 1) % save_num == 0:\n",
        "            ckpt_num += 1\n",
        "            if ckpt_num % replace_num == 0:\n",
        "                save_checkpoint(model_dict, optimizer_dict, scheduler_dict, ckpt_num, replace=True)\n",
        "            else:\n",
        "               save_checkpoint(model_dict, optimizer_dict, scheduler_dict, ckpt_num)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#########################################################################\n",
            "Start Pretraining Discriminator...\n",
            "Epoch: 0/50  Pre-Discriminator\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/utils.py:21: UserWarning: This overload of cuda is deprecated:\n",
            "\tcuda(torch.device device, bool async, *, torch.memory_format memory_format)\n",
            "Consider using one of the following signatures instead:\n",
            "\tcuda(torch.device device, bool non_blocking, *, torch.memory_format memory_format) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  var = var.cuda(async=True)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Pre-Discriminator loss: 0.85025\n",
            "Epoch: 1/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.56591\n",
            "Epoch: 2/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.58893\n",
            "Epoch: 3/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.26287\n",
            "Epoch: 4/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.31387\n",
            "Epoch: 5/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.22293\n",
            "Epoch: 6/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.17401\n",
            "Epoch: 7/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.12462\n",
            "Epoch: 8/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.14913\n",
            "Epoch: 9/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.10951\n",
            "Epoch: 10/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.10921\n",
            "Epoch: 11/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.09032\n",
            "Epoch: 12/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.04734\n",
            "Epoch: 13/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.04879\n",
            "Epoch: 14/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.05333\n",
            "Epoch: 15/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.03289\n",
            "Epoch: 16/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.02976\n",
            "Epoch: 17/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.02469\n",
            "Epoch: 18/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.02354\n",
            "Epoch: 19/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.02097\n",
            "Epoch: 20/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.01644\n",
            "Epoch: 21/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.01947\n",
            "Epoch: 22/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.01736\n",
            "Epoch: 23/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.02175\n",
            "Epoch: 24/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.01854\n",
            "Epoch: 25/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.01945\n",
            "Epoch: 26/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.01556\n",
            "Epoch: 27/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.01292\n",
            "Epoch: 28/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.02574\n",
            "Epoch: 29/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.00983\n",
            "Epoch: 30/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.01554\n",
            "Epoch: 31/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.01154\n",
            "Epoch: 32/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.02207\n",
            "Epoch: 33/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.00973\n",
            "Epoch: 34/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.00855\n",
            "Epoch: 35/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.00871\n",
            "Epoch: 36/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.00780\n",
            "Epoch: 37/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.00823\n",
            "Epoch: 38/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.01104\n",
            "Epoch: 39/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.00694\n",
            "Epoch: 40/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.00787\n",
            "Epoch: 41/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.00921\n",
            "Epoch: 42/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.00638\n",
            "Epoch: 43/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.00759\n",
            "Epoch: 44/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.00530\n",
            "Epoch: 45/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.00555\n",
            "Epoch: 46/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.00675\n",
            "Epoch: 47/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.00582\n",
            "Epoch: 48/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.00604\n",
            "Epoch: 49/50  Pre-Discriminator\n",
            "Pre-Discriminator loss: 0.00498\n",
            "#########################################################################\n",
            "Start Pretraining Generator...\n",
            "Epoch: 0/80  Pre-Generator\n",
            "Pre-Manager Loss: -0.99994, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 1/80  Pre-Generator\n",
            "Pre-Manager Loss: -0.99935, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 2/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00015, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 3/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00026, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 4/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00011, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 5/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00170, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 6/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00110, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 7/80  Pre-Generator\n",
            "Pre-Manager Loss: -0.99996, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 8/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00143, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 9/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00091, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 10/80  Pre-Generator\n",
            "Pre-Manager Loss: -0.99999, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 11/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00189, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 12/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00090, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 13/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00066, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 14/80  Pre-Generator\n",
            "Pre-Manager Loss: -0.99938, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 15/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00176, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 16/80  Pre-Generator\n",
            "Pre-Manager Loss: -0.99938, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 17/80  Pre-Generator\n",
            "Pre-Manager Loss: -0.99937, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 18/80  Pre-Generator\n",
            "Pre-Manager Loss: -0.99949, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 19/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00273, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 20/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00032, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 21/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00228, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 22/80  Pre-Generator\n",
            "Pre-Manager Loss: -0.99920, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 23/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00016, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 24/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00084, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 25/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00101, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 26/80  Pre-Generator\n",
            "Pre-Manager Loss: -0.99985, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 27/80  Pre-Generator\n",
            "Pre-Manager Loss: -0.99925, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 28/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00065, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 29/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00119, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 30/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00021, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 31/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00006, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 32/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00066, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 33/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00060, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 34/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00179, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 35/80  Pre-Generator\n",
            "Pre-Manager Loss: -0.99909, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 36/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00060, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 37/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00097, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 38/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00133, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 39/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00175, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 40/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00111, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 41/80  Pre-Generator\n",
            "Pre-Manager Loss: -0.99883, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 42/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00017, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 43/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00112, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 44/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00204, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 45/80  Pre-Generator\n",
            "Pre-Manager Loss: -0.99904, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 46/80  Pre-Generator\n",
            "Pre-Manager Loss: -0.99986, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 47/80  Pre-Generator\n",
            "Pre-Manager Loss: -0.99929, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 48/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00240, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 49/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00152, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 50/80  Pre-Generator\n",
            "Pre-Manager Loss: -0.99936, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 51/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00156, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 52/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00233, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 53/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00252, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 54/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00001, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 55/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00074, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 56/80  Pre-Generator\n",
            "Pre-Manager Loss: -0.99864, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 57/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00009, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 58/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00053, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 59/80  Pre-Generator\n",
            "Pre-Manager Loss: -0.99983, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 60/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00234, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 61/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00037, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 62/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00184, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 63/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00126, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 64/80  Pre-Generator\n",
            "Pre-Manager Loss: -0.99932, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 65/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00004, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 66/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00123, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 67/80  Pre-Generator\n",
            "Pre-Manager Loss: -0.99954, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 68/80  Pre-Generator\n",
            "Pre-Manager Loss: -0.99966, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 69/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00078, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 70/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00118, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 71/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00004, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 72/80  Pre-Generator\n",
            "Pre-Manager Loss: -0.99938, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 73/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00140, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 74/80  Pre-Generator\n",
            "Pre-Manager Loss: -0.99868, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 75/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00075, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 76/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00153, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 77/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00019, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 78/80  Pre-Generator\n",
            "Pre-Manager Loss: -0.99764, Pre-Worker Loss: 0.00163\n",
            "\n",
            "Epoch: 79/80  Pre-Generator\n",
            "Pre-Manager Loss: -1.00004, Pre-Worker Loss: 0.00163\n",
            "\n",
            "#########################################################################\n",
            "Start Adversarial Training...\n",
            "Epoch: 0/800  Adv\n",
            "Adv-Manager loss: -0.12287 Adv-Worker loss: 8.55549\n",
            "0/range(0, 5) Adv-Discriminator Loss: 0.00542\n",
            "1/range(0, 5) Adv-Discriminator Loss: 0.00432\n",
            "2/range(0, 5) Adv-Discriminator Loss: 0.00372\n",
            "3/range(0, 5) Adv-Discriminator Loss: 0.00303\n",
            "4/range(0, 5) Adv-Discriminator Loss: 0.00640\n",
            "Epoch: 1/800  Adv\n",
            "Adv-Manager loss: -0.12286 Adv-Worker loss: 8.56013\n",
            "0/range(0, 5) Adv-Discriminator Loss: 0.00254\n",
            "1/range(0, 5) Adv-Discriminator Loss: 0.00244\n",
            "2/range(0, 5) Adv-Discriminator Loss: 0.00252\n",
            "3/range(0, 5) Adv-Discriminator Loss: 0.00250\n",
            "4/range(0, 5) Adv-Discriminator Loss: 0.00304\n",
            "Epoch: 2/800  Adv\n",
            "Adv-Manager loss: -0.12306 Adv-Worker loss: 8.56293\n",
            "0/range(0, 5) Adv-Discriminator Loss: 0.00244\n",
            "1/range(0, 5) Adv-Discriminator Loss: 0.00247\n",
            "2/range(0, 5) Adv-Discriminator Loss: 0.00170\n",
            "3/range(0, 5) Adv-Discriminator Loss: 0.00325\n",
            "4/range(0, 5) Adv-Discriminator Loss: 0.00181\n",
            "Epoch: 3/800  Adv\n",
            "Adv-Manager loss: -0.12296 Adv-Worker loss: 8.55973\n",
            "0/range(0, 5) Adv-Discriminator Loss: 0.00175\n",
            "1/range(0, 5) Adv-Discriminator Loss: 0.00146\n",
            "2/range(0, 5) Adv-Discriminator Loss: 0.00199\n",
            "3/range(0, 5) Adv-Discriminator Loss: 0.00140\n",
            "4/range(0, 5) Adv-Discriminator Loss: 0.00145\n",
            "Epoch: 4/800  Adv\n",
            "Adv-Manager loss: -0.12269 Adv-Worker loss: 8.55631\n",
            "0/range(0, 5) Adv-Discriminator Loss: 0.00137\n",
            "1/range(0, 5) Adv-Discriminator Loss: 0.00164\n",
            "2/range(0, 5) Adv-Discriminator Loss: 0.00167\n",
            "3/range(0, 5) Adv-Discriminator Loss: 0.00148\n",
            "4/range(0, 5) Adv-Discriminator Loss: 0.00121\n",
            "Epoch: 5/800  Adv\n",
            "Adv-Manager loss: -0.12293 Adv-Worker loss: 8.55254\n",
            "0/range(0, 5) Adv-Discriminator Loss: 0.00144\n",
            "1/range(0, 5) Adv-Discriminator Loss: 0.00145\n",
            "2/range(0, 5) Adv-Discriminator Loss: 0.00124\n",
            "3/range(0, 5) Adv-Discriminator Loss: 0.00143\n",
            "4/range(0, 5) Adv-Discriminator Loss: 0.00179\n",
            "Epoch: 6/800  Adv\n",
            "Adv-Manager loss: -0.12288 Adv-Worker loss: 8.55777\n",
            "0/range(0, 5) Adv-Discriminator Loss: 0.00135\n",
            "1/range(0, 5) Adv-Discriminator Loss: 0.00133\n",
            "2/range(0, 5) Adv-Discriminator Loss: 0.00129\n",
            "3/range(0, 5) Adv-Discriminator Loss: 0.00119\n",
            "4/range(0, 5) Adv-Discriminator Loss: 0.00276\n",
            "Epoch: 7/800  Adv\n",
            "Adv-Manager loss: -0.12299 Adv-Worker loss: 8.55865\n",
            "0/range(0, 5) Adv-Discriminator Loss: 0.00124\n",
            "1/range(0, 5) Adv-Discriminator Loss: 0.00115\n",
            "2/range(0, 5) Adv-Discriminator Loss: 0.00136\n",
            "3/range(0, 5) Adv-Discriminator Loss: 0.00111\n",
            "4/range(0, 5) Adv-Discriminator Loss: 0.00153\n",
            "Epoch: 8/800  Adv\n",
            "Adv-Manager loss: -0.12303 Adv-Worker loss: 8.55342\n",
            "0/range(0, 5) Adv-Discriminator Loss: 0.00101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUARVCEO3DFa",
        "colab_type": "text"
      },
      "source": [
        "https://github.com/nurpeiis/LeakGAN-PyTorch"
      ]
    }
  ]
}
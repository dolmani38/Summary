{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "real_summary4.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dolmani38/Summary/blob/master/real_summary4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vz7O2Dt-VoI0"
      },
      "source": [
        "# **Beginners Guide to Text Generation using LSTMs**\n",
        "\n",
        "https://www.kaggle.com/shivamb/beginners-guide-to-text-generation-using-lstms\n",
        "\n",
        "참조 : https://github.com/williamSYSU/TextGAN-PyTorch\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "n2F_YwA-s-zL",
        "outputId": "c2583825-4233-452b-e654-bb5a346da9d1"
      },
      "source": [
        "!pip install sentence-transformers==0.3.0\n",
        "!pip install transformers==3.0.2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers==0.3.0\n",
            "  Downloading sentence-transformers-0.3.0.tar.gz (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 266 kB/s eta 0:00:011\n",
            "\u001b[?25hRequirement already satisfied: transformers>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers==0.3.0) (3.5.1)\n",
            "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from sentence-transformers==0.3.0) (4.45.0)\n",
            "Requirement already satisfied: torch>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers==0.3.0) (1.7.0)\n",
            "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers==0.3.0) (1.18.5)\n",
            "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from sentence-transformers==0.3.0) (0.23.2)\n",
            "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers==0.3.0) (1.4.1)\n",
            "Requirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from sentence-transformers==0.3.0) (3.2.4)\n",
            "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk->sentence-transformers==0.3.0) (1.14.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sentence-transformers==0.3.0) (2.1.0)\n",
            "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers==0.3.0) (1.18.5)\n",
            "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers==0.3.0) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sentence-transformers==0.3.0) (0.14.1)\n",
            "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers==0.3.0) (1.18.5)\n",
            "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch>=1.0.1->sentence-transformers==0.3.0) (0.18.2)\n",
            "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.0.1->sentence-transformers==0.3.0) (3.7.4.1)\n",
            "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch>=1.0.1->sentence-transformers==0.3.0) (0.6)\n",
            "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers==0.3.0) (1.18.5)\n",
            "Requirement already satisfied: sentencepiece==0.1.91 in /opt/conda/lib/python3.7/site-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (0.1.91)\n",
            "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (3.0.10)\n",
            "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (0.0.43)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (2020.4.4)\n",
            "Requirement already satisfied: tokenizers==0.9.3 in /opt/conda/lib/python3.7/site-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (0.9.3)\n",
            "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (2.23.0)\n",
            "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (20.1)\n",
            "Requirement already satisfied: protobuf in /opt/conda/lib/python3.7/site-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (3.14.0)\n",
            "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers==0.3.0) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from sentence-transformers==0.3.0) (4.45.0)\n",
            "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk->sentence-transformers==0.3.0) (1.14.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers>=3.0.2->sentence-transformers==0.3.0) (2.4.7)\n",
            "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk->sentence-transformers==0.3.0) (1.14.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (1.25.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (3.0.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (2020.4.4)\n",
            "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers>=3.0.2->sentence-transformers==0.3.0) (7.1.1)\n",
            "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk->sentence-transformers==0.3.0) (1.14.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sentence-transformers==0.3.0) (0.14.1)\n",
            "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from sentence-transformers==0.3.0) (4.45.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-0.3.0-py3-none-any.whl size=86752 sha256=f25574e1d03f6dd04ca25c452ebb3a4ae9c189c98de88fd42992ce3c3b40750e\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/15/94/49bc84289d2c77b5059bca513f840c6006d4e2cc7f10275d49\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentence-transformers\n",
            "Successfully installed sentence-transformers-0.3.0\n",
            "\u001b[33mWARNING: You are using pip version 20.3.1; however, version 20.3.3 is available.\n",
            "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Collecting transformers==3.0.2\n",
            "  Downloading transformers-3.0.2-py3-none-any.whl (769 kB)\n",
            "\u001b[K     |████████████████████████████████| 769 kB 884 kB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.2) (4.45.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.2) (0.1.91)\n",
            "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.2) (20.1)\n",
            "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.2) (0.0.43)\n",
            "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.2) (1.18.5)\n",
            "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.2) (2.23.0)\n",
            "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.2) (3.0.10)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.2) (2020.4.4)\n",
            "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==3.0.2) (1.14.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==3.0.2) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3.0.2) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3.0.2) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3.0.2) (1.25.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3.0.2) (2020.12.5)\n",
            "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==3.0.2) (1.14.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.2) (4.45.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.2) (2020.4.4)\n",
            "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==3.0.2) (7.1.1)\n",
            "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==3.0.2) (0.14.1)\n",
            "Collecting tokenizers==0.8.1.rc1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  Downloading tokenizers-0.8.1rc1-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 2.2 MB/s eta 0:00:01\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.9.3\n",
            "    Uninstalling tokenizers-0.9.3:\n",
            "      Successfully uninstalled tokenizers-0.9.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 3.5.1\n",
            "    Uninstalling transformers-3.5.1:\n",
            "      Successfully uninstalled transformers-3.5.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "allennlp 1.2.2 requires transformers<3.6,>=3.4, but you have transformers 3.0.2 which is incompatible.\u001b[0m\n",
            "Successfully installed tokenizers-0.8.1rc1 transformers-3.0.2\n",
            "\u001b[33mWARNING: You are using pip version 20.3.1; however, version 20.3.3 is available.\n",
            "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1m66KMMVs-P",
        "trusted": true
      },
      "source": [
        "# keras module for building LSTM \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "from tensorflow.python.framework import tensor_shape\n",
        "import keras.utils as ku \n",
        "\n",
        "# set seeds for reproducability\n",
        "from tensorflow.random import set_seed\n",
        "from numpy.random import seed\n",
        "set_seed(2)\n",
        "seed(1)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string, os \n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cLgLP9Ws-zX"
      },
      "source": [
        "# 여기서부터 본론..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "BUH5iWxCs-zX",
        "outputId": "c52d12ba-cb57-49f0-ee99-a7c877da910a"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import BertTokenizer\n",
        "# embedder download...\n",
        "embedder = SentenceTransformer('xlm-r-large-en-ko-nli-ststb')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n",
            "100%|██████████| 1.80G/1.80G [01:37<00:00, 18.4MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9D0W9vI4FB1c",
        "trusted": true
      },
      "source": [
        "document = \"\"\"\n",
        "주호영 국민의힘 원내대표는 22일 고위공직자범죄수사처(공수처)법 개정과 가덕도 신공항 건설 등을 밀어붙이고 있는 문재인 정권과 더불어민주당을 향해 \"이제 끝이 보인다\"며 \"짓밟힌 풀들이 아우성 치는 국민적 저항에 직면할 것\"이라고 경고했다.\n",
        "주 원내대표는 이날 자신의 페이스북에 \"문재인 정권이 공수처법 개정을 위한 '군사작전'에 돌입하겠다고 엄포를 놓고 있다\"며 \"정의당을 끌어들이기 위해 꼼수 선거법에 묶어 '패스트트랙'이라는 불법·탈법으로 만들어낸 공수처법을 시행도 해보지 않고 고치려 하는 것\"이라고 지적했다.\n",
        "이어 주 원내대표는 \"야당 원내대표인 제게 문재인 대통령은 사람 좋아보이는 표정으로 '공수처는 야당의 동의 없이는 절대 출범할 수 없는 것'이라고 얘기했고, 야당이 유엔 안보리 상임이사국처럼 공수처장 임명에 '비토권'을 행사할 수 있는데 무얼 걱정하느냐고, 여당 사람들이 우리를 속였다\"며 \"거짓말이라는 비난을 개의치 않는 사람들\"이라고 꼬집었다.\n",
        "주 원내대표는 \"이해찬 전 민주당 대표가 얘기한 '민주당 20년 집권'의 토대가 올해 안에 완성된다\"며 \"탈원전과 동남권 신공항은 문 대통령이 대선 공약으로 내건 사업이니 여기에 불법이 있었다고 시비를 거는 것은 민주주의를 부정하는 것이라고 청와대 출신 윤건영 민주당 의원이 윽박지른다. 이제 '민주주의 없는 민주당'이 법위에 군림하는 '반민주'를 거리낌없이 획책하는 것\"이라고 언급했다.\n",
        "그러면서 주 원내대표는 \"표를 얻기 위해 나라 곳간을 다 허물어뜨렸고, 재정 운용에서 신중함은 사라졌다\"며 \"괴물 공수처가 출범하면 공무원 누구나 대통령과 권력이 지시하는 범죄행위에 거리낌 없이 가담할 것이다. 청와대와 권부 요직에 앉아 불법으로 각종 이권을 챙기는 권력자들에 대한 사건이 불거져도 공수처가 사건을 가져가 버리면 그만\"이라고 우려했다.\n",
        "주 원내대표는 \"문 대통령은 제게 '공수처는 고위 공직자들을 처벌하는 것인데 왜 야당이 반대하는지 이해할 수 없다'고 했는데, 그런 분이 청와대와 대통령 주변을 감시하는 특별감찰관은 취임 이후 지금까지 왜 임명하지 않았는가\"라며 \"공수처는 권력형 비리의 쓰레기 하치장, 종말 처리장이 될 것\"이라고 비판했다.\n",
        "문재인 정부를 향해 주 원내대표는 \"문 대통령과 그 사도들은 법치가 미치지 않는 무오류의 화신이 될 것\"이라며 \"오류를 인정하지 않는 존재가 바로 신이며 그 아래에는 자신들의 지도자를 목숨바쳐 지킴으로서 정의를 실현하겠다는 추종자들로 넘쳐 난다. 공수처는 지도자의 신성을 인정하지 않는 세력을 정죄하는 수단으로 전락할 것\"이라고 질타했다.\n",
        "주 원내대표는 \"저도 법조인이지만 대통령과 공수처장이 마음대로 검사들과 수사관들을 임명하는 이 끔찍한 사법기구가 어떤 일을 할지 두렵기만 하다\"며 \"공수처는 검찰과 경찰 위에 있는 사법기구로, 헌법과 법으로 독립성을 보장하는 검찰총장을 이렇게 핍박하는 정권이 공수처를 어떻게 운영할지 불을 보듯 뻔한 일\"이라고 예측했다.\n",
        "그러면서 주 원내대표는 \"추미애 법무장관을 앞장 세워 윤석열 검찰의 권력 비리 수사를 저지하려다가 난관에 봉착하자 무슨 수를 써서라도 공수처를 출범시키려 한다. 공수처장 자리에는 추미애보다 더 한 막무가내 내 편을 앉힐 게 분명한 것\"이라며 \"문 정권의 파렴치와 오만함을 최전선에서 온 몸으로 겪어온 저로서는 민주당이 내일부터 국회에서 보일 행태가 환히 보인다. 180석의 민주당이 또 군사작전을 개시하면 그걸 누가 막겠는가\"라고 성토했다.\n",
        "주 원내대표는 \"공수처법을 막을 힘이 우리 야당에게는 없다. 삭발하고 장외투쟁해 봐야 눈 하나 깜짝할 사람들이 아닌 것\"이라며 \"대란대치(大亂大治), 세상을 온통 혼돈 속으로 밀어넣고 그걸 권력 유지에 이용한다는 게 이 정권의 통치기술\"이라고 규탄했다.\n",
        "아울러 주 원내대표는 \"권력은 바람, 국민은 풀이다. 바람이 불면 청보리 밭의 보리가 눕는다\"며 \"권력은 풀들이 다시는 일어서지 못하도록 풀을 짓밟지만 풀들은 다시 일어난다. 시인 김수영은 '바람보다 먼저 눕지만, 바람보다 먼저 일어나는' 민초의 힘을 노래했다\"고 말했다.\n",
        "마지막으로 주 원내대표는 \"문재인 정권은 이제 곧 국회에서 광장에서 짓밟힌 풀들이 일어서서 아우성치는 모습을 지켜보게 될 것\"이라며 \"대란대치를 끝장내려는 국민적 저항에 직면할 것\"이라고 거듭 강조했다.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIxs7REEQVFO",
        "trusted": true,
        "outputId": "b563dccd-6982-469a-cac6-ea0f128bf16b",
        "colab": {
          "referenced_widgets": [
            "af35823a95cf4b8a833c7864fb2013d7"
          ]
        }
      },
      "source": [
        "c = embedder.encode([document])\n",
        "c"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Batches', max=1.0, style=ProgressStyle(description_width=…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af35823a95cf4b8a833c7864fb2013d7"
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([ 0.5201436 , -0.09330627,  0.4559566 , ..., -0.54848397,\n",
              "        -0.11432096, -0.06106506], dtype=float32)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocHnmjt4FRUF",
        "trusted": true
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([document])\n",
        "total_words = len(tokenizer.word_index) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8OViY44Fk0J",
        "trusted": true,
        "outputId": "89223ad9-5a1a-484a-b4d3-e2c2ea665ba5"
      },
      "source": [
        "print(total_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "395\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PeHFwITF0Ux",
        "trusted": true,
        "outputId": "d2b933dc-d249-41c7-c55f-cd8668ea29ee"
      },
      "source": [
        "# max 512 token으로 만든다. 남는건 padding\n",
        "\n",
        "_MAX_TOKEN = 512\n",
        "_MAX_LENGTH = 40\n",
        "_NOISE_DIM = 100\n",
        "_MISMATCH_WORD = '@@@'\n",
        "word_keys = []\n",
        "word_values = []\n",
        "\n",
        "for word,index in tokenizer.word_index.items():\n",
        "    word_keys.append(index)\n",
        "    word_values.append(word)\n",
        "\n",
        "current_token_len = len(word_keys)\n",
        "\n",
        "if current_token_len > _MAX_TOKEN:\n",
        "    word_keys = word_keys[:_MAX_TOKEN]\n",
        "    word_values = word_values[:_MAX_TOKEN]\n",
        "else:\n",
        "    for i in range(current_token_len+1,_MAX_TOKEN+1):\n",
        "        word_keys.append(i)\n",
        "        word_values.append(_MISMATCH_WORD)\n",
        "\n",
        "for k in word_keys:\n",
        "  print(k,word_values[k-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 원내대표는\n",
            "2 주\n",
            "3 것\n",
            "4 이라고\n",
            "5 며\n",
            "6 문재인\n",
            "7 않는\n",
            "8 문\n",
            "9 이라며\n",
            "10 이제\n",
            "11 풀들이\n",
            "12 수\n",
            "13 대통령과\n",
            "14 공수처는\n",
            "15 될\n",
            "16 있는\n",
            "17 향해\n",
            "18 보인다\n",
            "19 짓밟힌\n",
            "20 국민적\n",
            "21 저항에\n",
            "22 직면할\n",
            "23 정권이\n",
            "24 위해\n",
            "25 공수처법을\n",
            "26 제게\n",
            "27 대통령은\n",
            "28 '공수처는\n",
            "29 없는\n",
            "30 야당이\n",
            "31 공수처장\n",
            "32 사람들이\n",
            "33 민주당\n",
            "34 그러면서\n",
            "35 공수처가\n",
            "36 청와대와\n",
            "37 왜\n",
            "38 그\n",
            "39 인정하지\n",
            "40 이\n",
            "41 공수처를\n",
            "42 권력\n",
            "43 게\n",
            "44 정권의\n",
            "45 민주당이\n",
            "46 국회에서\n",
            "47 그걸\n",
            "48 권력은\n",
            "49 먼저\n",
            "50 주호영\n",
            "51 국민의힘\n",
            "52 22일\n",
            "53 고위공직자범죄수사처\n",
            "54 공수처\n",
            "55 법\n",
            "56 개정과\n",
            "57 가덕도\n",
            "58 신공항\n",
            "59 건설\n",
            "60 등을\n",
            "61 밀어붙이고\n",
            "62 정권과\n",
            "63 더불어민주당을\n",
            "64 끝이\n",
            "65 아우성\n",
            "66 치는\n",
            "67 경고했다\n",
            "68 이날\n",
            "69 자신의\n",
            "70 페이스북에\n",
            "71 공수처법\n",
            "72 개정을\n",
            "73 위한\n",
            "74 '군사작전'에\n",
            "75 돌입하겠다고\n",
            "76 엄포를\n",
            "77 놓고\n",
            "78 있다\n",
            "79 정의당을\n",
            "80 끌어들이기\n",
            "81 꼼수\n",
            "82 선거법에\n",
            "83 묶어\n",
            "84 '패스트트랙'이라는\n",
            "85 불법·탈법으로\n",
            "86 만들어낸\n",
            "87 시행도\n",
            "88 해보지\n",
            "89 않고\n",
            "90 고치려\n",
            "91 하는\n",
            "92 지적했다\n",
            "93 이어\n",
            "94 야당\n",
            "95 원내대표인\n",
            "96 사람\n",
            "97 좋아보이는\n",
            "98 표정으로\n",
            "99 야당의\n",
            "100 동의\n",
            "101 없이는\n",
            "102 절대\n",
            "103 출범할\n",
            "104 것'이라고\n",
            "105 얘기했고\n",
            "106 유엔\n",
            "107 안보리\n",
            "108 상임이사국처럼\n",
            "109 임명에\n",
            "110 '비토권'을\n",
            "111 행사할\n",
            "112 있는데\n",
            "113 무얼\n",
            "114 걱정하느냐고\n",
            "115 여당\n",
            "116 우리를\n",
            "117 속였다\n",
            "118 거짓말이라는\n",
            "119 비난을\n",
            "120 개의치\n",
            "121 사람들\n",
            "122 꼬집었다\n",
            "123 이해찬\n",
            "124 전\n",
            "125 대표가\n",
            "126 얘기한\n",
            "127 '민주당\n",
            "128 20년\n",
            "129 집권'의\n",
            "130 토대가\n",
            "131 올해\n",
            "132 안에\n",
            "133 완성된다\n",
            "134 탈원전과\n",
            "135 동남권\n",
            "136 신공항은\n",
            "137 대통령이\n",
            "138 대선\n",
            "139 공약으로\n",
            "140 내건\n",
            "141 사업이니\n",
            "142 여기에\n",
            "143 불법이\n",
            "144 있었다고\n",
            "145 시비를\n",
            "146 거는\n",
            "147 것은\n",
            "148 민주주의를\n",
            "149 부정하는\n",
            "150 것이라고\n",
            "151 청와대\n",
            "152 출신\n",
            "153 윤건영\n",
            "154 의원이\n",
            "155 윽박지른다\n",
            "156 '민주주의\n",
            "157 민주당'이\n",
            "158 법위에\n",
            "159 군림하는\n",
            "160 '반민주'를\n",
            "161 거리낌없이\n",
            "162 획책하는\n",
            "163 언급했다\n",
            "164 표를\n",
            "165 얻기\n",
            "166 나라\n",
            "167 곳간을\n",
            "168 다\n",
            "169 허물어뜨렸고\n",
            "170 재정\n",
            "171 운용에서\n",
            "172 신중함은\n",
            "173 사라졌다\n",
            "174 괴물\n",
            "175 출범하면\n",
            "176 공무원\n",
            "177 누구나\n",
            "178 권력이\n",
            "179 지시하는\n",
            "180 범죄행위에\n",
            "181 거리낌\n",
            "182 없이\n",
            "183 가담할\n",
            "184 것이다\n",
            "185 권부\n",
            "186 요직에\n",
            "187 앉아\n",
            "188 불법으로\n",
            "189 각종\n",
            "190 이권을\n",
            "191 챙기는\n",
            "192 권력자들에\n",
            "193 대한\n",
            "194 사건이\n",
            "195 불거져도\n",
            "196 사건을\n",
            "197 가져가\n",
            "198 버리면\n",
            "199 그만\n",
            "200 우려했다\n",
            "201 고위\n",
            "202 공직자들을\n",
            "203 처벌하는\n",
            "204 것인데\n",
            "205 반대하는지\n",
            "206 이해할\n",
            "207 없다'고\n",
            "208 했는데\n",
            "209 그런\n",
            "210 분이\n",
            "211 대통령\n",
            "212 주변을\n",
            "213 감시하는\n",
            "214 특별감찰관은\n",
            "215 취임\n",
            "216 이후\n",
            "217 지금까지\n",
            "218 임명하지\n",
            "219 않았는가\n",
            "220 라며\n",
            "221 권력형\n",
            "222 비리의\n",
            "223 쓰레기\n",
            "224 하치장\n",
            "225 종말\n",
            "226 처리장이\n",
            "227 비판했다\n",
            "228 정부를\n",
            "229 사도들은\n",
            "230 법치가\n",
            "231 미치지\n",
            "232 무오류의\n",
            "233 화신이\n",
            "234 오류를\n",
            "235 존재가\n",
            "236 바로\n",
            "237 신이며\n",
            "238 아래에는\n",
            "239 자신들의\n",
            "240 지도자를\n",
            "241 목숨바쳐\n",
            "242 지킴으로서\n",
            "243 정의를\n",
            "244 실현하겠다는\n",
            "245 추종자들로\n",
            "246 넘쳐\n",
            "247 난다\n",
            "248 지도자의\n",
            "249 신성을\n",
            "250 세력을\n",
            "251 정죄하는\n",
            "252 수단으로\n",
            "253 전락할\n",
            "254 질타했다\n",
            "255 저도\n",
            "256 법조인이지만\n",
            "257 공수처장이\n",
            "258 마음대로\n",
            "259 검사들과\n",
            "260 수사관들을\n",
            "261 임명하는\n",
            "262 끔찍한\n",
            "263 사법기구가\n",
            "264 어떤\n",
            "265 일을\n",
            "266 할지\n",
            "267 두렵기만\n",
            "268 하다\n",
            "269 검찰과\n",
            "270 경찰\n",
            "271 위에\n",
            "272 사법기구로\n",
            "273 헌법과\n",
            "274 법으로\n",
            "275 독립성을\n",
            "276 보장하는\n",
            "277 검찰총장을\n",
            "278 이렇게\n",
            "279 핍박하는\n",
            "280 어떻게\n",
            "281 운영할지\n",
            "282 불을\n",
            "283 보듯\n",
            "284 뻔한\n",
            "285 일\n",
            "286 예측했다\n",
            "287 추미애\n",
            "288 법무장관을\n",
            "289 앞장\n",
            "290 세워\n",
            "291 윤석열\n",
            "292 검찰의\n",
            "293 비리\n",
            "294 수사를\n",
            "295 저지하려다가\n",
            "296 난관에\n",
            "297 봉착하자\n",
            "298 무슨\n",
            "299 수를\n",
            "300 써서라도\n",
            "301 출범시키려\n",
            "302 한다\n",
            "303 자리에는\n",
            "304 추미애보다\n",
            "305 더\n",
            "306 한\n",
            "307 막무가내\n",
            "308 내\n",
            "309 편을\n",
            "310 앉힐\n",
            "311 분명한\n",
            "312 파렴치와\n",
            "313 오만함을\n",
            "314 최전선에서\n",
            "315 온\n",
            "316 몸으로\n",
            "317 겪어온\n",
            "318 저로서는\n",
            "319 내일부터\n",
            "320 보일\n",
            "321 행태가\n",
            "322 환히\n",
            "323 180석의\n",
            "324 또\n",
            "325 군사작전을\n",
            "326 개시하면\n",
            "327 누가\n",
            "328 막겠는가\n",
            "329 라고\n",
            "330 성토했다\n",
            "331 막을\n",
            "332 힘이\n",
            "333 우리\n",
            "334 야당에게는\n",
            "335 없다\n",
            "336 삭발하고\n",
            "337 장외투쟁해\n",
            "338 봐야\n",
            "339 눈\n",
            "340 하나\n",
            "341 깜짝할\n",
            "342 아닌\n",
            "343 대란대치\n",
            "344 大亂大治\n",
            "345 세상을\n",
            "346 온통\n",
            "347 혼돈\n",
            "348 속으로\n",
            "349 밀어넣고\n",
            "350 유지에\n",
            "351 이용한다는\n",
            "352 통치기술\n",
            "353 규탄했다\n",
            "354 아울러\n",
            "355 바람\n",
            "356 국민은\n",
            "357 풀이다\n",
            "358 바람이\n",
            "359 불면\n",
            "360 청보리\n",
            "361 밭의\n",
            "362 보리가\n",
            "363 눕는다\n",
            "364 다시는\n",
            "365 일어서지\n",
            "366 못하도록\n",
            "367 풀을\n",
            "368 짓밟지만\n",
            "369 풀들은\n",
            "370 다시\n",
            "371 일어난다\n",
            "372 시인\n",
            "373 김수영은\n",
            "374 '바람보다\n",
            "375 눕지만\n",
            "376 바람보다\n",
            "377 일어나는'\n",
            "378 민초의\n",
            "379 힘을\n",
            "380 노래했다\n",
            "381 고\n",
            "382 말했다\n",
            "383 마지막으로\n",
            "384 정권은\n",
            "385 곧\n",
            "386 광장에서\n",
            "387 일어서서\n",
            "388 아우성치는\n",
            "389 모습을\n",
            "390 지켜보게\n",
            "391 대란대치를\n",
            "392 끝장내려는\n",
            "393 거듭\n",
            "394 강조했다\n",
            "395 @@@\n",
            "396 @@@\n",
            "397 @@@\n",
            "398 @@@\n",
            "399 @@@\n",
            "400 @@@\n",
            "401 @@@\n",
            "402 @@@\n",
            "403 @@@\n",
            "404 @@@\n",
            "405 @@@\n",
            "406 @@@\n",
            "407 @@@\n",
            "408 @@@\n",
            "409 @@@\n",
            "410 @@@\n",
            "411 @@@\n",
            "412 @@@\n",
            "413 @@@\n",
            "414 @@@\n",
            "415 @@@\n",
            "416 @@@\n",
            "417 @@@\n",
            "418 @@@\n",
            "419 @@@\n",
            "420 @@@\n",
            "421 @@@\n",
            "422 @@@\n",
            "423 @@@\n",
            "424 @@@\n",
            "425 @@@\n",
            "426 @@@\n",
            "427 @@@\n",
            "428 @@@\n",
            "429 @@@\n",
            "430 @@@\n",
            "431 @@@\n",
            "432 @@@\n",
            "433 @@@\n",
            "434 @@@\n",
            "435 @@@\n",
            "436 @@@\n",
            "437 @@@\n",
            "438 @@@\n",
            "439 @@@\n",
            "440 @@@\n",
            "441 @@@\n",
            "442 @@@\n",
            "443 @@@\n",
            "444 @@@\n",
            "445 @@@\n",
            "446 @@@\n",
            "447 @@@\n",
            "448 @@@\n",
            "449 @@@\n",
            "450 @@@\n",
            "451 @@@\n",
            "452 @@@\n",
            "453 @@@\n",
            "454 @@@\n",
            "455 @@@\n",
            "456 @@@\n",
            "457 @@@\n",
            "458 @@@\n",
            "459 @@@\n",
            "460 @@@\n",
            "461 @@@\n",
            "462 @@@\n",
            "463 @@@\n",
            "464 @@@\n",
            "465 @@@\n",
            "466 @@@\n",
            "467 @@@\n",
            "468 @@@\n",
            "469 @@@\n",
            "470 @@@\n",
            "471 @@@\n",
            "472 @@@\n",
            "473 @@@\n",
            "474 @@@\n",
            "475 @@@\n",
            "476 @@@\n",
            "477 @@@\n",
            "478 @@@\n",
            "479 @@@\n",
            "480 @@@\n",
            "481 @@@\n",
            "482 @@@\n",
            "483 @@@\n",
            "484 @@@\n",
            "485 @@@\n",
            "486 @@@\n",
            "487 @@@\n",
            "488 @@@\n",
            "489 @@@\n",
            "490 @@@\n",
            "491 @@@\n",
            "492 @@@\n",
            "493 @@@\n",
            "494 @@@\n",
            "495 @@@\n",
            "496 @@@\n",
            "497 @@@\n",
            "498 @@@\n",
            "499 @@@\n",
            "500 @@@\n",
            "501 @@@\n",
            "502 @@@\n",
            "503 @@@\n",
            "504 @@@\n",
            "505 @@@\n",
            "506 @@@\n",
            "507 @@@\n",
            "508 @@@\n",
            "509 @@@\n",
            "510 @@@\n",
            "511 @@@\n",
            "512 @@@\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F740wH5sF9lG",
        "trusted": true
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (Input,\n",
        "                                     Dense, \n",
        "                                     BatchNormalization, \n",
        "                                     LeakyReLU,\n",
        "                                     Softmax,\n",
        "                                     Reshape, \n",
        "                                     Conv2DTranspose,\n",
        "                                     Conv2D,\n",
        "                                     Dropout,\n",
        "                                     Flatten,\n",
        "                                     Lambda)\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JILw73lq6Gc0",
        "trusted": true
      },
      "source": [
        "\n",
        "keys=tf.constant(word_keys,tf.int32)\n",
        "values=tf.constant(word_values, tf.string)\n",
        "# build a lookup table\n",
        "word_table = tf.lookup.StaticHashTable(\n",
        "    initializer=tf.lookup.KeyValueTensorInitializer(keys,values),\n",
        "    default_value=tf.constant(' '),\n",
        "    name=\"class_weight\"\n",
        ")\n",
        "\n",
        "\n",
        "# build a lookup table\n",
        "#word_table = tf.lookup.StaticHashTable(tf.lookup.KeyValueTensorInitializer(tf.constant(word_keys), tf.constant(word_values)),default_value=-1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "OKH9DAGks-zb"
      },
      "source": [
        "import sys\n",
        "\n",
        "def to_text(w):\n",
        "    #r_value = None\n",
        "    #print(w)\n",
        "    texts = []\n",
        "    try:\n",
        "        #codes = []\n",
        "        \n",
        "        #embeddings = []\n",
        "\n",
        "        for z in w:\n",
        "            text = \"\"\n",
        "            #code = []\n",
        "            for v in z:\n",
        "                try:\n",
        "                    #print(v)\n",
        "                    key = tf.argmax(v,output_type=tf.int32) #tf.constant(,dtype=tf.int32)\n",
        "                    #if type=='code':\n",
        "                    #    code.append(key.numpy())\n",
        "                    #else:\n",
        "                    #    text += word_table.lookup(key).numpy().decode('utf-8') + ' '\n",
        "                    text += word_table.lookup(key).numpy().decode('utf-8') + ' '\n",
        "                except Exception as ex:\n",
        "                    tf.print(ex,sys.exc_info())\n",
        "                    #code.append(0)\n",
        "                    text += '[   ] '                  \n",
        "            #print(generated_text)\n",
        "            #if type == 'code':\n",
        "            #    codes.append(code)\n",
        "            #else:\n",
        "            #    texts.append(text)\n",
        "            texts.append(text)\n",
        "\n",
        "    except Exception as ex:\n",
        "        tf.print(ex,sys.exc_info())\n",
        "        #codes = np.random([w.shape[0],30])\n",
        "        #texts = '----------------------------------------------------'\n",
        "    '''\n",
        "    if type == 'code':\n",
        "        r_value = tf.constant(codes,dtype=tf.int32)\n",
        "    elif type == 'embedding':\n",
        "        r_value = tf.constant(embedder.encode(texts),dtype=tf.float32)\n",
        "    else:\n",
        "        r_value = tf.constant(texts,dtype=tf.string)\n",
        "    '''\n",
        "    return tf.constant(texts,dtype=tf.string)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBrB8gNSggTC",
        "trusted": true,
        "outputId": "87075db3-d003-42a4-8c4b-3555bc56210a"
      },
      "source": [
        "w = tf.random.normal([2,_MAX_LENGTH,_MAX_TOKEN])\n",
        "e = to_text(w)\n",
        "print(e)\n",
        "for t in e:\n",
        "  print(t.numpy().decode('utf-8'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b'\\xec\\x9c\\xbd\\xeb\\xb0\\x95\\xec\\xa7\\x80\\xeb\\xa5\\xb8\\xeb\\x8b\\xa4 \\xec\\x8b\\x9c\\xed\\x96\\x89\\xeb\\x8f\\x84 \\xed\\x95\\xa0\\xec\\xa7\\x80 \\xea\\xb0\\x81\\xec\\xa2\\x85 \\xea\\xb2\\xaa\\xec\\x96\\xb4\\xec\\x98\\xa8 \\xeb\\x88\\x88 \\xea\\xbc\\xbc\\xec\\x88\\x98 \\xeb\\xb0\\x94\\xeb\\x9e\\x8c\\xeb\\xb3\\xb4\\xeb\\x8b\\xa4 @@@ @@@ @@@ \\xea\\xb6\\x8c\\xeb\\xb6\\x80 \\xec\\x96\\xb4\\xeb\\x96\\xbb\\xea\\xb2\\x8c @@@ \\xea\\xb0\\x9c\\xec\\xa0\\x95\\xea\\xb3\\xbc \\xea\\xb4\\x91\\xec\\x9e\\xa5\\xec\\x97\\x90\\xec\\x84\\x9c @@@ \\xeb\\x8f\\x85\\xeb\\xa6\\xbd\\xec\\x84\\xb1\\xec\\x9d\\x84 \\xed\\x95\\x9c \\xec\\xa0\\x95\\xec\\xa3\\x84\\xed\\x95\\x98\\xeb\\x8a\\x94 \\xec\\x9e\\x84\\xeb\\xaa\\x85\\xec\\x97\\x90 @@@ @@@ \\xec\\x8b\\xa0\\xec\\x9d\\xb4\\xeb\\xa9\\xb0 \\xeb\\xac\\xb8 \\xed\\x97\\x8c\\xeb\\xb2\\x95\\xea\\xb3\\xbc @@@ @@@ \\xec\\xa3\\xbc \\xeb\\xa7\\x89\\xea\\xb2\\xa0\\xeb\\x8a\\x94\\xea\\xb0\\x80 @@@ \\xec\\x9c\\xa4\\xea\\xb1\\xb4\\xec\\x98\\x81 \\xec\\xa0\\x95\\xea\\xb6\\x8c\\xec\\x9d\\x98 @@@ \\xec\\xa7\\x93\\xeb\\xb0\\x9f\\xed\\x9e\\x8c \\xeb\\xb0\\x80\\xec\\x96\\xb4\\xeb\\x84\\xa3\\xea\\xb3\\xa0 \\xec\\x95\\x8a\\xec\\x95\\x98\\xeb\\x8a\\x94\\xea\\xb0\\x80 \\xec\\x9c\\x84\\xed\\x95\\xb4 @@@ @@@ '\n",
            " b'@@@ \\xea\\xb5\\xad\\xed\\x9a\\x8c\\xec\\x97\\x90\\xec\\x84\\x9c \\xeb\\xaf\\xbc\\xec\\xa3\\xbc\\xeb\\x8b\\xb9\\xec\\x9d\\xb4 \\xeb\\x81\\x8c\\xec\\x96\\xb4\\xeb\\x93\\xa4\\xec\\x9d\\xb4\\xea\\xb8\\xb0 @@@ \\xeb\\xb0\\x94\\xeb\\xa1\\x9c \\xec\\x9d\\xb4\\xeb\\xa0\\x87\\xea\\xb2\\x8c \\xea\\xb5\\xad\\xed\\x9a\\x8c\\xec\\x97\\x90\\xec\\x84\\x9c \\xeb\\xa7\\x89\\xea\\xb2\\xa0\\xeb\\x8a\\x94\\xea\\xb0\\x80 @@@ @@@ \\xec\\x95\\x89\\xec\\x95\\x84 \\xea\\xb0\\x80\\xec\\xa0\\xb8\\xea\\xb0\\x80 \\xec\\xa0\\x95\\xec\\x9d\\x98\\xeb\\x8b\\xb9\\xec\\x9d\\x84 \\xec\\x84\\xb1\\xed\\x86\\xa0\\xed\\x96\\x88\\xeb\\x8b\\xa4 @@@ \\xed\\x92\\x80\\xec\\x9d\\x84 \\xeb\\x86\\x93\\xea\\xb3\\xa0 \\xeb\\x8d\\x94\\xeb\\xb6\\x88\\xec\\x96\\xb4\\xeb\\xaf\\xbc\\xec\\xa3\\xbc\\xeb\\x8b\\xb9\\xec\\x9d\\x84 \\xed\\x99\\x98\\xed\\x9e\\x88 \\xec\\xa0\\x80\\xed\\x95\\xad\\xec\\x97\\x90 @@@ @@@ \\xea\\xb7\\xb8\\xeb\\x9f\\xac\\xeb\\xa9\\xb4\\xec\\x84\\x9c @@@ @@@ \\xec\\x9e\\x90\\xec\\x8b\\xa0\\xec\\x9d\\x98 \\xeb\\x9d\\xbc\\xeb\\xa9\\xb0 \\xec\\xa7\\x81\\xeb\\xa9\\xb4\\xed\\x95\\xa0 \\xea\\xb0\\x9c\\xec\\x8b\\x9c\\xed\\x95\\x98\\xeb\\xa9\\xb4 @@@ \\xec\\x95\\xbc\\xeb\\x8b\\xb9 \\xec\\x96\\xb4\\xeb\\x96\\xa4 @@@ \\xeb\\xa7\\x88\\xec\\x9d\\x8c\\xeb\\x8c\\x80\\xeb\\xa1\\x9c \\xeb\\x8c\\x80\\xed\\x86\\xb5\\xeb\\xa0\\xb9\\xea\\xb3\\xbc \\xeb\\x8c\\x80\\xed\\x86\\xb5\\xeb\\xa0\\xb9\\xec\\x9d\\x80 \\xea\\xb0\\x90\\xec\\x8b\\x9c\\xed\\x95\\x98\\xeb\\x8a\\x94 \\xec\\x9d\\xb4\\xeb\\x9d\\xbc\\xeb\\xa9\\xb0 \\xeb\\xb3\\xb4\\xec\\x9d\\xbc '], shape=(2,), dtype=string)\n",
            "윽박지른다 시행도 할지 각종 겪어온 눈 꼼수 바람보다 @@@ @@@ @@@ 권부 어떻게 @@@ 개정과 광장에서 @@@ 독립성을 한 정죄하는 임명에 @@@ @@@ 신이며 문 헌법과 @@@ @@@ 주 막겠는가 @@@ 윤건영 정권의 @@@ 짓밟힌 밀어넣고 않았는가 위해 @@@ @@@ \n",
            "@@@ 국회에서 민주당이 끌어들이기 @@@ 바로 이렇게 국회에서 막겠는가 @@@ @@@ 앉아 가져가 정의당을 성토했다 @@@ 풀을 놓고 더불어민주당을 환히 저항에 @@@ @@@ 그러면서 @@@ @@@ 자신의 라며 직면할 개시하면 @@@ 야당 어떤 @@@ 마음대로 대통령과 대통령은 감시하는 이라며 보일 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFPD7V7P8QUJ",
        "trusted": true
      },
      "source": [
        "\n",
        "@tf.custom_gradient\n",
        "def to_embedding(w):\n",
        "\n",
        "    def grad(dy):\n",
        "        dy_arr = tf.reshape(dy,(dy.shape[0],1024,1))\n",
        "        #tf.print(dy_arr)\n",
        "        dy_arr_st = tf.image.resize(dy_arr, size=(dy.shape[0],_MAX_LENGTH*_MAX_TOKEN),method=tf.image.ResizeMethod.BILINEAR)\n",
        "        dy_arr_st = tf.reshape(dy_arr_st,shape=(dy.shape[0],_MAX_LENGTH,_MAX_TOKEN))\n",
        "        return dy_arr_st\n",
        "\n",
        "    #print(w)    \n",
        "    texts = []\n",
        "    value = None\n",
        "    try:\n",
        "        for z in w:\n",
        "            text = \"\"\n",
        "            for v in z:\n",
        "                try:\n",
        "                    key = tf.argmax(v,output_type=tf.int32) #tf.constant(,dtype=tf.int32)\n",
        "                    text += word_table.lookup(key).numpy().decode('utf-8') + ' '\n",
        "                except Exception as ex:\n",
        "                    tf.print(ex,sys.exc_info())\n",
        "                    text += '[   ] '                  \n",
        "            texts.append(text)\n",
        "        value = tf.constant(embedder.encode(texts,show_progress_bar=False),dtype=tf.float32)\n",
        "    except Exception as ex:\n",
        "        tf.print(ex,sys.exc_info())\n",
        "\n",
        "    return value, grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmSdp5mq9KaT",
        "trusted": true,
        "outputId": "55373f9b-71dd-4141-9c3f-603313565a97"
      },
      "source": [
        "e = to_embedding(w)\n",
        "print(e)\n",
        "for t in e:\n",
        "  print(t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[ 0.46808648 -0.38874424  0.7548216  ... -0.30340677  0.11780797\n",
            "  -0.00394414]\n",
            " [ 0.1789437  -0.2935444   0.53903025 ... -0.25947365 -0.3661812\n",
            "   0.3764538 ]], shape=(2, 1024), dtype=float32)\n",
            "tf.Tensor(\n",
            "[ 0.46808648 -0.38874424  0.7548216  ... -0.30340677  0.11780797\n",
            " -0.00394414], shape=(1024,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[ 0.1789437  -0.2935444   0.53903025 ... -0.25947365 -0.3661812\n",
            "  0.3764538 ], shape=(1024,), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "eqDAAlIFs-zb"
      },
      "source": [
        "from keras import backend as K\n",
        "from keras.layers import Layer\n",
        "\n",
        "#tf.executing_eagerly()\n",
        "\n",
        "class Post_processing(Layer):\n",
        "\n",
        "    def __init__(self, output_dim, encoder_func=None,Tout=tf.float32, **kwargs):\n",
        "        self.output_dim = output_dim\n",
        "        self.encoder = encoder_func\n",
        "        self.Tout = Tout\n",
        "        super(Post_processing, self).__init__(**kwargs)\n",
        "    '''\n",
        "    def build(self, input_shape):\n",
        "        tf.print('build',input_shape)\n",
        "        # 이 레이어에 대해 학습가능한 가중치 변수를 만듭니다.\n",
        "        self.kernel = self.add_weight(name='kernel', \n",
        "                                      shape=(input_shape[1], self.output_dim),\n",
        "                                      initializer='uniform',\n",
        "                                      trainable=True)\n",
        "        super(Post_processing, self).build(input_shape)  # 끝에서 꼭 이 함수를 호출하십시오\n",
        "    '''\n",
        "    def call(self, input_data):\n",
        "        #tf.print('Post_processing : call input_data',input_data.shape)\n",
        "        value = tf.py_function(self.encoder,[input_data],Tout=self.Tout,name='encode_func')\n",
        "        #print('value.shape:',value.shape)\n",
        "        #value.set_shape((input_data.shape[0],self.output_dim))\n",
        "        if self.output_dim > 0:\n",
        "            value.set_shape((input_data.shape[0],self.output_dim))\n",
        "        else:\n",
        "            value.set_shape((input_data.shape[0],))\n",
        "        #return tf.reshape(value,[input_data.shape[0]])  \n",
        "\n",
        "        #value = tf.Variable((tf.zeros([input_data.shape[0],1024]) if self.Tout==tf.float32 else tf.zeros([input_data.shape[0],])),dtype=self.Tout,shape=( (input_data.shape[0],1024) if self.Tout==tf.float32 else (input_data.shape[0],)))\n",
        "        #tf.py_function(self.encoder,[input_data],Tout=self.Tout)\n",
        "        return value\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        tf.print('compute_output_shape:',input_shape)\n",
        "        input_shape = tensor_shape.TensorShape(input_shape).as_list()\n",
        "        if self.output_dim > 0:\n",
        "            return tensor_shape.TensorShape([input_shape[0], self.output_dim])\n",
        "        return tensor_shape.TensorShape([input_shape[0]])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "OeU6sVCrs-zc",
        "outputId": "d3d8063f-2a23-41eb-ba20-d6c926163519"
      },
      "source": [
        "e = Post_processing(1024,to_embedding,Tout=tf.float32)(w)\n",
        "print(e)\n",
        "for c in e:\n",
        "  print(c)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[ 0.13558488 -0.3644542  -0.21993382 ...  0.03499798 -0.13391383\n",
            "  -0.09278788]\n",
            " [ 0.6617238  -0.4898074   0.651245   ... -0.08602053  0.3437744\n",
            "  -0.47278643]], shape=(2, 1024), dtype=float32)\n",
            "tf.Tensor(\n",
            "[ 0.13558488 -0.3644542  -0.21993382 ...  0.03499798 -0.13391383\n",
            " -0.09278788], shape=(1024,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[ 0.6617238  -0.4898074   0.651245   ... -0.08602053  0.3437744\n",
            " -0.47278643], shape=(1024,), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgYVnQzk_FYu",
        "trusted": true,
        "outputId": "b01e687f-9db9-4884-c41b-1b0251781e22"
      },
      "source": [
        "e = Post_processing(0,to_text,Tout=tf.string)(w)\n",
        "print(e)\n",
        "for c in e:\n",
        "  print(c.numpy().decode('utf-8'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b\"\\xea\\xb2\\x80\\xec\\xb0\\xb0\\xea\\xb3\\xbc \\xec\\x96\\x91\\xec\\x9e\\x90\\xec\\x97\\xad\\xed\\x95\\x99 \\xec\\x95\\x9e\\xec\\x9e\\xa5 \\xea\\xb7\\xb8\\xeb\\xa7\\x8c \\xec\\x96\\x91\\xec\\x9e\\x90\\xec\\x97\\xad\\xed\\x95\\x99 \\xec\\x96\\x91\\xec\\x9e\\x90\\xec\\x97\\xad\\xed\\x95\\x99 \\xec\\x82\\xac\\xeb\\x8f\\x84\\xeb\\x93\\xa4\\xec\\x9d\\x80 22\\xec\\x9d\\xbc \\xeb\\xb3\\xb4\\xec\\x9d\\xb8\\xeb\\x8b\\xa4 \\xec\\x96\\x91\\xec\\x9e\\x90\\xec\\x97\\xad\\xed\\x95\\x99 \\xea\\xb3\\xa0 \\xec\\x96\\x91\\xec\\x9e\\x90\\xec\\x97\\xad\\xed\\x95\\x99 \\xec\\xa3\\xbc\\xeb\\xb3\\x80\\xec\\x9d\\x84 \\xeb\\x8f\\x85\\xeb\\xa6\\xbd\\xec\\x84\\xb1\\xec\\x9d\\x84 \\xec\\x96\\x91\\xec\\x9e\\x90\\xec\\x97\\xad\\xed\\x95\\x99 \\xec\\x96\\x91\\xec\\x9e\\x90\\xec\\x97\\xad\\xed\\x95\\x99 \\xec\\x95\\xbc\\xeb\\x8b\\xb9\\xec\\x9d\\x98 \\xec\\x9e\\xac\\xec\\xa0\\x95 \\xea\\xb9\\x9c\\xec\\xa7\\x9d\\xed\\x95\\xa0 \\xec\\xb6\\x9c\\xeb\\xb2\\x94\\xed\\x95\\x98\\xeb\\xa9\\xb4 \\xec\\x96\\x91\\xec\\x9e\\x90\\xec\\x97\\xad\\xed\\x95\\x99 \\xec\\x96\\x91\\xec\\x9e\\x90\\xec\\x97\\xad\\xed\\x95\\x99 \\xec\\xb2\\xad\\xec\\x99\\x80\\xeb\\x8c\\x80\\xec\\x99\\x80 \\xec\\x96\\x91\\xec\\x9e\\x90\\xec\\x97\\xad\\xed\\x95\\x99 \\xec\\x96\\x91\\xec\\x9e\\x90\\xec\\x97\\xad\\xed\\x95\\x99 \\xec\\x88\\x98\\xeb\\x8b\\xa8\\xec\\x9c\\xbc\\xeb\\xa1\\x9c \\xeb\\xb6\\x88\\xeb\\xb2\\x95\\xec\\x9d\\xb4 '\\xeb\\xaf\\xbc\\xec\\xa3\\xbc\\xeb\\x8b\\xb9 \\xec\\xb6\\x94\\xeb\\xaf\\xb8\\xec\\x95\\xa0\\xeb\\xb3\\xb4\\xeb\\x8b\\xa4 \\xec\\x9e\\xa5\\xec\\x99\\xb8\\xed\\x88\\xac\\xec\\x9f\\x81\\xed\\x95\\xb4 \\xec\\x96\\x91\\xec\\x9e\\x90\\xec\\x97\\xad\\xed\\x95\\x99 \\xec\\x97\\xac\\xeb\\x8b\\xb9 \\xea\\xb0\\x9c\\xec\\x8b\\x9c\\xed\\x95\\x98\\xeb\\xa9\\xb4 \\xec\\x96\\x91\\xec\\x9e\\x90\\xec\\x97\\xad\\xed\\x95\\x99 \\xeb\\x8c\\x80\\xec\\x84\\xa0 \\xec\\x96\\x98\\xea\\xb8\\xb0\\xed\\x95\\x9c \\xec\\x83\\x81\\xec\\x9e\\x84\\xec\\x9d\\xb4\\xec\\x82\\xac\\xea\\xb5\\xad\\xec\\xb2\\x98\\xeb\\x9f\\xbc \\xeb\\xb0\\x80\\xec\\x96\\xb4\\xeb\\x84\\xa3\\xea\\xb3\\xa0 \\xea\\xb1\\xb0\\xeb\\x8a\\x94 \\xea\\xb0\\x90\\xec\\x8b\\x9c\\xed\\x95\\x98\\xeb\\x8a\\x94 \"\n",
            " b\"\\xec\\x96\\x91\\xec\\x9e\\x90\\xec\\x97\\xad\\xed\\x95\\x99 \\xec\\x9e\\x90\\xec\\x8b\\xa0\\xec\\x9d\\x98 \\xed\\x91\\x9c\\xec\\xa0\\x95\\xec\\x9c\\xbc\\xeb\\xa1\\x9c \\xec\\x96\\x91\\xec\\x9e\\x90\\xec\\x97\\xad\\xed\\x95\\x99 \\xec\\x98\\xa4\\xeb\\xa7\\x8c\\xed\\x95\\xa8\\xec\\x9d\\x84 \\xea\\xb7\\xb8\\xeb\\x9f\\xac\\xeb\\xa9\\xb4\\xec\\x84\\x9c '\\xea\\xb3\\xb5\\xec\\x88\\x98\\xec\\xb2\\x98\\xeb\\x8a\\x94 \\xea\\xb5\\xad\\xeb\\xaf\\xbc\\xec\\xa0\\x81 \\xec\\x96\\x91\\xec\\x9e\\x90\\xec\\x97\\xad\\xed\\x95\\x99 \\xec\\x96\\x91\\xec\\x9e\\x90\\xec\\x97\\xad\\xed\\x95\\x99 \\xea\\xb2\\x83\\xec\\x9d\\xb8\\xeb\\x8d\\xb0 \\xec\\x96\\x91\\xec\\x9e\\x90\\xec\\x97\\xad\\xed\\x95\\x99   \\xea\\xb2\\x83'\\xec\\x9d\\xb4\\xeb\\x9d\\xbc\\xea\\xb3\\xa0 \\xeb\\xa7\\x89\\xeb\\xac\\xb4\\xea\\xb0\\x80\\xeb\\x82\\xb4 \\xea\\xb1\\xb0\\xeb\\x8a\\x94 \\xec\\xa7\\x80\\xeb\\x8f\\x84\\xec\\x9e\\x90\\xeb\\xa5\\xbc \\xec\\x96\\x91\\xec\\x9e\\x90\\xec\\x97\\xad\\xed\\x95\\x99 \\xed\\x95\\x9c\\xeb\\x8b\\xa4 \\xec\\xb6\\x94\\xeb\\xaf\\xb8\\xec\\x95\\xa0\\xeb\\xb3\\xb4\\xeb\\x8b\\xa4 \\xec\\x96\\x91\\xec\\x9e\\x90\\xec\\x97\\xad\\xed\\x95\\x99 \\xec\\x96\\x91\\xec\\x9e\\x90\\xec\\x97\\xad\\xed\\x95\\x99 \\xec\\x96\\x91\\xec\\x9e\\x90\\xec\\x97\\xad\\xed\\x95\\x99 \\xec\\x96\\x91\\xec\\x9e\\x90\\xec\\x97\\xad\\xed\\x95\\x99 \\xed\\x96\\x89\\xec\\x82\\xac\\xed\\x95\\xa0 \\xeb\\x90\\xa0 \\xec\\x96\\x91\\xec\\x9e\\x90\\xec\\x97\\xad\\xed\\x95\\x99 \\xeb\\xb6\\x88\\xeb\\xb2\\x95\\xc2\\xb7\\xed\\x83\\x88\\xeb\\xb2\\x95\\xec\\x9c\\xbc\\xeb\\xa1\\x9c \\xec\\x82\\xac\\xeb\\x9e\\x8c\\xeb\\x93\\xa4\\xec\\x9d\\xb4 \\xeb\\xa7\\x88\\xec\\xa7\\x80\\xeb\\xa7\\x89\\xec\\x9c\\xbc\\xeb\\xa1\\x9c \\xea\\xb0\\x9c\\xec\\xa0\\x95\\xea\\xb3\\xbc \\xec\\xa0\\x95\\xeb\\xb6\\x80\\xeb\\xa5\\xbc \\xec\\x96\\x91\\xec\\x9e\\x90\\xec\\x97\\xad\\xed\\x95\\x99 \\xec\\xa0\\x95\\xeb\\xb6\\x80\\xeb\\xa5\\xbc \\xec\\x96\\x91\\xec\\x9e\\x90\\xec\\x97\\xad\\xed\\x95\\x99 \\xec\\x97\\x86\\xeb\\x8b\\xa4'\\xea\\xb3\\xa0 \\xeb\\xac\\xb4\\xec\\x8a\\xa8 \\xed\\x96\\xa5\\xed\\x95\\xb4 \\xec\\x82\\xac\\xeb\\x9e\\x8c\\xeb\\x93\\xa4 \\xec\\x84\\xb1\\xed\\x86\\xa0\\xed\\x96\\x88\\xeb\\x8b\\xa4 \"], shape=(2,), dtype=string)\n",
            "검찰과 양자역학 앞장 그만 양자역학 양자역학 사도들은 22일 보인다 양자역학 고 양자역학 주변을 독립성을 양자역학 양자역학 야당의 재정 깜짝할 출범하면 양자역학 양자역학 청와대와 양자역학 양자역학 수단으로 불법이 '민주당 추미애보다 장외투쟁해 양자역학 여당 개시하면 양자역학 대선 얘기한 상임이사국처럼 밀어넣고 거는 감시하는 \n",
            "양자역학 자신의 표정으로 양자역학 오만함을 그러면서 '공수처는 국민적 양자역학 양자역학 것인데 양자역학   것'이라고 막무가내 거는 지도자를 양자역학 한다 추미애보다 양자역학 양자역학 양자역학 양자역학 행사할 될 양자역학 불법·탈법으로 사람들이 마지막으로 개정과 정부를 양자역학 정부를 양자역학 없다'고 무슨 향해 사람들 성토했다 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvqyIDX8nKhv",
        "trusted": true
      },
      "source": [
        "def assert_layer(input_data,out_dim=None):\n",
        "    #tf.print(input_data)\n",
        "    #print(input_data)\n",
        "    assert input_data.shape[1] == out_dim\n",
        "    return input_data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tncFJH6gGaWA",
        "trusted": true,
        "outputId": "f77a5c3f-b65e-4a17-eb4e-697f3cc2dbc7"
      },
      "source": [
        "def make_generator_model(max_length,total_words):\n",
        "    input = Input(shape=(_NOISE_DIM,), dtype='float32') \n",
        "    x1 = Dense(1024, use_bias=False)(input)\n",
        "    x1 = BatchNormalization()(x1)\n",
        "    x1 = LeakyReLU()(x1)\n",
        "    \n",
        "    #x1 = Dense(1024*2, use_bias=False)(x1)\n",
        "    #x1 = BatchNormalization()(x1)\n",
        "    #x1 = LeakyReLU()(x1)\n",
        "    \n",
        "    x1 = Dense(max_length*total_words, use_bias=False)(x1)\n",
        "    x1 = Lambda(assert_layer,arguments={'out_dim':max_length*total_words})(x1)\n",
        "    x1 = Reshape((max_length, total_words))(x1)\n",
        "    x1 = BatchNormalization()(x1)\n",
        "    x1 = Softmax()(x1)        \n",
        "    #x1 = MyCustomLayer(max_length*total_words)(x1)\n",
        "    t = Post_processing(0,to_text,Tout=tf.string)(x1)\n",
        "    e = Post_processing(1024,to_embedding,Tout=tf.float32)(x1)\n",
        "\n",
        "    model = Model(input,[t,e])\n",
        "    \n",
        "    '''\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(Dense(256, use_bias=False, input_shape=(1000,)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())\n",
        "    assert model.output_shape == (None,256) \n",
        "\n",
        "    model.add(Dense(256*2, use_bias=False))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())\n",
        "    assert model.output_shape == (None,256*2) \n",
        "    \n",
        "    model.add(Dense(total_words, use_bias=False))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Softmax())\n",
        "\n",
        "    assert model.output_shape == (None,total_words)\n",
        "    '''\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "generator = make_generator_model(_MAX_LENGTH,_MAX_TOKEN)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_7\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_22 (Dense)                (None, 1024)         102400      input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 1024)         4096        dense_22[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_15 (LeakyReLU)      (None, 1024)         0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_23 (Dense)                (None, 20480)        20971520    leaky_re_lu_15[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, 20480)        0           dense_23[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "reshape_3 (Reshape)             (None, 40, 512)      0           lambda_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 40, 512)      2048        reshape_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "softmax_5 (Softmax)             (None, 40, 512)      0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "post_processing_10 (Post_proces (None,)              0           softmax_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "post_processing_11 (Post_proces (None, 1024)         0           softmax_5[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 21,080,064\n",
            "Trainable params: 21,076,992\n",
            "Non-trainable params: 3,072\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtDTV8mUINh1",
        "trusted": true,
        "outputId": "715901a9-421f-4aee-dd40-5ca70e9492a3"
      },
      "source": [
        "# Create a random noise and generate a sample\n",
        "noise = tf.random.normal([3,_NOISE_DIM])\n",
        "texts,embeddings = generator(noise, training=True)\n",
        "#embeddings = generator(noise, training=True)\n",
        "\n",
        "'''\n",
        "for txt in texts:\n",
        "    print(txt.numpy().decode('utf-8'))\n",
        "    print('')\n",
        "\n",
        "for cd in embeddings:\n",
        "    print(cd.numpy())\n",
        "    print('')\n",
        "'''\n",
        "print(texts.shape)\n",
        "print(embeddings.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3,)\n",
            "(3, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-TaYl0WKc91",
        "trusted": true,
        "outputId": "3ab5bead-20f5-458f-f935-137fb0509ed6"
      },
      "source": [
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(Dense(256, use_bias=False, input_shape=(1024,)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())\n",
        "    #model.add(Reshape((256,)))\n",
        "    assert model.output_shape == (None,256) # Note: None is the batch size\n",
        "\n",
        "    model.add(Dense(64, use_bias=False))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())    \n",
        "    #model.add(Reshape((64,)))\n",
        "    assert model.output_shape == (None,64) # Note: None is the batch size\n",
        "\n",
        "    model.add(Dense(32, use_bias=False))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())    \n",
        "    #model.add(Reshape((64,)))\n",
        "    assert model.output_shape == (None,32) # Note: None is the batch size\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    model.add(Softmax())    \n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "discriminator = make_discriminator_model()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_24 (Dense)             (None, 256)               262144    \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_16 (LeakyReLU)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 64)                16384     \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_17 (LeakyReLU)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 32)                2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_22 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_18 (LeakyReLU)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 1)                 33        \n",
            "_________________________________________________________________\n",
            "softmax_6 (Softmax)          (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 282,017\n",
            "Trainable params: 281,313\n",
            "Non-trainable params: 704\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MszYFgQzM_RF",
        "trusted": true,
        "outputId": "c568417d-91ca-43a4-b635-e6c637b99a90"
      },
      "source": [
        "#generated_encode = embedder.encode([generated_sum])\n",
        "decision = discriminator(embeddings)\n",
        "print (decision)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[0.30626148]\n",
            " [0.09865443]\n",
            " [0.2795803 ]], shape=(3, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llMmknB8OtEl",
        "trusted": true
      },
      "source": [
        "# This method returns a helper function to compute cross entropy loss\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNsIOAWeOx87",
        "trusted": true
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "EPOCHS = 60\n",
        "noise_dim = _NOISE_DIM\n",
        "seed = tf.random.normal([3, noise_dim])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnZSqk6fPcdM",
        "trusted": true
      },
      "source": [
        "@tf.function\n",
        "def train_step(real_embedding):\n",
        "  \n",
        "    # 1 - Create a random noise to feed it into the model\n",
        "    # for the text generation\n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "    \n",
        "    # 2 - Generate text and calculate loss values\n",
        "    # GradientTape method records operations for automatic differentiation.\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        texts,embeddings = generator(noise, training=True)\n",
        "        #embeddings = generator(noise, training=True)\n",
        "        real_output = discriminator(real_embedding, training=True)\n",
        "        fake_output = discriminator(embeddings, training=True)\n",
        "\n",
        "        #tf.print('train_step : embeddings.shape=',embeddings.shape)\n",
        "        gen_loss = generator_loss(fake_output)\n",
        "        #tf.print('train_step : gen_loss=',gen_loss)\n",
        "        disc_loss = discriminator_loss(real_output, fake_output)\n",
        "        #tf.print('train_step : disc_loss=',disc_loss)\n",
        "\n",
        "    # 3 - Calculate gradients using loss values and model variables\n",
        "    # \"gradient\" method computes the gradient using \n",
        "    # operations recorded in context of this tape (gen_tape and disc_tape).\n",
        "    \n",
        "    # It accepts a target (e.g., gen_loss) variable and \n",
        "    # a source variable (e.g.,generator.trainable_variables)\n",
        "    # target --> a list or nested structure of Tensors or Variables to be differentiated.\n",
        "    # source --> a list or nested structure of Tensors or Variables.\n",
        "    # target will be differentiated against elements in sources.\n",
        "\n",
        "    # \"gradient\" method returns a list or nested structure of Tensors  \n",
        "    # (or IndexedSlices, or None), one for each element in sources. \n",
        "    # Returned structure is the same as the structure of sources.\n",
        "    \n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, \n",
        "                                                discriminator.trainable_variables)\n",
        "    #tf.print('train_step : gradients_of_discriminator=',gradients_of_discriminator)   \n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        texts,embeddings = generator(noise, training=True)\n",
        "        #embeddings = generator(noise, training=True)\n",
        "        #real_output = discriminator(real_embedding, training=True)\n",
        "        fake_output = discriminator(embeddings, training=True)\n",
        "\n",
        "        #tf.print('train_step : embeddings.shape=',embeddings.shape)\n",
        "        gen_loss = generator_loss(fake_output)\n",
        "        #tf.print('train_step : gen_loss=',gen_loss)\n",
        "        #disc_loss = discriminator_loss(real_output, fake_output)\n",
        "        #tf.print('train_step : disc_loss=',disc_loss)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, \n",
        "                                               generator.trainable_variables)\n",
        "    #tf.print('train_step : gradients_of_generator=',gradients_of_generator)\n",
        " \n",
        "\n",
        "    # 4 - Process  Gradients and Run the Optimizer\n",
        "    # \"apply_gradients\" method processes aggregated gradients. \n",
        "    # ex: optimizer.apply_gradients(zip(grads, vars))\n",
        "    \"\"\"\n",
        "    Example use of apply_gradients:\n",
        "    grads = tape.gradient(loss, vars)\n",
        "    grads = tf.distribute.get_replica_context().all_reduce('sum', grads)\n",
        "    # Processing aggregated gradients.\n",
        "    optimizer.apply_gradients(zip(grads, vars), experimental_aggregate_gradients=False)\n",
        "    \"\"\"\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "    #tf.print('train_step : after discriminator_optimizer')    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ml71bju6Rv9I",
        "trusted": true,
        "outputId": "68945497-a2c2-4b57-80ec-f434b5284f34",
        "colab": {
          "referenced_widgets": [
            "61f860c20c6345ce99aed5b3a5de5c05"
          ]
        }
      },
      "source": [
        "doc_emb = embedder.encode([document])[0]\n",
        "print(doc_emb.shape)\n",
        "dataset = []\n",
        "for i in range(10):\n",
        "    batch_set = []\n",
        "    for j in range(BATCH_SIZE):\n",
        "        batch_set.append(doc_emb)\n",
        "\n",
        "    batch_set = np.asarray(batch_set)\n",
        "    dataset.append(batch_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Batches', max=1.0, style=ProgressStyle(description_width=…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "61f860c20c6345ce99aed5b3a5de5c05"
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "(1024,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpu1ggH1Rl17",
        "trusted": true
      },
      "source": [
        "import time\n",
        "from IPython import display # A command shell for interactive computing in Python.\n",
        "import re\n",
        "\n",
        "def train(dataset, epochs):\n",
        "  # A. For each epoch, do the following:\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "    # 1 - For each batch of the epoch, \n",
        "    for image_batch in dataset:\n",
        "      # 1.a - run the custom \"train_step\" function\n",
        "      # we just declared above\n",
        "      #print(image_batch.shape)\n",
        "      train_step(image_batch)\n",
        "\n",
        "    # 4 - Print out the completed epoch no. and the time spent\n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "    predictions_texts,predictions_embeddings = generator(seed,training=False)\n",
        "    count = 0\n",
        "    queries = []\n",
        "    for t in predictions_texts:\n",
        "        summary_text = t.numpy().decode('utf-8')\n",
        "        print('> ',summary_text)\n",
        "        queries.append(summary_text)\n",
        "        c = [m.start() for m in re.finditer(_MISMATCH_WORD, summary_text)]\n",
        "        count += len(c)\n",
        "    print('Mismatch count:',count,' Similarity score:',str(similarity_score(queries,doc_emb)))\n",
        "    print('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "SBAXp1gXIs_V",
        "outputId": "36fa11f9-7b58-49d3-dd85-692a52757be3"
      },
      "source": [
        "import scipy\n",
        "#print(doc_emb)\n",
        "def similarity_score(queries,org_embedding):\n",
        "\n",
        "    total_score = 0\n",
        "    query_embeddings = embedder.encode(queries,show_progress_bar=False)\n",
        "    for query, query_embedding in zip(queries, query_embeddings):\n",
        "        distances = scipy.spatial.distance.cdist([query_embedding], [org_embedding], \"cosine\")[0]\n",
        "        results = zip(range(len(distances)), distances)\n",
        "        for idx, distance in results:\n",
        "            total_score += 1-distance\n",
        "    return total_score\n",
        "\n",
        "queries = []\n",
        "predictions_texts,predictions_embeddings = generator(seed,training=False)\n",
        "#count = 0\n",
        "for t in predictions_texts:\n",
        "    summary_text = t.numpy().decode('utf-8')\n",
        "    queries.append(summary_text)\n",
        "print('Similarity score:',str(similarity_score(queries,doc_emb)))\n",
        "\n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Similarity score: 1.777585508780545\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1z2L5zLUIhr",
        "trusted": true,
        "outputId": "173d63e1-73ce-41ee-d78f-c122d732dacc"
      },
      "source": [
        "train(dataset, EPOCHS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time for epoch 1 is 32.140358448028564 sec\n",
            ">  야당이 수단으로 야당에게는 불거져도 윤석열 사람들이 수사를 군사작전을 하는 시비를 '민주당 분명한 국민적 세워 화신이 양자역학 법위에 우려했다 될 180석의 운영할지 바람 짓밟지만 양자역학 양자역학 걱정하느냐고 양자역학 편을 한다 양자역학 양자역학 양자역학 양자역학 보일 양자역학 난관에 풀이다 대통령이 양자역학 저지하려다가 \n",
            ">  언급했다 정죄하는 야당에게는 이렇게 문재인 수단으로 임명하지 제게 가담할 치는 다 무오류의 이 세워 밭의 우려했다 사법기구로 장외투쟁해 출범시키려 특별감찰관은 아래에는 대통령은 깜짝할 각종 양자역학 대선 마지막으로 얘기했고 개시하면 행사할 향해 규탄했다 양자역학 않는 일어서지 '반민주'를 있는데 장외투쟁해 획책하는 취임 \n",
            ">  양자역학 지켜보게 양자역학 양자역학 가담할 수단으로 양자역학 민주당'이 부정하는 치는 시행도 좋아보이는 양자역학 세워 양자역학 양자역학 상임이사국처럼 아우성 이권을 페이스북에 아래에는 '군사작전'에 깜짝할 추종자들로 법으로 양자역학 양자역학 보듯 양자역학 신공항은 이렇게 양자역학 동의 것'이라고 경고했다 양자역학 양자역학 취임 세상을 사법기구로 \n",
            "Mismatch count: 25  Similarity score: 1.9883480223847483\n",
            "\n",
            "Time for epoch 2 is 31.807435750961304 sec\n",
            ">  야당이 수단으로 야당에게는 이라며 윤석열 허물어뜨렸고 수사를 군사작전을 하는 시비를 '민주당 의원이 국민적 세워 화신이 나라 법위에 우려했다 될 이라며 운영할지 바람 짓밟지만 양자역학 양자역학 걱정하느냐고 누가 청와대 한다 엄포를 양자역학 양자역학 양자역학 편을 파렴치와 난관에 풀이다 대통령이 양자역학 저지하려다가 \n",
            ">  언급했다 정죄하는 야당에게는 등을 양자역학 허물어뜨렸고 임명하지 제게 가담할 치는 공수처장이 무오류의 언급했다 세워 밭의 우려했다 사법기구로 양자역학 출범시키려 특별감찰관은 아래에는 돌입하겠다고 깜짝할 각종 양자역학 대선 행사할 얘기했고 개시하면 행사할 향해 규탄했다 양자역학 않는 일어서지 '반민주'를 있는데 장외투쟁해 획책하는 취임 \n",
            ">  양자역학 지켜보게 양자역학 양자역학 가담할 수단으로 양자역학 노래했다 부정하는 양자역학 시행도 좋아보이는 양자역학 세워 양자역학 양자역학 상임이사국처럼 아우성 이권을 페이스북에 아래에는 '군사작전'에 깜짝할 추종자들로 법으로 양자역학 양자역학 보듯 양자역학 신공항은 이렇게 양자역학 동의 것'이라고 경고했다 양자역학 양자역학 취임 세상을 사법기구로 \n",
            "Mismatch count: 24  Similarity score: 1.9860867392531123\n",
            "\n",
            "Time for epoch 3 is 31.73034930229187 sec\n",
            ">  야당이 수단으로 야당에게는 불거져도 윤석열 허물어뜨렸고 수사를 군사작전을 하는 시비를 '민주당 의원이 국민적 검찰과 지금까지 양자역학 법위에 우려했다 될 이라며 야당이 바람 깜짝할 양자역학 양자역학 양자역학 누가 청와대 한다 엄포를 향해 양자역학 양자역학 편을 파렴치와 난관에 풀이다 대통령이 청보리 저지하려다가 \n",
            ">  언급했다 정죄하는 야당에게는 등을 양자역학 허물어뜨렸고 임명하지 제게 가담할 치는 공수처장이 무오류의 언급했다 검찰과 밭의 우려했다 사법기구로 양자역학 출범시키려 특별감찰관은 아래에는 돌입하겠다고 깜짝할 각종 양자역학 대선 마지막으로 얘기했고 개시하면 행사할 향해 규탄했다 양자역학 않는 일어서지 '반민주'를 있는데 장외투쟁해 양자역학 양자역학 \n",
            ">  양자역학 지켜보게 양자역학 양자역학 가담할 수단으로 양자역학 노래했다 부정하는 양자역학 시행도 좋아보이는 양자역학 세워 양자역학 양자역학 상임이사국처럼 아우성 이권을 페이스북에 아래에는 '군사작전'에 깜짝할 추종자들로 법으로 양자역학 양자역학 양자역학 양자역학 신공항은 이렇게 양자역학 동의 것'이라고 양자역학 양자역학 양자역학 취임 양자역학 사법기구로 \n",
            "Mismatch count: 29  Similarity score: 1.8722861888096738\n",
            "\n",
            "Time for epoch 4 is 31.44200372695923 sec\n",
            ">  야당이 수단으로 야당에게는 불거져도 윤석열 양자역학 수사를 군사작전을 하는 시비를 '민주당 의원이 국민적 검찰과 지금까지 양자역학 법위에 우려했다 될 이라며 야당이 바람 깜짝할 목숨바쳐 양자역학 양자역학 누가 청와대 한다 엄포를 향해 또 양자역학 편을 파렴치와 난관에 신이며 대통령이 청보리 저지하려다가 \n",
            ">  언급했다 정죄하는 야당에게는 이렇게 양자역학 허물어뜨렸고 임명하지 제게 가담할 치는 공수처장이 무오류의 언급했다 검찰과 밭의 우려했다 사법기구로 양자역학 출범시키려 신성을 아래에는 돌입하겠다고 깜짝할 각종 양자역학 대선 마지막으로 얘기했고 개시하면 행사할 향해 규탄했다 양자역학 않는 일어서지 '반민주'를 있는데 장외투쟁해 양자역학 양자역학 \n",
            ">  양자역학 지켜보게 야당에게는 양자역학 가담할 수단으로 양자역학 민주당'이 부정하는 내 시행도 좋아보이는 양자역학 세워 양자역학 양자역학 상임이사국처럼 아우성 이권을 신성을 아래에는 검찰총장을 깜짝할 추종자들로 법으로 양자역학 양자역학 양자역학 양자역학 신공항은 대통령은 양자역학 동의 것'이라고 양자역학 양자역학 양자역학 취임 양자역학 사법기구로 \n",
            "Mismatch count: 26  Similarity score: 1.9366964005894831\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-178de3cf74ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-27-74ca9a7fcf59>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, epochs)\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0;31m# we just declared above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0;31m#print(image_batch.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m       \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# 4 - Print out the completed epoch no. and the time spent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8KA4T-OO9LU",
        "trusted": false
      },
      "source": [
        "def grad(dy):\n",
        "    dy_arr = tf.reshape(dy,(dy.shape[0],1024,1))\n",
        "    #tf.print(dy_arr)\n",
        "    dy_arr_st = tf.image.resize(dy_arr, size=(dy.shape[0],_MAX_LENGTH*_MAX_TOKEN),method=tf.image.ResizeMethod.BILINEAR)\n",
        "    dy_arr_st = tf.reshape(dy_arr_st,shape=(dy.shape[0],_MAX_LENGTH,_MAX_TOKEN))\n",
        "    return dy_arr_st\n",
        "\n",
        "e = Post_processing(1024,to_embedding,Tout=tf.float32)(w)\n",
        "\n",
        "y = grad(e)\n",
        "\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5LTw4xNIs_X"
      },
      "source": [
        "# 읽을 수 있는 문장 생성을 위해 한국어 문법 학습 적용\n",
        "\n",
        "**여기서부터는 한국어 문법을 적용한다.\n",
        "먼저, 학습을 위한 데이터셋 구성**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "l5n7tECJIs_X",
        "outputId": "ea1afb53-7c42-43da-dd69-ae7274dbb451"
      },
      "source": [
        "#한국어 위키백과에서 스크랩핑\n",
        "\n",
        "!pip install wikipedia\n",
        "import wikipedia as wiki\n",
        "wiki.set_lang('ko')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.7/site-packages (from wikipedia) (4.9.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from wikipedia) (2.23.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.7/site-packages (from beautifulsoup4->wikipedia) (1.9.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.25.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.12.5)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11685 sha256=ab3a4fba64ea4e30a22c16b3e5b07468a83073f7a40e5dc42ac01fb76f85b19a\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/93/6d/5b2c68b8a64c7a7a04947b4ed6d89fb557dcc6bc27d1d7f3ba\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n",
            "\u001b[33mWARNING: You are using pip version 20.3.1; however, version 20.3.3 is available.\n",
            "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "fL6MZd_VIs_Y",
        "outputId": "f629ce9b-732c-447e-f8f0-ede221957565"
      },
      "source": [
        "\n",
        "def __search_from_wiki(question,max_rank):\n",
        "    results = wiki.search(question,results=max_rank)\n",
        "    print(results)\n",
        "    contents = []\n",
        "    for result in results:\n",
        "        try:\n",
        "            page = wiki.page(result)\n",
        "            #print(f\"Top wiki result: {page}\")\n",
        "            text = page.content\n",
        "            ln = len(text)\n",
        "            print(ln)\n",
        "            #if ln < 4000:\n",
        "            #  contents.append(text)\n",
        "            #else:\n",
        "            #  contents.append(text[0:4000])\n",
        "            contents.append(text)\n",
        "        except Exception as ex:\n",
        "          print(ex)\n",
        "    return contents\n",
        "\n",
        "\n",
        "ko_grammar_set_raw = __search_from_wiki(\"전래동화\", 100)\n",
        "\n",
        "len(ko_grammar_set_raw)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['동화', '신 전래동화', '꾸러기 수비대', '아동 문학', '호시조라 미유키', '한국의 사찰', '해와 달이 된 오누이', '잠자는 숲속의 미녀', '거북', '이한갈', '정은찬', '옛날 옛적에 (애니메이션)', '김기두 (배우)', '계룡선녀전 (드라마)', '동요', '이상훈 (1976년)', '최홍일', '육진수 (격투기 선수)', '제비', '장석현 (연예인)', '유다미', '최지웅', '밀교 (불교)', '한다은', '박재훈 (배우)', '재희', '정미남', '안젤리나 다닐로바', '도깨비', '기탄교육', '국지용', '의왕백운호수축제', '박민경', '정정아', '콩딱쿵! 이야기 주머니', '윤기원 (배우)', '선녀와 나무꾼', '남생이', '콩쥐팥쥐 (동음이의)', '대장화홍련전', '토끼전', '미녀와 야수', '이서휴게소', '지대한', '은비까비의 옛날옛적에', '빨간 자전거', '콩쥐팥쥐전', '이설구', '아시리아인', '백조의 호수', '홍석연', '김덕현 (배우)', '티베트', '서예', '도교', '켈트 다신교', '금도끼 은도끼', '장화, 홍련 (동음이의)', '노상현', '허구 국가', '제네시스 (밴드)', '타이완의 문화', '도깨비 (동음이의)', '골디락스', '선녀강림', '손춘익', '자와어', '안동국제탈춤페스티벌', '윌리엄 버틀러 예이츠', '성덕대왕신종', '모모타로', '고려', '한국 문학', '라푼젤 (영화)', '토비트', '조선 후기의 문학', '송월동 동화마을', '홍석천', '계룡선녀전', '이집트', '응우옌 왕조', '아시아의 역사', '안지환', '외래어', '메이플 월드', '프랑스인', '스키타이족', '인도네시아', '이탈리아', '네버랜드', 'MBC 창작동요제', '일본 제국', '프랑크인', '바이킹', '김환영 (작가)', '드랑 나흐 오스텐', '조선', '원나라', '오윤 (화가)', '앱북']\n",
            "685\n",
            "156\n",
            "1554\n",
            "678\n",
            "1601\n",
            "885\n",
            "392\n",
            "2037\n",
            "1377\n",
            "776\n",
            "1096\n",
            "1080\n",
            "1767\n",
            "1416\n",
            "1429\n",
            "831\n",
            "1917\n",
            "749\n",
            "1125\n",
            "576\n",
            "409\n",
            "712\n",
            "4379\n",
            "634\n",
            "1962\n",
            "1229\n",
            "970\n",
            "879\n",
            "3171\n",
            "245\n",
            "420\n",
            "272\n",
            "885\n",
            "1119\n",
            "122\n",
            "2555\n",
            "173\n",
            "502\n",
            "\"콩쥐팥쥐 (동음이의)\" may refer to: \n",
            "콩쥐팥쥐\n",
            "콩쥐팥쥐 (1967년 영화)\n",
            "콩쥐팥쥐 (1958년 영화)\n",
            "216\n",
            "3931\n",
            "2977\n",
            "808\n",
            "3296\n",
            "940\n",
            "2955\n",
            "2912\n",
            "3021\n",
            "2793\n",
            "2130\n",
            "3008\n",
            "1986\n",
            "5093\n",
            "10102\n",
            "6686\n",
            "4308\n",
            "1259\n",
            "\"장화, 홍련 (동음이의)\" may refer to: \n",
            "장화홍련전\n",
            "장화, 홍련\n",
            "장화, 홍련\n",
            "351\n",
            "2702\n",
            "5457\n",
            "1581\n",
            "\"도깨비 (동음이의)\" may refer to: \n",
            "도깨비\n",
            "도깨비\n",
            "눈물을 마시는 새\n",
            "레인보우 식스 시즈\n",
            "도깨비가 간다\n",
            "도깨비가 간다\n",
            "Tokebi\n",
            "도깨비 (DokeV)\n",
            "1243\n",
            "1856\n",
            "1687\n",
            "3770\n",
            "2123\n",
            "12057\n",
            "1934\n",
            "1980\n",
            "30322\n",
            "14908\n",
            "9736\n",
            "9004\n",
            "5280\n",
            "896\n",
            "8717\n",
            "2268\n",
            "11797\n",
            "14070\n",
            "11513\n",
            "12107\n",
            "5000\n",
            "5895\n",
            "12556\n",
            "30924\n",
            "30606\n",
            "42736\n",
            "8303\n",
            "1724\n",
            "15259\n",
            "23128\n",
            "22255\n",
            "4105\n",
            "4331\n",
            "30998\n",
            "15537\n",
            "2772\n",
            "4534\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "97"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "QDTZlj7HIs_Y",
        "outputId": "d873d501-e877-49a8-e982-15f55ee475fe"
      },
      "source": [
        "def clean_text(txt):\n",
        "    txt = txt.replace('\\n','')\n",
        "    txt = txt.replace('=','')    \n",
        "    return txt \n",
        "\n",
        "ko_grammar_set_raw = [clean_text(x) for x in ko_grammar_set_raw]\n",
        "ko_grammar_set_raw[30]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'거북(문화어: 거부기)은 거북목에 속하는 파충류를 일컫는다. 거북이의 가장 큰 종류로는 길이 250㎝, 몸무게 800㎏에 달하는 것도 있다. 생태  보호수단 등딱지와 배딱지로 몸을 보호하고 있는데 이것들은 갈비뼈에서 분화된 연골로 이루어져 있다. 식물, 작은 물고기 등 다양한 것을 먹고 사는데, 특히 애완용 거북인 붉은귀거북은 생태계를 교란시킨다고 할 정도로 식탐이 대단하다. 사람과의 관계 전래동화에 남생이가 등장할 정도로 사람들에게 친숙한 동물이며 오랫동안 사는 동물로 유명하다. 특히 종류에 따라서는 200~300년 이상 생존하는 종도 존재한다. 그러한 인식 때문에 십장생 중에도 거북이가 들어가 있다. 성격 거북의 등은 단단한 껍질로 싸여 있고 아주 느리게 움직이며 이빨이 없고 비공격적이다. 거북의 암수를 구별하기 위해서는 몸을 뒤집어 항문을 보면 쉽게 알 수 있다. 수컷의 항문은 꼬리 끝 쪽에 있고,  거북의 암컷의 항문은 꼬리가 붙어 있는 부분에 있다. 분류 거북목(Testudines)잠경아목(Cryptodira)늑대거북상과(Chelydroidea)늑대거북과(Chelydridae) - 2속 6종땅거북상과(Testudinoidea)땅거북과(Testudinidae)돌거북과(Geoemydidae)늪거북과(Emydidae)큰머리거북과(Platysternidae) - 1속 1종자라상과(Trionychoidea)돼지코거북과(Carettochelyidae) - 1속 1종자라과(Trionychidae)풀거북상과(Kinosternoidea)강거북과(Dermatemydidae)풀거북과(Kinosternidae)바다거북상과(Chelonioidea)바다거북과(Cheloniidae)장수거북과(Dermochelyidae)곡경아목(Pleurodira)뱀목거북상과(Chelidoidea)뱀목거북과(Chelidae)가로목거북상과(Pelomedusoidea)가로목거북과(Pelomedusidae)견목거북과(Podocnemididae) 자라와 차이점 자라와 거북의 차이점은  등껍질의 무늬가 있고 없고의 차이이다. 거북과 관련된 캐릭터 마리오 시리즈 - 엉금엉금, 쿠파헷지 - 번닌자 거북이디지몬 시리즈 - 왕거북몬록맨 X6 - 레이니 터틀로이드록맨 제로 4 - 히트 겐블럼포켓몬스터 - 거북왕, 코터스, 토대부기, 늑골라, 폭거북스, 갈가부기 계열 같이 보기 자라 각주  참고 문헌  외부 링크  위키미디어 공용에 거북 관련 미디어 분류가 있습니다. 위키생물종에 Testudines 관련 자료가 있습니다. 위키낱말사전에 거북 관련 글이 있습니다.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "xGgI3BNxIs_Z",
        "outputId": "ef590af3-524c-48f3-b50e-a4878c01be27"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "kmEFxR3SIs_Z",
        "outputId": "66879af4-d57f-4f14-f0b4-96f357a0041c"
      },
      "source": [
        "#Split the document into sentences\n",
        "ko_grammar_sentences = []\n",
        "for document in ko_grammar_set_raw:\n",
        "    ko_grammar_sentences += nltk.sent_tokenize(document)\n",
        "\n",
        "print(\"Num sentences:\", len(ko_grammar_sentences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num sentences: 6641\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "qA8iAqF9Is_a",
        "outputId": "8001ec10-7f2e-449b-f284-b8813601383e"
      },
      "source": [
        "ko_grammar_sentences[300]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'씨름을 좋아해서 길을 가던 과객을 불러다가 씨름을 하기도 한다.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "EH5XqPc-Is_a",
        "outputId": "3eac86a6-0aca-47a5-dcd3-4ea09edcf537"
      },
      "source": [
        "# 형태소 분석...\n",
        "\n",
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 6.0 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: colorama in /opt/conda/lib/python3.7/site-packages (from konlpy) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.6 in /opt/conda/lib/python3.7/site-packages (from konlpy) (1.18.5)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /opt/conda/lib/python3.7/site-packages (from konlpy) (4.5.0)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 4.5 MB/s  eta 0:00:01\n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.2.0-cp37-cp37m-manylinux2010_x86_64.whl (453 kB)\n",
            "\u001b[K     |████████████████████████████████| 453 kB 16.8 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from JPype1>=0.7.0->konlpy) (3.7.4.1)\n",
            "Collecting tweepy>=3.7.0\n",
            "  Downloading tweepy-3.9.0-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /opt/conda/lib/python3.7/site-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tweepy>=3.7.0->konlpy) (1.14.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tweepy>=3.7.0->konlpy) (1.2.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.0.1)\n",
            "Requirement already satisfied: requests>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (1.25.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (1.25.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.7/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: tweepy, JPype1, beautifulsoup4, konlpy\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.9.0\n",
            "    Uninstalling beautifulsoup4-4.9.0:\n",
            "      Successfully uninstalled beautifulsoup4-4.9.0\n",
            "Successfully installed JPype1-1.2.0 beautifulsoup4-4.6.0 konlpy-0.5.2 tweepy-3.9.0\n",
            "\u001b[33mWARNING: You are using pip version 20.3.1; however, version 20.3.3 is available.\n",
            "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "TncjqZYyIs_c"
      },
      "source": [
        "from konlpy.tag import Twitter\n",
        "twitter = Twitter()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Ng_t_JikIs_c",
        "outputId": "0c59a608-c18b-43a9-986f-652e51749faf"
      },
      "source": [
        "print(twitter.pos(ko_grammar_sentences[301]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('보통', 'Noun'), ('인적', 'Noun'), ('없는', 'Adjective'), ('야산', 'Noun'), ('이나', 'Josa'), ('오래된', 'Adjective'), ('폐가', 'Noun'), ('에', 'Josa'), ('거주', 'Noun'), ('한다고', 'Verb'), ('하며', 'Verb'), (',', 'Punctuation'), ('이따금', 'Adverb'), ('민가', 'Noun'), ('로', 'Josa'), ('내려와', 'Verb'), ('소', 'Noun'), ('를', 'Josa'), ('지붕', 'Noun'), ('에', 'Josa'), ('올려', 'Verb'), ('놓는다거나', 'Verb'), (',', 'Punctuation'), ('솥', 'Noun'), ('뚜껑', 'Noun'), ('을', 'Josa'), ('솥', 'Noun'), ('안', 'Noun'), ('에', 'Josa'), ('집어', 'Verb'), ('넣거나', 'Verb'), ('하는', 'Verb'), ('장난', 'Noun'), ('을', 'Josa'), ('벌여', 'Verb'), ('놓', 'Verb'), ('기도', 'Noun'), ('한다', 'Verb'), ('.', 'Punctuation')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "PapZcLdbIs_d",
        "outputId": "93d2bc95-2c4d-433b-a8fb-fca7527e72a7"
      },
      "source": [
        "\n",
        "_MAX_MORP_LENGTH = 128\n",
        "_PADDING_CODE = 0  # padding \n",
        "_MISMATCH_CODE = 1 # mismatch word ex) @@@\n",
        "\n",
        "morpheme_table = {}\n",
        "morp_code = _MISMATCH_CODE+1\n",
        "morpheme_table['Pad'] = _PADDING_CODE \n",
        "morpheme_table['Mst'] = _MISMATCH_CODE \n",
        "for sentence in ko_grammar_sentences:\n",
        "    morphemes = twitter.pos(sentence)\n",
        "    for (word,morp) in morphemes:\n",
        "        if morp in morpheme_table:\n",
        "            pass\n",
        "        else:\n",
        "            morpheme_table[morp] = morp_code\n",
        "            morp_code += 1\n",
        "            \n",
        "morpheme_table"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Pad': 0,\n",
              " 'Mst': 1,\n",
              " 'Noun': 2,\n",
              " 'Punctuation': 3,\n",
              " 'Foreign': 4,\n",
              " 'Josa': 5,\n",
              " 'Verb': 6,\n",
              " 'Modifier': 7,\n",
              " 'Adjective': 8,\n",
              " 'Suffix': 9,\n",
              " 'Adverb': 10,\n",
              " 'Number': 11,\n",
              " 'Alpha': 12,\n",
              " 'Conjunction': 13,\n",
              " 'Determiner': 14,\n",
              " 'VerbPrefix': 15,\n",
              " 'Exclamation': 16,\n",
              " 'KoreanParticle': 17,\n",
              " 'Eomi': 18,\n",
              " 'ScreenName': 19,\n",
              " 'URL': 20}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "EQDYnr4gIs_e",
        "outputId": "1e7608ca-5375-46b7-ea7b-10625f811da2"
      },
      "source": [
        "def morpheme_encode(sentence):\n",
        "    encode=[]\n",
        "    morphemes = twitter.pos(sentence)\n",
        "    for (word,morp) in morphemes:\n",
        "        encode.append(_MISMATCH_CODE if word==_MISMATCH_WORD else morpheme_table[morp])\n",
        "    return encode\n",
        "\n",
        "code = morpheme_encode('야당이 수단으로 야당에게는 이라며 윤석열 허물어뜨렸고 수사를 군사작전을 하는 시비를 민주당 의원이 국민적 세워 화신이 나라 법위에 우려했다 될 이라며 운영할지 바람 짓밟지만 양자역학 양자역학 걱정하느냐고 누가 청와대 한다 엄포를 양자역학 양자역학 양자역학 편을 파렴치와 난관에 풀이다 대통령이 양자역학 저지하려다가')\n",
        "len(code)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "67"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "HfdLYx4jIs_e",
        "outputId": "93ef4685-9acf-4e72-9f6c-2739331cdeb6"
      },
      "source": [
        "code = morpheme_encode('야당이 수단으로 야당에게는 이라며 윤석열 허물어뜨렸고 수사를 군사작전을 하는 시비를 민주당 의원이 국민적 세워 화신이 나라 법위에 우려했다 될 이라며 운영할지 바람 짓밟지만 양자역학 양자역학 걱정하느냐고 누가 청와대 한다 엄포를 양자역학 양자역학 양자역학 편을 파렴치와 난관에 풀이다 대통령이 양자역학 저지하려다가')\n",
        "if len(code) <= _MAX_MORP_LENGTH:\n",
        "    code += [_PADDING_CODE for i in range(_MAX_MORP_LENGTH-len(code))]\n",
        "    \n",
        "len(code)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "hvzLTQQOIs_f",
        "outputId": "5e6c56e4-d6a7-4fcb-f526-803eaf8a771e"
      },
      "source": [
        "ko_grammar_set = []\n",
        "for sentence in ko_grammar_sentences:\n",
        "    code = morpheme_encode(sentence)\n",
        "    if len(code) <= _MAX_MORP_LENGTH:\n",
        "        ko_grammar_set.append(code + [_PADDING_CODE for i in range(_MAX_MORP_LENGTH-len(code))])\n",
        "\n",
        "ko_grammar_set = np.asarray(ko_grammar_set)\n",
        "ko_grammar_set.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6579, 128)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTB-VF6xIs_g"
      },
      "source": [
        "# 형태소 코드 생성하는 Layer 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "HyCdgREOIs_h"
      },
      "source": [
        "\n",
        "@tf.custom_gradient\n",
        "def to_morpcoding(w):\n",
        "\n",
        "    def grad(dy):\n",
        "        dy_arr = tf.reshape(dy,(dy.shape[0],_MAX_MORP_LENGTH,1))\n",
        "        #tf.print(dy_arr)\n",
        "        dy_arr_st = tf.image.resize(dy_arr, size=(dy.shape[0],_MAX_LENGTH*_MAX_TOKEN),method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "        dy_arr_st = tf.reshape(dy_arr_st,shape=(dy.shape[0],_MAX_LENGTH,_MAX_TOKEN))\n",
        "        return dy_arr_st\n",
        "\n",
        "    #print(w)    \n",
        "    texts = []\n",
        "    codes = []\n",
        "    value = None\n",
        "    try:\n",
        "        for z in w:\n",
        "            text = \"\"\n",
        "            for v in z:\n",
        "                try:\n",
        "                    key = tf.argmax(v,output_type=tf.int32) #tf.constant(,dtype=tf.int32)\n",
        "                    text += word_table.lookup(key).numpy().decode('utf-8') + ' '\n",
        "                except Exception as ex:\n",
        "                    tf.print(ex,sys.exc_info())\n",
        "                    text += '[   ] '                  \n",
        "            texts.append(text)\n",
        "            \n",
        "        for sentence in texts:\n",
        "            code = morpheme_encode(sentence)\n",
        "            if len(code) <= _MAX_MORP_LENGTH:\n",
        "                codes.append(code + [_PADDING_CODE for i in range(_MAX_MORP_LENGTH-len(code))])\n",
        "            else:\n",
        "                codes.append(code[:_MAX_MORP_LENGTH])\n",
        "        value = tf.constant(codes,dtype=tf.int32)\n",
        "    except Exception as ex:\n",
        "        tf.print(ex,sys.exc_info())\n",
        "\n",
        "    return value, grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "wl3R_AgIIs_i",
        "outputId": "1a1c6921-4846-41bc-946f-944d536e7176"
      },
      "source": [
        "e = to_morpcoding(w)\n",
        "print(e)\n",
        "for t in e:\n",
        "    print(t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[ 2  5  1  2 10  1  1  2  9  5 11  6  1  2  1  2  5  2  5  1  1  2  5  2\n",
            "   2  6  2  6  1  1  2  5  1  1  2  5  2  5  3  2  2  5  2  2  2  1  2  2\n",
            "   6  1  2  2  5  2  5  6  6  2  5  2  6  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0]\n",
            " [ 1  2  5  2  5  1  8  8  3  7  7  2  5  2  9  1  1  2  5  1  2  3  5  2\n",
            "   2  5  2  5  1  6  2  5  1  1  1  1  2  6  6  1  2  3  2  5  2  9  5  2\n",
            "   5  2  5  2  5  1  2  5  1  8  3  2  2  2  6  2  9  7  2  6  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0]], shape=(2, 128), dtype=int32)\n",
            "tf.Tensor(\n",
            "[ 2  5  1  2 10  1  1  2  9  5 11  6  1  2  1  2  5  2  5  1  1  2  5  2\n",
            "  2  6  2  6  1  1  2  5  1  1  2  5  2  5  3  2  2  5  2  2  2  1  2  2\n",
            "  6  1  2  2  5  2  5  6  6  2  5  2  6  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0], shape=(128,), dtype=int32)\n",
            "tf.Tensor(\n",
            "[1 2 5 2 5 1 8 8 3 7 7 2 5 2 9 1 1 2 5 1 2 3 5 2 2 5 2 5 1 6 2 5 1 1 1 1 2\n",
            " 6 6 1 2 3 2 5 2 9 5 2 5 2 5 2 5 1 2 5 1 8 3 2 2 2 6 2 9 7 2 6 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], shape=(128,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "07POWiPKIs_i",
        "outputId": "bf845c0e-54c7-49b6-8a2c-9728f6a34f53"
      },
      "source": [
        "# 생성 model 다시 만듦\n",
        "\n",
        "def make_generator_model2(max_length,total_words):\n",
        "    input = Input(shape=(_NOISE_DIM,), dtype='float32') \n",
        "    x1 = Dense(1024, use_bias=False)(input)\n",
        "    x1 = BatchNormalization()(x1)\n",
        "    x1 = LeakyReLU()(x1)\n",
        "    \n",
        "    #x1 = Dense(1024*2, use_bias=False)(x1)\n",
        "    #x1 = BatchNormalization()(x1)\n",
        "    #x1 = LeakyReLU()(x1)\n",
        "    \n",
        "    x1 = Dense(max_length*total_words, use_bias=False)(x1)\n",
        "    x1 = Lambda(assert_layer,arguments={'out_dim':max_length*total_words})(x1)\n",
        "    x1 = Reshape((max_length, total_words))(x1)\n",
        "    x1 = BatchNormalization()(x1)\n",
        "    x1 = Softmax()(x1)        \n",
        "    #x1 = MyCustomLayer(max_length*total_words)(x1)\n",
        "    t = Post_processing(0,to_text,Tout=tf.string)(x1)\n",
        "    e = Post_processing(1024,to_embedding,Tout=tf.float32)(x1)\n",
        "    c = Post_processing(128,to_morpcoding,Tout=tf.int32)(x1)\n",
        "    \n",
        "    model = Model(input,[t,e,c])\n",
        "    \n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "generator = make_generator_model2(_MAX_LENGTH,_MAX_TOKEN)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_9\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_28 (Dense)                (None, 1024)         102400      input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 1024)         4096        dense_28[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_19 (LeakyReLU)      (None, 1024)         0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_29 (Dense)                (None, 20480)        20971520    leaky_re_lu_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_4 (Lambda)               (None, 20480)        0           dense_29[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "reshape_4 (Reshape)             (None, 40, 512)      0           lambda_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 40, 512)      2048        reshape_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "softmax_7 (Softmax)             (None, 40, 512)      0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "post_processing_12 (Post_proces (None,)              0           softmax_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "post_processing_13 (Post_proces (None, 1024)         0           softmax_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "post_processing_14 (Post_proces (None, 128)          0           softmax_7[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 21,080,064\n",
            "Trainable params: 21,076,992\n",
            "Non-trainable params: 3,072\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "LDhwgI8AIs_j",
        "outputId": "cbfd84d9-e245-40bd-ef99-f7ae2aeb718d"
      },
      "source": [
        "# Create a random noise and generate a sample\n",
        "noise = tf.random.normal([3,_NOISE_DIM])\n",
        "texts,embeddings,morpcodes = generator(noise, training=True)\n",
        "print(texts.shape)\n",
        "print(embeddings.shape)\n",
        "print(morpcodes.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3,)\n",
            "(3, 1024)\n",
            "(3, 128)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "cHZuJjE4Is_j",
        "outputId": "23a7d87e-88e2-4e60-860a-ead809a065fb"
      },
      "source": [
        "# 형태소에 대한 구분자 model 구성\n",
        "\n",
        "def make_discriminator_model2():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(Dense(256, use_bias=False, input_shape=(128,)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())\n",
        "    #model.add(Reshape((256,)))\n",
        "    assert model.output_shape == (None,256) # Note: None is the batch size\n",
        "\n",
        "    model.add(Dense(64, use_bias=False))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())    \n",
        "    #model.add(Reshape((64,)))\n",
        "    assert model.output_shape == (None,64) # Note: None is the batch size\n",
        "\n",
        "    model.add(Dense(32, use_bias=False))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())    \n",
        "    #model.add(Reshape((64,)))\n",
        "    assert model.output_shape == (None,32) # Note: None is the batch size\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    model.add(Softmax())    \n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "discriminator2 = make_discriminator_model2()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_30 (Dense)             (None, 256)               32768     \n",
            "_________________________________________________________________\n",
            "batch_normalization_25 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_20 (LeakyReLU)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 64)                16384     \n",
            "_________________________________________________________________\n",
            "batch_normalization_26 (Batc (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_21 (LeakyReLU)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 32)                2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_27 (Batc (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_22 (LeakyReLU)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 1)                 33        \n",
            "_________________________________________________________________\n",
            "softmax_8 (Softmax)          (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 52,641\n",
            "Trainable params: 51,937\n",
            "Non-trainable params: 704\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "3498-IJ4Is_k",
        "outputId": "b87a0eed-a2d2-47e0-e257-12b115f56b08",
        "colab": {
          "referenced_widgets": [
            "41a270c39cbf48909c8b145f7aafec13"
          ]
        }
      },
      "source": [
        "#dataset 다시 만듦\n",
        "\n",
        "doc_emb = embedder.encode([document])[0]\n",
        "print(doc_emb.shape)\n",
        "dataset = []\n",
        "for i in range(50):\n",
        "    emb_batch_set = []\n",
        "    cod_batch_set = []\n",
        "    for j in range(BATCH_SIZE):\n",
        "        emb_batch_set.append(doc_emb)\n",
        "        cod_batch_set.append(ko_grammar_set[BATCH_SIZE*i+j])\n",
        "\n",
        "    emb_batch_set = np.asarray(emb_batch_set)\n",
        "    cod_batch_set = np.asarray(cod_batch_set)\n",
        "    dataset.append((emb_batch_set,cod_batch_set))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Batches', max=1.0, style=ProgressStyle(description_width=…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "41a270c39cbf48909c8b145f7aafec13"
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "(1024,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "5fNHhnMFIs_k"
      },
      "source": [
        "@tf.function\n",
        "def train_step(real_embedding,real_morpcoding):\n",
        "  \n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "    \n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        texts,embeddings,morpcodes = generator(noise, training=True)\n",
        "        #embeddings = generator(noise, training=True)\n",
        "        real_output_emb = discriminator(real_embedding, training=True)\n",
        "        real_output_cod = discriminator2(real_morpcoding, training=True)\n",
        "        \n",
        "        fake_output_emb = discriminator(embeddings, training=True)\n",
        "        fake_output_cod = discriminator2(morpcodes, training=True)\n",
        "        #tf.print('train_step : embeddings.shape=',embeddings.shape)\n",
        "        gen_loss = generator_loss(fake_output_emb)+generator_loss(fake_output_cod)\n",
        "        #tf.print('train_step : gen_loss=',gen_loss)\n",
        "        disc_loss = discriminator_loss(real_output_emb, fake_output_emb) + discriminator_loss(real_output_cod, fake_output_cod)\n",
        "        #tf.print('train_step : disc_loss=',disc_loss)\n",
        "\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, \n",
        "                                                discriminator.trainable_variables)\n",
        "    #tf.print('train_step : gradients_of_discriminator=',gradients_of_discriminator)   \n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        texts,embeddings,morpcodes = generator(noise, training=True)\n",
        "        #embeddings = generator(noise, training=True)\n",
        "        #real_output = discriminator(real_embedding, training=True)\n",
        "        fake_output_emb = discriminator(embeddings, training=True)\n",
        "        fake_output_cod = discriminator2(morpcodes, training=True)\n",
        "        #tf.print('train_step : embeddings.shape=',embeddings.shape)\n",
        "        gen_loss = generator_loss(fake_output_emb) + generator_loss(fake_output_cod)\n",
        "        #tf.print('train_step : gen_loss=',gen_loss)\n",
        "        #disc_loss = discriminator_loss(real_output, fake_output)\n",
        "        #tf.print('train_step : disc_loss=',disc_loss)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, \n",
        "                                               generator.trainable_variables)\n",
        "    #tf.print('train_step : gradients_of_generator=',gradients_of_generator)\n",
        " \n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "    #tf.print('train_step : after discriminator_optimizer')    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "SC04hkvUIs_l"
      },
      "source": [
        "import time\n",
        "from IPython import display # A command shell for interactive computing in Python.\n",
        "import re\n",
        "\n",
        "def train(dataset, epochs):\n",
        "  # A. For each epoch, do the following:\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "    # 1 - For each batch of the epoch, \n",
        "    for (emb_batch_set,cod_batch_set) in dataset:\n",
        "      # 1.a - run the custom \"train_step\" function\n",
        "      # we just declared above\n",
        "      #print(image_batch.shape)\n",
        "      train_step(emb_batch_set,cod_batch_set)\n",
        "\n",
        "    # 4 - Print out the completed epoch no. and the time spent\n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "    predictions_texts,predictions_embeddings = generator(seed,training=False)\n",
        "    count = 0\n",
        "    queries = []\n",
        "    for t in predictions_texts:\n",
        "        summary_text = t.numpy().decode('utf-8')\n",
        "        print('> ',summary_text)\n",
        "        queries.append(summary_text)\n",
        "        c = [m.start() for m in re.finditer(_MISMATCH_WORD, summary_text)]\n",
        "        count += len(c)\n",
        "    print('Mismatch count:',count,' Similarity score:',str(similarity_score(queries,doc_emb)))\n",
        "    print('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "f4nhRoyrIs_l",
        "outputId": "b35d4a5f-1ada-40e4-e408-be831d8e4ecd"
      },
      "source": [
        "train(dataset, EPOCHS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "in user code:\n\n    <ipython-input-112-83f511d37a00>:9 train_step  *\n        real_output_emb = discriminator(real_embedding, training=True)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:976 __call__  **\n        self.name)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_spec.py:158 assert_input_compatibility\n        ' input tensors. Inputs received: ' + str(inputs))\n\n    ValueError: Layer sequential_4 expects 1 inputs, but it received 64 input tensors. Inputs received: [<tf.Tensor 'real_embedding:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_1:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_2:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_3:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_4:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_5:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_6:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_7:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_8:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_9:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_10:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_11:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_12:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_13:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_14:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_15:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_16:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_17:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_18:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_19:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_20:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_21:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_22:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_23:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_24:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_25:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_26:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_27:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_28:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_29:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_30:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_31:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_32:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_33:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_34:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_35:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_36:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_37:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_38:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_39:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_40:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_41:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_42:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_43:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_44:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_45:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_46:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_47:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_48:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_49:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_50:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_51:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_52:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_53:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_54:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_55:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_56:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_57:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_58:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_59:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_60:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_61:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_62:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_63:0' shape=(1024,) dtype=float32>]\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-114-178de3cf74ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-113-2e0860e43aff>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, epochs)\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0;31m# we just declared above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0;31m#print(image_batch.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m       \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_batch_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcod_batch_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# 4 - Print out the completed epoch no. and the time spent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    821\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    696\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 697\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2855\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2856\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    <ipython-input-112-83f511d37a00>:9 train_step  *\n        real_output_emb = discriminator(real_embedding, training=True)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:976 __call__  **\n        self.name)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_spec.py:158 assert_input_compatibility\n        ' input tensors. Inputs received: ' + str(inputs))\n\n    ValueError: Layer sequential_4 expects 1 inputs, but it received 64 input tensors. Inputs received: [<tf.Tensor 'real_embedding:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_1:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_2:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_3:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_4:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_5:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_6:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_7:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_8:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_9:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_10:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_11:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_12:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_13:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_14:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_15:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_16:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_17:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_18:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_19:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_20:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_21:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_22:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_23:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_24:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_25:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_26:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_27:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_28:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_29:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_30:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_31:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_32:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_33:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_34:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_35:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_36:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_37:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_38:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_39:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_40:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_41:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_42:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_43:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_44:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_45:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_46:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_47:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_48:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_49:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_50:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_51:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_52:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_53:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_54:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_55:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_56:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_57:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_58:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_59:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_60:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_61:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_62:0' shape=(1024,) dtype=float32>, <tf.Tensor 'real_embedding_63:0' shape=(1024,) dtype=float32>]\n"
          ]
        }
      ]
    }
  ]
}
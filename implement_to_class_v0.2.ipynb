{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled17.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dolmani38/Summary/blob/master/implement_to_class_v0.2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0P7TgWUxiipp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb882cf3-c94f-46f8-9371-c0137c4b2c09"
      },
      "source": [
        "\r\n",
        "if True:\r\n",
        "    from google.colab import drive\r\n",
        "    drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YzOPVi7jFnw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caa01917-fcb1-4d6f-b3cc-3ee4f9a90b6f"
      },
      "source": [
        "!pip install sentence-transformers==0.3.0\r\n",
        "!pip install transformers==3.0.2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers==0.3.0 in /usr/local/lib/python3.6/dist-packages (0.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.0) (1.19.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.0) (0.22.2.post1)\n",
            "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.0) (1.7.0+cu101)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.0) (3.2.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.0) (4.41.1)\n",
            "Requirement already satisfied: transformers>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.0) (3.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.0) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers==0.3.0) (1.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.1->sentence-transformers==0.3.0) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.1->sentence-transformers==0.3.0) (0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.1->sentence-transformers==0.3.0) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers==0.3.0) (1.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (20.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (0.1.94)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (2.23.0)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (0.8.1rc1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (0.0.43)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers>=3.0.2->sentence-transformers==0.3.0) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=3.0.2->sentence-transformers==0.3.0) (7.1.2)\n",
            "Requirement already satisfied: transformers==3.0.2 in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (1.19.4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.0.43)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (2.23.0)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.8.1rc1)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.1.94)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (20.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.0.2) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iibqGArHjJ3V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1b5fcda-9b62-4a17-e1f5-3aa81232abe6"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "# set seeds for reproducability\r\n",
        "from numpy.random import seed\r\n",
        "seed(1)\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import string, os \r\n",
        "\r\n",
        "import urllib.request\r\n",
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings(\"ignore\")\r\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LT7gPTGQjOa_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef7fcd09-554b-43fc-fcf2-ba3396bf163a"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "# Get the GPU device name.\r\n",
        "device_name = tf.test.gpu_device_name()\r\n",
        "\r\n",
        "# The device name should look like the following:\r\n",
        "if device_name == '/device:GPU:0':\r\n",
        "    print('Found GPU at: {}'.format(device_name))\r\n",
        "else:\r\n",
        "    print('GPU device not found')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrOQ7W4yjRgU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85871800-7200-46df-9222-9862ac1e13ab"
      },
      "source": [
        "import torch\r\n",
        "\r\n",
        "# If there's a GPU available...\r\n",
        "if torch.cuda.is_available():    \r\n",
        "\r\n",
        "    # Tell PyTorch to use the GPU.    \r\n",
        "    device = torch.device(\"cuda\")\r\n",
        "\r\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\r\n",
        "\r\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\r\n",
        "\r\n",
        "# If not...\r\n",
        "else:\r\n",
        "    print('No GPU available, using the CPU instead.')\r\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla K80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "di_LncPQXJny"
      },
      "source": [
        "\r\n",
        "# Print iterations progress\r\n",
        "class ProgressBar:\r\n",
        "\r\n",
        "    def __init__(self,total=20, prefix = '', suffix = '', decimals = 1, length = 20, fill = '|', printEnd = \"\\r\"):\r\n",
        "        self.total = total\r\n",
        "        self.prefix = prefix\r\n",
        "        self.suffix = suffix\r\n",
        "        self.decimals = decimals\r\n",
        "        self.length = length\r\n",
        "        self.fill = fill\r\n",
        "        self.printEnd = printEnd\r\n",
        "        self.ite = 0\r\n",
        "\r\n",
        "    def printProgress(self,iteration, text):\r\n",
        "        self.ite += iteration\r\n",
        "        percent = (\"{0:.\" + str(self.decimals) + \"f}\").format(100 * (self.ite / float(self.total)))\r\n",
        "\r\n",
        "        filledLength = int(self.length * self.ite // self.total)\r\n",
        "        bar = self.fill * filledLength + '.' * (self.length - filledLength)\r\n",
        "        print(f'\\r{self.prefix} |{bar}| {percent}% {self.suffix}  {text}', end=\"\", flush=True)\r\n",
        "        # Print New Line on Complete\r\n",
        "        if self.ite == self.total: \r\n",
        "            print()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouegI-08jiR3"
      },
      "source": [
        "import torch\r\n",
        "import torchvision\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from torchvision import datasets\r\n",
        "from torchvision import transforms"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx0ym0-PilL_"
      },
      "source": [
        "#Grammar Discriminator Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOWA-qTalRdM"
      },
      "source": [
        "# coding=utf-8\r\n",
        "# Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team and Jangwon Park\r\n",
        "#\r\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n",
        "# you may not use this file except in compliance with the License.\r\n",
        "# You may obtain a copy of the License at\r\n",
        "#\r\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\r\n",
        "#\r\n",
        "# Unless required by applicable law or agreed to in writing, software\r\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n",
        "# See the License for the specific language governing permissions and\r\n",
        "# limitations under the License.\r\n",
        "\"\"\" Tokenization classes for KoBert model.\"\"\"\r\n",
        "\r\n",
        "\r\n",
        "import logging\r\n",
        "import os\r\n",
        "import unicodedata\r\n",
        "from shutil import copyfile\r\n",
        "\r\n",
        "from transformers import PreTrainedTokenizer\r\n",
        "\r\n",
        "\r\n",
        "logger = logging.getLogger(__name__)\r\n",
        "\r\n",
        "VOCAB_FILES_NAMES = {\"vocab_file\": \"tokenizer_78b3253a26.model\",\r\n",
        "                     \"vocab_txt\": \"vocab.txt\"}\r\n",
        "\r\n",
        "PRETRAINED_VOCAB_FILES_MAP = {\r\n",
        "    \"vocab_file\": {\r\n",
        "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/tokenizer_78b3253a26.model\",\r\n",
        "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/tokenizer_78b3253a26.model\",\r\n",
        "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/tokenizer_78b3253a26.model\"\r\n",
        "    },\r\n",
        "    \"vocab_txt\": {\r\n",
        "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/vocab.txt\",\r\n",
        "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/vocab.txt\",\r\n",
        "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/vocab.txt\"\r\n",
        "    }\r\n",
        "}\r\n",
        "\r\n",
        "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\r\n",
        "    \"monologg/kobert\": 512,\r\n",
        "    \"monologg/kobert-lm\": 512,\r\n",
        "    \"monologg/distilkobert\": 512\r\n",
        "}\r\n",
        "\r\n",
        "PRETRAINED_INIT_CONFIGURATION = {\r\n",
        "    \"monologg/kobert\": {\"do_lower_case\": False},\r\n",
        "    \"monologg/kobert-lm\": {\"do_lower_case\": False},\r\n",
        "    \"monologg/distilkobert\": {\"do_lower_case\": False}\r\n",
        "}\r\n",
        "\r\n",
        "SPIECE_UNDERLINE = u'▁'\r\n",
        "\r\n",
        "\r\n",
        "class KoBertTokenizer(PreTrainedTokenizer):\r\n",
        "    \"\"\"\r\n",
        "        SentencePiece based tokenizer. Peculiarities:\r\n",
        "            - requires `SentencePiece <https://github.com/google/sentencepiece>`_\r\n",
        "    \"\"\"\r\n",
        "    vocab_files_names = VOCAB_FILES_NAMES\r\n",
        "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\r\n",
        "    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\r\n",
        "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\r\n",
        "\r\n",
        "    def __init__(\r\n",
        "            self,\r\n",
        "            vocab_file,\r\n",
        "            vocab_txt,\r\n",
        "            do_lower_case=False,\r\n",
        "            remove_space=True,\r\n",
        "            keep_accents=False,\r\n",
        "            unk_token=\"[UNK]\",\r\n",
        "            sep_token=\"[SEP]\",\r\n",
        "            pad_token=\"[PAD]\",\r\n",
        "            cls_token=\"[CLS]\",\r\n",
        "            mask_token=\"[MASK]\",\r\n",
        "            **kwargs):\r\n",
        "        super().__init__(\r\n",
        "            unk_token=unk_token,\r\n",
        "            sep_token=sep_token,\r\n",
        "            pad_token=pad_token,\r\n",
        "            cls_token=cls_token,\r\n",
        "            mask_token=mask_token,\r\n",
        "            **kwargs\r\n",
        "        )\r\n",
        "\r\n",
        "        # Build vocab\r\n",
        "        self.token2idx = dict()\r\n",
        "        self.idx2token = []\r\n",
        "        with open(vocab_txt, 'r', encoding='utf-8') as f:\r\n",
        "            for idx, token in enumerate(f):\r\n",
        "                token = token.strip()\r\n",
        "                self.token2idx[token] = idx\r\n",
        "                self.idx2token.append(token)\r\n",
        "\r\n",
        "        try:\r\n",
        "            import sentencepiece as spm\r\n",
        "        except ImportError:\r\n",
        "            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\r\n",
        "                           \"pip install sentencepiece\")\r\n",
        "\r\n",
        "        self.do_lower_case = do_lower_case\r\n",
        "        self.remove_space = remove_space\r\n",
        "        self.keep_accents = keep_accents\r\n",
        "        self.vocab_file = vocab_file\r\n",
        "        self.vocab_txt = vocab_txt\r\n",
        "\r\n",
        "        self.sp_model = spm.SentencePieceProcessor()\r\n",
        "        self.sp_model.Load(vocab_file)\r\n",
        "\r\n",
        "    @property\r\n",
        "    def vocab_size(self):\r\n",
        "        return len(self.idx2token)\r\n",
        "\r\n",
        "    def get_vocab(self):\r\n",
        "        return dict(self.token2idx, **self.added_tokens_encoder)\r\n",
        "\r\n",
        "    def __getstate__(self):\r\n",
        "        state = self.__dict__.copy()\r\n",
        "        state[\"sp_model\"] = None\r\n",
        "        return state\r\n",
        "\r\n",
        "    def __setstate__(self, d):\r\n",
        "        self.__dict__ = d\r\n",
        "        try:\r\n",
        "            import sentencepiece as spm\r\n",
        "        except ImportError:\r\n",
        "            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\r\n",
        "                           \"pip install sentencepiece\")\r\n",
        "        self.sp_model = spm.SentencePieceProcessor()\r\n",
        "        self.sp_model.Load(self.vocab_file)\r\n",
        "\r\n",
        "    def preprocess_text(self, inputs):\r\n",
        "        if self.remove_space:\r\n",
        "            outputs = \" \".join(inputs.strip().split())\r\n",
        "        else:\r\n",
        "            outputs = inputs\r\n",
        "        outputs = outputs.replace(\"``\", '\"').replace(\"''\", '\"')\r\n",
        "\r\n",
        "        if not self.keep_accents:\r\n",
        "            outputs = unicodedata.normalize('NFKD', outputs)\r\n",
        "            outputs = \"\".join([c for c in outputs if not unicodedata.combining(c)])\r\n",
        "        if self.do_lower_case:\r\n",
        "            outputs = outputs.lower()\r\n",
        "\r\n",
        "        return outputs\r\n",
        "\r\n",
        "    def _tokenize(self, text, return_unicode=True, sample=False):\r\n",
        "        \"\"\" Tokenize a string. \"\"\"\r\n",
        "        text = self.preprocess_text(text)\r\n",
        "\r\n",
        "        if not sample:\r\n",
        "            pieces = self.sp_model.EncodeAsPieces(text)\r\n",
        "        else:\r\n",
        "            pieces = self.sp_model.SampleEncodeAsPieces(text, 64, 0.1)\r\n",
        "        new_pieces = []\r\n",
        "        for piece in pieces:\r\n",
        "            if len(piece) > 1 and piece[-1] == str(\",\") and piece[-2].isdigit():\r\n",
        "                cur_pieces = self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, \"\"))\r\n",
        "                if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\r\n",
        "                    if len(cur_pieces[0]) == 1:\r\n",
        "                        cur_pieces = cur_pieces[1:]\r\n",
        "                    else:\r\n",
        "                        cur_pieces[0] = cur_pieces[0][1:]\r\n",
        "                cur_pieces.append(piece[-1])\r\n",
        "                new_pieces.extend(cur_pieces)\r\n",
        "            else:\r\n",
        "                new_pieces.append(piece)\r\n",
        "\r\n",
        "        return new_pieces\r\n",
        "\r\n",
        "    def _convert_token_to_id(self, token):\r\n",
        "        \"\"\" Converts a token (str/unicode) in an id using the vocab. \"\"\"\r\n",
        "        return self.token2idx.get(token, self.token2idx[self.unk_token])\r\n",
        "\r\n",
        "    def _convert_id_to_token(self, index, return_unicode=True):\r\n",
        "        \"\"\"Converts an index (integer) in a token (string/unicode) using the vocab.\"\"\"\r\n",
        "        return self.idx2token[index]\r\n",
        "\r\n",
        "    def convert_tokens_to_string(self, tokens):\r\n",
        "        \"\"\"Converts a sequence of tokens (strings for sub-words) in a single string.\"\"\"\r\n",
        "        out_string = \"\".join(tokens).replace(SPIECE_UNDERLINE, \" \").strip()\r\n",
        "        return out_string\r\n",
        "\r\n",
        "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\r\n",
        "        \"\"\"\r\n",
        "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\r\n",
        "        by concatenating and adding special tokens.\r\n",
        "        A KoBERT sequence has the following format:\r\n",
        "            single sequence: [CLS] X [SEP]\r\n",
        "            pair of sequences: [CLS] A [SEP] B [SEP]\r\n",
        "        \"\"\"\r\n",
        "        if token_ids_1 is None:\r\n",
        "            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\r\n",
        "        cls = [self.cls_token_id]\r\n",
        "        sep = [self.sep_token_id]\r\n",
        "        return cls + token_ids_0 + sep + token_ids_1 + sep\r\n",
        "\r\n",
        "    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\r\n",
        "        \"\"\"\r\n",
        "        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\r\n",
        "        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\r\n",
        "        Args:\r\n",
        "            token_ids_0: list of ids (must not contain special tokens)\r\n",
        "            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\r\n",
        "                for sequence pairs\r\n",
        "            already_has_special_tokens: (default False) Set to True if the token list is already formated with\r\n",
        "                special tokens for the model\r\n",
        "        Returns:\r\n",
        "            A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        if already_has_special_tokens:\r\n",
        "            if token_ids_1 is not None:\r\n",
        "                raise ValueError(\r\n",
        "                    \"You should not supply a second sequence if the provided sequence of \"\r\n",
        "                    \"ids is already formated with special tokens for the model.\"\r\n",
        "                )\r\n",
        "            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\r\n",
        "\r\n",
        "        if token_ids_1 is not None:\r\n",
        "            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\r\n",
        "        return [1] + ([0] * len(token_ids_0)) + [1]\r\n",
        "\r\n",
        "    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\r\n",
        "        \"\"\"\r\n",
        "        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\r\n",
        "        A KoBERT sequence pair mask has the following format:\r\n",
        "        0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\r\n",
        "        | first sequence    | second sequence\r\n",
        "        if token_ids_1 is None, only returns the first portion of the mask (0's).\r\n",
        "        \"\"\"\r\n",
        "        sep = [self.sep_token_id]\r\n",
        "        cls = [self.cls_token_id]\r\n",
        "        if token_ids_1 is None:\r\n",
        "            return len(cls + token_ids_0 + sep) * [0]\r\n",
        "        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\r\n",
        "\r\n",
        "    def save_vocabulary(self, save_directory):\r\n",
        "        \"\"\" Save the sentencepiece vocabulary (copy original file) and special tokens file\r\n",
        "            to a directory.\r\n",
        "        \"\"\"\r\n",
        "        if not os.path.isdir(save_directory):\r\n",
        "            logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))\r\n",
        "            return\r\n",
        "\r\n",
        "        # 1. Save sentencepiece model\r\n",
        "        out_vocab_model = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_file\"])\r\n",
        "\r\n",
        "        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_model):\r\n",
        "            copyfile(self.vocab_file, out_vocab_model)\r\n",
        "\r\n",
        "        # 2. Save vocab.txt\r\n",
        "        index = 0\r\n",
        "        out_vocab_txt = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_txt\"])\r\n",
        "        with open(out_vocab_txt, \"w\", encoding=\"utf-8\") as writer:\r\n",
        "            for token, token_index in sorted(self.token2idx.items(), key=lambda kv: kv[1]):\r\n",
        "                if index != token_index:\r\n",
        "                    logger.warning(\r\n",
        "                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\r\n",
        "                        \" Please check that the vocabulary is not corrupted!\".format(out_vocab_txt)\r\n",
        "                    )\r\n",
        "                    index = token_index\r\n",
        "                writer.write(token + \"\\n\")\r\n",
        "                index += 1\r\n",
        "\r\n",
        "        return out_vocab_model, out_vocab_txt"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WF9qEJYhiqh8"
      },
      "source": [
        "from transformers import BertTokenizer, AutoTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\r\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset, random_split\r\n",
        "\r\n",
        "import time\r\n",
        "import datetime\r\n",
        "\r\n",
        "# 간단한 전처리\r\n",
        "def clean_text(txt):\r\n",
        "    txt = txt.replace('\\n',' ')\r\n",
        "    txt = txt.replace('\\r',' ')    \r\n",
        "    txt = txt.replace('=','')\r\n",
        "    txt = txt.replace('\\\"','')   \r\n",
        "    txt = txt.replace('\\'','')\r\n",
        "    #txt = txt.replace(',','')\r\n",
        "    txt = txt.replace('..','')\r\n",
        "    txt = txt.replace('...','')\r\n",
        "    #txt = txt.replace('.','. ')\r\n",
        "    txt = txt.replace('.','. ')\r\n",
        "    txt = txt.replace('  ',' ')\r\n",
        "    txt = txt.replace('  ',' ')    \r\n",
        "    txt = txt.replace('  ',' ')   \r\n",
        "    txt = txt.replace('  ',' ')           \r\n",
        "    txt = txt.replace('  ',' ')\r\n",
        "    txt = txt.replace('  ',' ')    \r\n",
        "    txt = txt.replace('  ',' ')   \r\n",
        "    txt = txt.replace('  ',' ')             \r\n",
        "    return txt.strip()\r\n",
        "\r\n",
        "def shuffling(txt):\r\n",
        "    txt_list = txt.split(' ')\r\n",
        "    random.shuffle(txt_list)\r\n",
        "    return ' '.join(txt_list)\r\n",
        "\r\n",
        "def collect_training_dataset_for_grammar_discriminator(source_urls=[]):\r\n",
        "    ko_sentences_dataset = []\r\n",
        "    for url in source_urls:\r\n",
        "        raw_text = urllib.request.urlopen(url).read().decode('utf-8')\r\n",
        "        ko_sentences_dataset += nltk.sent_tokenize(clean_text(raw_text))\r\n",
        "\r\n",
        "    sentences = []\r\n",
        "    labels = []\r\n",
        "\r\n",
        "    for txt in ko_sentences_dataset:\r\n",
        "        txt = txt.strip()\r\n",
        "        if len(txt) > 40:\r\n",
        "            #ko_grammar_dataset.append([txt,1])\r\n",
        "            txt = txt.replace('.','')\r\n",
        "            sentences.append(txt) # '.'의 위치를 보고 True, False를 판단 하기 땜에...\r\n",
        "            labels.append(1)\r\n",
        "            sentences.append(shuffling(txt))\r\n",
        "            labels.append(0)\r\n",
        "\r\n",
        "    return sentences,labels\r\n",
        "\r\n",
        "# Function to calculate the accuracy of our predictions vs labels\r\n",
        "def flat_accuracy(preds, labels):\r\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\r\n",
        "    labels_flat = labels.flatten()\r\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\r\n",
        "\r\n",
        "def format_time(elapsed):\r\n",
        "    '''\r\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\r\n",
        "    '''\r\n",
        "    # Round to the nearest second.\r\n",
        "    elapsed_rounded = int(round((elapsed)))\r\n",
        "    \r\n",
        "    # Format as hh:mm:ss\r\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\r\n",
        "\r\n",
        "class Grammar_Discriminator:\r\n",
        "    \"\"\"\r\n",
        "    # dataset 주입하고 train해서 사용하는 경우, \r\n",
        "    gd = Grammar_Discriminator(pretraoned_kobert_model_name='monologg/kobert')\r\n",
        "    gd.set_dataset(dataset = (sentences,labels))\r\n",
        "    gd.train(epochs = 4)\r\n",
        "\r\n",
        "\r\n",
        "    # 기존에 학습한 모델을 load 할 경우\r\n",
        "    gd = Grammar_Discriminator(input_dir = './drive/MyDrive/Colab Notebooks/summary/grammar_check_model')\r\n",
        "    \r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, pretraoned_kobert_model_name='monologg/kobert', input_dir=None):\r\n",
        "\r\n",
        "        if input_dir is None:\r\n",
        "            self.tokenizer = KoBertTokenizer.from_pretrained(pretraoned_kobert_model_name)\r\n",
        "            self.discriminator = BertForSequenceClassification.from_pretrained(\r\n",
        "                                    pretraoned_kobert_model_name, # Use the 12-layer BERT model, with an uncased vocab.\r\n",
        "                                    num_labels = 2, # The number of output labels--2 for binary classification.\r\n",
        "                                                    # You can increase this for multi-class tasks.   \r\n",
        "                                    output_attentions = False, # Whether the model returns attentions weights.\r\n",
        "                                    output_hidden_states = False, # Whether the model returns all hidden-states.\r\n",
        "                                )            \r\n",
        "        else:\r\n",
        "            self.__load_model(input_dir)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    def set_dataset(self, sentences,labels):\r\n",
        "        # Print the original sentence.\r\n",
        "        print(' Original: ', sentences[0])\r\n",
        "\r\n",
        "        # Print the sentence split into tokens.\r\n",
        "        print('Tokenized: ', self.tokenizer.tokenize(sentences[0]))\r\n",
        "\r\n",
        "        # Print the sentence mapped to token ids.\r\n",
        "        print('Token IDs: ', self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(sentences[0])))   \r\n",
        "\r\n",
        "        # Tokenize all of the sentences and map the tokens to thier word IDs.\r\n",
        "        input_ids = []\r\n",
        "        attention_masks = []\r\n",
        "\r\n",
        "        # For every sentence...\r\n",
        "        for sent in sentences:\r\n",
        "            # `encode_plus` will:\r\n",
        "            #   (1) Tokenize the sentence.\r\n",
        "            #   (2) Prepend the `[CLS]` token to the start.\r\n",
        "            #   (3) Append the `[SEP]` token to the end.\r\n",
        "            #   (4) Map tokens to their IDs.\r\n",
        "            #   (5) Pad or truncate the sentence to `max_length`\r\n",
        "            #   (6) Create attention masks for [PAD] tokens.\r\n",
        "            encoded_dict = self.tokenizer.encode_plus(\r\n",
        "                                sent,                      # Sentence to encode.\r\n",
        "                                add_special_tokens = True, # Add '[CLS]' and '[SEP]'\r\n",
        "                                max_length = 64,           # Pad & truncate all sentences.\r\n",
        "                                pad_to_max_length = True,\r\n",
        "                                return_attention_mask = True,   # Construct attn. masks.\r\n",
        "                                return_tensors = 'pt',     # Return pytorch tensors.\r\n",
        "                                truncation = True,\r\n",
        "                        )\r\n",
        "            \r\n",
        "            # Add the encoded sentence to the list.    \r\n",
        "            input_ids.append(encoded_dict['input_ids'])\r\n",
        "            \r\n",
        "            # And its attention mask (simply differentiates padding from non-padding).\r\n",
        "            attention_masks.append(encoded_dict['attention_mask'])\r\n",
        "\r\n",
        "        # Convert the lists into tensors.\r\n",
        "        input_ids = torch.cat(input_ids, dim=0)\r\n",
        "        attention_masks = torch.cat(attention_masks, dim=0)\r\n",
        "        labels = torch.tensor(labels)\r\n",
        "\r\n",
        "        # Print sentence 0, now as a list of IDs.\r\n",
        "        print('Original: ', sentences[0])\r\n",
        "        print('Token IDs:', input_ids[0])\r\n",
        "\r\n",
        "        # Training & Validation Split\r\n",
        "        # Divide up our training set to use 90% for training and 10% for validation.\r\n",
        "\r\n",
        "        # Combine the training inputs into a TensorDataset.\r\n",
        "        dataset = TensorDataset(input_ids, attention_masks, labels)\r\n",
        "\r\n",
        "        # Create a 90-10 train-validation split.\r\n",
        "\r\n",
        "        # Calculate the number of samples to include in each set.\r\n",
        "        train_size = int(0.9 * len(dataset))\r\n",
        "        val_size = len(dataset) - train_size\r\n",
        "\r\n",
        "        # Divide the dataset by randomly selecting samples.\r\n",
        "        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\r\n",
        "\r\n",
        "        print('{:>5,} training samples'.format(train_size))\r\n",
        "        print('{:>5,} validation samples'.format(val_size))\r\n",
        "\r\n",
        "        # The DataLoader needs to know our batch size for training, so we specify it \r\n",
        "        # here. For fine-tuning BERT on a specific task, the authors recommend a batch \r\n",
        "        # size of 16 or 32.\r\n",
        "        self.batch_size = 32\r\n",
        "\r\n",
        "        # Create the DataLoaders for our training and validation sets.\r\n",
        "        # We'll take training samples in random order. \r\n",
        "        self.train_dataloader = DataLoader(\r\n",
        "                    train_dataset,  # The training samples.\r\n",
        "                    sampler = RandomSampler(train_dataset), # Select batches randomly\r\n",
        "                    batch_size = self.batch_size # Trains with this batch size.\r\n",
        "                )\r\n",
        "\r\n",
        "        # For validation the order doesn't matter, so we'll just read them sequentially.\r\n",
        "        self.validation_dataloader = DataLoader(\r\n",
        "                    val_dataset, # The validation samples.\r\n",
        "                    sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\r\n",
        "                    batch_size = self.batch_size # Evaluate with this batch size.\r\n",
        "                )        \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    def train(self,epochs=4):\r\n",
        "        # Tell pytorch to run this model on the GPU.\r\n",
        "        self.discriminator.cuda()\r\n",
        "\r\n",
        "        # Get all of the model's parameters as a list of tuples.\r\n",
        "        params = list(self.discriminator.named_parameters())\r\n",
        "\r\n",
        "        print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\r\n",
        "\r\n",
        "        print('==== Embedding Layer ====\\n')\r\n",
        "\r\n",
        "        for p in params[0:5]:\r\n",
        "            print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\r\n",
        "\r\n",
        "        print('\\n==== First Transformer ====\\n')\r\n",
        "\r\n",
        "        for p in params[5:21]:\r\n",
        "            print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\r\n",
        "\r\n",
        "        print('\\n==== Output Layer ====\\n')\r\n",
        "\r\n",
        "        for p in params[-4:]:\r\n",
        "            print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))  \r\n",
        "\r\n",
        "        # Note: AdamW is a class from the huggingface library (as opposed to pytorch) \r\n",
        "        # I believe the 'W' stands for 'Weight Decay fix\"\r\n",
        "        self.optimizer = AdamW(self.discriminator.parameters(),\r\n",
        "                        lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\r\n",
        "                        eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\r\n",
        "                        )\r\n",
        "\r\n",
        "        # Number of training epochs. The BERT authors recommend between 2 and 4. \r\n",
        "        # We chose to run for 4, but we'll see later that this may be over-fitting the\r\n",
        "        # training data.\r\n",
        "        #epochs = 2\r\n",
        "\r\n",
        "        # Total number of training steps is [number of batches] x [number of epochs]. \r\n",
        "        # (Note that this is not the same as the number of training samples).\r\n",
        "        total_steps = len(self.train_dataloader) * epochs\r\n",
        "\r\n",
        "        # Create the learning rate scheduler.\r\n",
        "        scheduler = get_linear_schedule_with_warmup(self.optimizer, \r\n",
        "                                                    num_warmup_steps = 0, # Default value in run_glue.py\r\n",
        "                                                    num_training_steps = total_steps)\r\n",
        "            \r\n",
        "        # This training code is based on the `run_glue.py` script here:\r\n",
        "        # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\r\n",
        "\r\n",
        "        # Set the seed value all over the place to make this reproducible.\r\n",
        "        seed_val = 42\r\n",
        "\r\n",
        "        random.seed(seed_val)\r\n",
        "        np.random.seed(seed_val)\r\n",
        "        torch.manual_seed(seed_val)\r\n",
        "        torch.cuda.manual_seed_all(seed_val)\r\n",
        "\r\n",
        "        # We'll store a number of quantities such as training and validation loss, \r\n",
        "        # validation accuracy, and timings.\r\n",
        "        training_stats = []\r\n",
        "\r\n",
        "        # Measure the total training time for the whole run.\r\n",
        "        total_t0 = time.time()\r\n",
        "\r\n",
        "        # For each epoch...\r\n",
        "        for epoch_i in range(0, epochs):\r\n",
        "            \r\n",
        "            # ========================================\r\n",
        "            #               Training\r\n",
        "            # ========================================\r\n",
        "            \r\n",
        "            # Perform one full pass over the training set.\r\n",
        "\r\n",
        "            print(\"\")\r\n",
        "            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\r\n",
        "            print('Training...')\r\n",
        "\r\n",
        "            # Measure how long the training epoch takes.\r\n",
        "            t0 = time.time()\r\n",
        "\r\n",
        "            # Reset the total loss for this epoch.\r\n",
        "            total_train_loss = 0\r\n",
        "\r\n",
        "            # Put the model into training mode. Don't be mislead--the call to \r\n",
        "            # `train` just changes the *mode*, it doesn't *perform* the training.\r\n",
        "            # `dropout` and `batchnorm` layers behave differently during training\r\n",
        "            # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\r\n",
        "            self.discriminator.train()\r\n",
        "\r\n",
        "            # For each batch of training data...\r\n",
        "            for step, batch in enumerate(self.train_dataloader):\r\n",
        "\r\n",
        "                # Progress update every 40 batches.\r\n",
        "                if step % 40 == 0 and not step == 0:\r\n",
        "                    # Calculate elapsed time in minutes.\r\n",
        "                    elapsed = format_time(time.time() - t0)\r\n",
        "                    \r\n",
        "                    # Report progress.\r\n",
        "                    print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(self.train_dataloader), elapsed))\r\n",
        "\r\n",
        "                # Unpack this training batch from our dataloader. \r\n",
        "                #\r\n",
        "                # As we unpack the batch, we'll also copy each tensor to the GPU using the \r\n",
        "                # `to` method.\r\n",
        "                #\r\n",
        "                # `batch` contains three pytorch tensors:\r\n",
        "                #   [0]: input ids \r\n",
        "                #   [1]: attention masks\r\n",
        "                #   [2]: labels \r\n",
        "                b_input_ids = batch[0].to(device)\r\n",
        "                b_input_mask = batch[1].to(device)\r\n",
        "                b_labels = batch[2].to(device)\r\n",
        "\r\n",
        "                # Always clear any previously calculated gradients before performing a\r\n",
        "                # backward pass. PyTorch doesn't do this automatically because \r\n",
        "                # accumulating the gradients is \"convenient while training RNNs\". \r\n",
        "                # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\r\n",
        "                self.discriminator.zero_grad()        \r\n",
        "\r\n",
        "                # Perform a forward pass (evaluate the model on this training batch).\r\n",
        "                # The documentation for this `model` function is here: \r\n",
        "                # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\r\n",
        "                # It returns different numbers of parameters depending on what arguments\r\n",
        "                # arge given and what flags are set. For our useage here, it returns\r\n",
        "                # the loss (because we provided labels) and the \"logits\"--the model\r\n",
        "                # outputs prior to activation.\r\n",
        "                loss, logits = self.discriminator(b_input_ids, \r\n",
        "                                    token_type_ids=None, \r\n",
        "                                    attention_mask=b_input_mask, \r\n",
        "                                    labels=b_labels)\r\n",
        "\r\n",
        "                # Accumulate the training loss over all of the batches so that we can\r\n",
        "                # calculate the average loss at the end. `loss` is a Tensor containing a\r\n",
        "                # single value; the `.item()` function just returns the Python value \r\n",
        "                # from the tensor.\r\n",
        "                total_train_loss += loss.item()\r\n",
        "\r\n",
        "                # Perform a backward pass to calculate the gradients.\r\n",
        "                loss.backward()\r\n",
        "\r\n",
        "                # Clip the norm of the gradients to 1.0.\r\n",
        "                # This is to help prevent the \"exploding gradients\" problem.\r\n",
        "                torch.nn.utils.clip_grad_norm_(self.discriminator.parameters(), 1.0)\r\n",
        "\r\n",
        "                # Update parameters and take a step using the computed gradient.\r\n",
        "                # The optimizer dictates the \"update rule\"--how the parameters are\r\n",
        "                # modified based on their gradients, the learning rate, etc.\r\n",
        "                self.optimizer.step()\r\n",
        "\r\n",
        "                # Update the learning rate.\r\n",
        "                scheduler.step()\r\n",
        "\r\n",
        "            # Calculate the average loss over all of the batches.\r\n",
        "            avg_train_loss = total_train_loss / len(self.train_dataloader)            \r\n",
        "            \r\n",
        "            # Measure how long this epoch took.\r\n",
        "            training_time = format_time(time.time() - t0)\r\n",
        "\r\n",
        "            print(\"\")\r\n",
        "            print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\r\n",
        "            print(\"  Training epcoh took: {:}\".format(training_time))\r\n",
        "                \r\n",
        "            # ========================================\r\n",
        "            #               Validation\r\n",
        "            # ========================================\r\n",
        "            # After the completion of each training epoch, measure our performance on\r\n",
        "            # our validation set.\r\n",
        "\r\n",
        "            print(\"\")\r\n",
        "            print(\"Running Validation...\")\r\n",
        "\r\n",
        "            t0 = time.time()\r\n",
        "\r\n",
        "            # Put the model in evaluation mode--the dropout layers behave differently\r\n",
        "            # during evaluation.\r\n",
        "            self.discriminator.eval()\r\n",
        "\r\n",
        "            # Tracking variables \r\n",
        "            total_eval_accuracy = 0\r\n",
        "            total_eval_loss = 0\r\n",
        "            nb_eval_steps = 0\r\n",
        "\r\n",
        "            # Evaluate data for one epoch\r\n",
        "            for batch in self.validation_dataloader:\r\n",
        "                \r\n",
        "                # Unpack this training batch from our dataloader. \r\n",
        "                #\r\n",
        "                # As we unpack the batch, we'll also copy each tensor to the GPU using \r\n",
        "                # the `to` method.\r\n",
        "                #\r\n",
        "                # `batch` contains three pytorch tensors:\r\n",
        "                #   [0]: input ids \r\n",
        "                #   [1]: attention masks\r\n",
        "                #   [2]: labels \r\n",
        "                b_input_ids = batch[0].to(device)\r\n",
        "                b_input_mask = batch[1].to(device)\r\n",
        "                b_labels = batch[2].to(device)\r\n",
        "                \r\n",
        "                # Tell pytorch not to bother with constructing the compute graph during\r\n",
        "                # the forward pass, since this is only needed for backprop (training).\r\n",
        "                with torch.no_grad():        \r\n",
        "\r\n",
        "                    # Forward pass, calculate logit predictions.\r\n",
        "                    # token_type_ids is the same as the \"segment ids\", which \r\n",
        "                    # differentiates sentence 1 and 2 in 2-sentence tasks.\r\n",
        "                    # The documentation for this `model` function is here: \r\n",
        "                    # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\r\n",
        "                    # Get the \"logits\" output by the model. The \"logits\" are the output\r\n",
        "                    # values prior to applying an activation function like the softmax.\r\n",
        "                    (loss, logits) = self.discriminator(b_input_ids, \r\n",
        "                                        token_type_ids=None, \r\n",
        "                                        attention_mask=b_input_mask,\r\n",
        "                                        labels=b_labels)\r\n",
        "                    \r\n",
        "                # Accumulate the validation loss.\r\n",
        "                total_eval_loss += loss.item()\r\n",
        "\r\n",
        "                # Move logits and labels to CPU\r\n",
        "                logits = logits.detach().cpu().numpy()\r\n",
        "                label_ids = b_labels.to('cpu').numpy()\r\n",
        "\r\n",
        "                # Calculate the accuracy for this batch of test sentences, and\r\n",
        "                # accumulate it over all batches.\r\n",
        "                total_eval_accuracy += flat_accuracy(logits, label_ids)\r\n",
        "                \r\n",
        "\r\n",
        "            # Report the final accuracy for this validation run.\r\n",
        "            avg_val_accuracy = total_eval_accuracy / len(self.validation_dataloader)\r\n",
        "            print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\r\n",
        "\r\n",
        "            # Calculate the average loss over all of the batches.\r\n",
        "            avg_val_loss = total_eval_loss / len(self.validation_dataloader)\r\n",
        "            \r\n",
        "            # Measure how long the validation run took.\r\n",
        "            validation_time = format_time(time.time() - t0)\r\n",
        "            \r\n",
        "            print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\r\n",
        "            print(\"  Validation took: {:}\".format(validation_time))\r\n",
        "\r\n",
        "            # Record all statistics from this epoch.\r\n",
        "            training_stats.append(\r\n",
        "                {\r\n",
        "                    'epoch': epoch_i + 1,\r\n",
        "                    'Training Loss': avg_train_loss,\r\n",
        "                    'Valid. Loss': avg_val_loss,\r\n",
        "                    'Valid. Accur.': avg_val_accuracy,\r\n",
        "                    'Training Time': training_time,\r\n",
        "                    'Validation Time': validation_time\r\n",
        "                }\r\n",
        "            )\r\n",
        "\r\n",
        "        print(\"\")\r\n",
        "        print(\"Training complete!\")\r\n",
        "\r\n",
        "        print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\r\n",
        "            \r\n",
        "\r\n",
        "        return training_stats\r\n",
        "\r\n",
        "    def save_model(self, output_dir = './model_save/'):\r\n",
        "        # Create output directory if needed\r\n",
        "        if not os.path.exists(output_dir):\r\n",
        "            os.makedirs(output_dir)\r\n",
        "\r\n",
        "        print(\"Saving model to %s\" % output_dir)\r\n",
        "\r\n",
        "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\r\n",
        "        # They can then be reloaded using `from_pretrained()`\r\n",
        "        model_to_save = self.discriminator.module if hasattr(self.discriminator, 'module') else self.discriminator  # Take care of distributed/parallel training\r\n",
        "        model_to_save.save_pretrained(output_dir)\r\n",
        "        self.tokenizer.save_pretrained(output_dir)\r\n",
        "\r\n",
        "        # Good practice: save your training arguments together with the trained model\r\n",
        "        # torch.save(args, os.path.join(output_dir, 'training_args.bin'))\r\n",
        "\r\n",
        "    def __load_model(self, input_dir = './drive/MyDrive/Colab Notebooks/summary/grammar_check_model'):\r\n",
        "        print('Loading BERT tokenizer...')\r\n",
        "        self.tokenizer = KoBertTokenizer.from_pretrained(input_dir)\r\n",
        "        self.discriminator = BertForSequenceClassification.from_pretrained(input_dir)\r\n",
        "\r\n",
        "    def transfer_learning(self, sentences, train_for = True):\r\n",
        "        \r\n",
        "        input_ids = []\r\n",
        "        attention_masks = []\r\n",
        "\r\n",
        "        # For every sentence...\r\n",
        "        for sent in sentences:\r\n",
        "            # `encode_plus` will:\r\n",
        "            #   (1) Tokenize the sentence.\r\n",
        "            #   (2) Prepend the `[CLS]` token to the start.\r\n",
        "            #   (3) Append the `[SEP]` token to the end.\r\n",
        "            #   (4) Map tokens to their IDs.\r\n",
        "            #   (5) Pad or truncate the sentence to `max_length`\r\n",
        "            #   (6) Create attention masks for [PAD] tokens.\r\n",
        "            encoded_dict = self.tokenizer.encode_plus(\r\n",
        "                                sent,                      # Sentence to encode.\r\n",
        "                                add_special_tokens = True, # Add '[CLS]' and '[SEP]'\r\n",
        "                                max_length = 64,           # Pad & truncate all sentences.\r\n",
        "                                pad_to_max_length = True,\r\n",
        "                                return_attention_mask = True,   # Construct attn. masks.\r\n",
        "                                return_tensors = 'pt',     # Return pytorch tensors.\r\n",
        "                                truncation = True,\r\n",
        "                        )\r\n",
        "            # Add the encoded sentence to the list.    \r\n",
        "            input_ids.append(encoded_dict['input_ids'])\r\n",
        "\r\n",
        "            # And its attention mask (simply differentiates padding from non-padding).\r\n",
        "            attention_masks.append(encoded_dict['attention_mask'])\r\n",
        "        \r\n",
        "        if train_for:\r\n",
        "            b_labels = torch.ones(len(sentences),dtype=torch.long).to(device)\r\n",
        "        else:\r\n",
        "            b_labels = torch.zeros(len(sentences),dtype=torch.long).to(device)\r\n",
        "        #print(b_labels)\r\n",
        "        # Convert the lists into tensors.\r\n",
        "        input_ids = torch.cat(input_ids, dim=0).to(device)\r\n",
        "        attention_masks = torch.cat(attention_masks, dim=0).to(device)    \r\n",
        "        #if str(discriminator1.device) == 'cpu':\r\n",
        "        #    pass\r\n",
        "        #else:\r\n",
        "        #    input_ids = input_ids.to(device)\r\n",
        "        #    attention_masks = attention_masks.to(device)        \r\n",
        "\r\n",
        "        loss, logits = self.discriminator(input_ids, \r\n",
        "                            token_type_ids=None, \r\n",
        "                            attention_mask=attention_masks, \r\n",
        "                                labels=b_labels)\r\n",
        "        #return torch.sigmoid(outputs[0][:,1])\r\n",
        "        #return outputs[0][:,1]\r\n",
        "        return loss, logits\r\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCSwzh41Tfwh"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\r\n",
        "from transformers import BertTokenizer\r\n",
        "from scipy.signal import find_peaks\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from scipy.misc import electrocardiogram\r\n",
        "import scipy\r\n",
        "\r\n",
        "\r\n",
        "class Similarity_Discriminator:\r\n",
        "\r\n",
        "    _instance = None\r\n",
        "    _embedder = None\r\n",
        "    def __new__(cls,pre_trained_model_name='xlm-r-large-en-ko-nli-ststb'):\r\n",
        "        if cls._instance is None:\r\n",
        "            print('Creating Similarity_Discriminator object')\r\n",
        "            cls._instance = super(Similarity_Discriminator, cls).__new__(cls)\r\n",
        "            # Put any initialization here.\r\n",
        "            cls._embedder = SentenceTransformer(pre_trained_model_name)\r\n",
        "        return cls._instance\r\n",
        "\r\n",
        "    def encode(self,texts):\r\n",
        "        return self._embedder.encode(texts,show_progress_bar=False)\r\n",
        "\r\n",
        "    def similiraty(self, queries, org_text_emb):\r\n",
        "        scores = []\r\n",
        "        query_embeddings = self._embedder.encode(queries,show_progress_bar=False)\r\n",
        "        for query, query_embedding in zip(queries, query_embeddings):\r\n",
        "            distances = scipy.spatial.distance.cdist([query_embedding], [org_text_emb], \"cosine\")[0]\r\n",
        "            results = zip(range(len(distances)), distances)\r\n",
        "            for idx, distance in results:\r\n",
        "                scores.append(1-distance)\r\n",
        "\r\n",
        "        return scores        "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtJdhYnAobSt"
      },
      "source": [
        "\r\n",
        "class Source:\r\n",
        "\r\n",
        "    def __init__(self,org_text):\r\n",
        "        self.org_text = org_text\r\n",
        "\r\n",
        "    def __crean_text(self, txt):\r\n",
        "        txt = txt.replace('\\n',' ')\r\n",
        "        txt = txt.replace('\\r',' ')    \r\n",
        "        txt = txt.replace('=','')\r\n",
        "        txt = txt.replace('\\\"','')   \r\n",
        "        txt = txt.replace('\\'','')\r\n",
        "        #txt = txt.replace(',','')\r\n",
        "        txt = txt.replace('..','')\r\n",
        "        txt = txt.replace('...','')\r\n",
        "        #txt = txt.replace('.','. ')\r\n",
        "        txt = txt.replace('.','. ')\r\n",
        "        txt = txt.replace('  ',' ')\r\n",
        "        txt = txt.replace('  ',' ')    \r\n",
        "        txt = txt.replace('  ',' ')   \r\n",
        "        txt = txt.replace('  ',' ')           \r\n",
        "        txt = txt.replace('  ',' ')\r\n",
        "        txt = txt.replace('  ',' ')    \r\n",
        "        txt = txt.replace('  ',' ')   \r\n",
        "        txt = txt.replace('  ',' ')           \r\n",
        "        return txt.strip()\r\n",
        "\r\n",
        "    def analysis_frame_terms(self,s_discriminator,story_filters=np.array([[0,1],[0,1,2],[0,1,2,3]]),peak_base_line = 0.0):\r\n",
        "\r\n",
        "        self.org_text = self.__crean_text(self.org_text.strip())\r\n",
        "        print('------------------------------------------------------------------')\r\n",
        "        print(self.org_text)\r\n",
        "        print('------------------------------------------------------------------')\r\n",
        "        self.org_sentences = np.asarray(nltk.sent_tokenize(self.org_text))\r\n",
        "        self.org_term_set = (' ' + self.org_text + ' ').split(' ')\r\n",
        "        self.org_source_length = len(self.org_term_set)\r\n",
        "        self.term_table = {}\r\n",
        "        #morp_table = {}\r\n",
        "\r\n",
        "        for index, word in zip(range(len(self.org_term_set)),self.org_term_set):\r\n",
        "            self.term_table[index] = word\r\n",
        "\r\n",
        "        print('Token table of origin text')\r\n",
        "        print('---------------------------------------------')\r\n",
        "        print(' Code     Token     ')\r\n",
        "        for k in self.term_table.keys():\r\n",
        "            print( f'  {str(k).ljust(5)}     {self.term_table[k]}')\r\n",
        "        print('---------------------------------------------')\r\n",
        "\r\n",
        "\r\n",
        "        self.s_discriminator = s_discriminator\r\n",
        "        # 원문의 embedding...\r\n",
        "        self.org_text_emb = self.s_discriminator.encode([self.org_text])[0]\r\n",
        "\r\n",
        "        # weight 들의 초기화\r\n",
        "        terms = np.array(list(self.term_table.values()))\r\n",
        "\r\n",
        "        word_filters=np.array([[0]])\r\n",
        "\r\n",
        "        story_weights = np.zeros(self.org_source_length,)\r\n",
        "        word_weights = np.zeros(self.org_source_length,)\r\n",
        "\r\n",
        "        #terms = np.array(list(self.term_table.values()))\r\n",
        "\r\n",
        "        # story에 지배적인 word를 찾는다.\r\n",
        "        # 먼저 word의 강세 분석\r\n",
        "        for filter in word_filters:\r\n",
        "            #print(filter)\r\n",
        "            last_idx = len(terms)-(max(filter)+1)\r\n",
        "            pb = ProgressBar(last_idx,prefix='word density scan :')\r\n",
        "            for conv in range(last_idx,0,-1):\r\n",
        "                pb.printProgress(+1,f'filer:{filter} {conv}/{last_idx}       ')\r\n",
        "                t = np.array(filter) + conv\r\n",
        "                part_sen = ' '.join(terms[t]) \r\n",
        "                score = self.s_discriminator.similiraty([part_sen.strip()],self.org_text_emb)[0]\r\n",
        "                word_weights[t] += score \r\n",
        "\r\n",
        "        # story의 강세 분석\r\n",
        "        for filter in story_filters:\r\n",
        "            #print(filter)\r\n",
        "            last_idx = len(terms)-(max(filter)+1)\r\n",
        "            pb = ProgressBar(last_idx,prefix='story density scan:')\r\n",
        "            for conv in range(last_idx,0,-1):\r\n",
        "                pb.printProgress(+1,f'filer:{filter} {conv}/{last_idx}       ')\r\n",
        "                t = np.array(filter) + conv\r\n",
        "                part_sen = ' '.join(terms[t]) \r\n",
        "                score = self.s_discriminator.similiraty([part_sen.strip()],self.org_text_emb)[0]\r\n",
        "                story_weights[t] += score\r\n",
        "\r\n",
        "        #각각의 peak를 산출\r\n",
        "        word_peaks, _ = find_peaks(word_weights, height=0)\r\n",
        "        story_peaks, _ = find_peaks(story_weights, height=0)\r\n",
        "\r\n",
        "        #두개의 peak가 겹치는 word에 대해 한개 word가 유사도에 미치는 영향이 큰것으로 간주\r\n",
        "        #해당 word를 유사도 판단 필터에서 제외하고 다시 필터링...\r\n",
        "        #이를 통해 story에 대한 word를 최대한 추출 한다.\r\n",
        "\r\n",
        "        dup_order = []\r\n",
        "        for i in range(self.org_source_length):\r\n",
        "            #lst = \"\"\r\n",
        "            if (i in word_peaks) and (i in story_peaks):\r\n",
        "                if terms[i].endswith('.'):\r\n",
        "                    pass\r\n",
        "                else:\r\n",
        "                    dup_order.append(i)\r\n",
        "                    \r\n",
        "        # Story에 대한 weight을 추출하기 위해, word에 유독 강세가 있는 term을 제외 시킨다.\r\n",
        "        print('Negative words:',terms[dup_order])\r\n",
        "        terms[dup_order] = '---'\r\n",
        "        '''\r\n",
        "        print('Token table of origin text')\r\n",
        "        print('---------------------------------------------')\r\n",
        "        print(' Code         Token      ')\r\n",
        "        print('')\r\n",
        "        for index, word in zip(range(len(terms)),terms):\r\n",
        "            print( f'  {str(index).ljust(8)}    {word}')\r\n",
        "        print('---------------------------------------------')\r\n",
        "        '''\r\n",
        "        self.story_weights = np.zeros(self.org_source_length,)\r\n",
        "        # 그리고 다시 story 분석 스캔\r\n",
        "        for filter in story_filters:\r\n",
        "            #print(filter)\r\n",
        "            last_idx = len(terms)-(max(filter)+1)\r\n",
        "            pb = ProgressBar(last_idx,prefix='story density scan:')\r\n",
        "            for conv in range(last_idx):\r\n",
        "                pb.printProgress(+1,f'filer:{filter} {conv}/{last_idx}       ')\r\n",
        "                t = np.array(filter) + conv\r\n",
        "                part_sen = ' '.join(terms[t]) \r\n",
        "                #part_sen = part_sen.replace('소녀','---')\r\n",
        "                score = self.s_discriminator.similiraty([part_sen.strip()],self.org_text_emb)[0]\r\n",
        "                self.story_weights[t] += score        \r\n",
        "\r\n",
        "\r\n",
        "        # base line\r\n",
        "        base_line = peak_base_line\r\n",
        "        # 다시 peak 추출\r\n",
        "        story_peaks, _ = find_peaks(self.story_weights, height=base_line)\r\n",
        "        print(story_peaks)\r\n",
        "        \r\n",
        "        self.story_peaks = np.append(story_peaks,len(story_weights)-2)\r\n",
        "        print(self.story_peaks)\r\n",
        "        # story density 표출\r\n",
        "        plt.figure(figsize=(12, 6))\r\n",
        "        plt.plot(self.story_weights)\r\n",
        "        plt.plot(self.story_peaks, self.story_weights[self.story_peaks], \"x\")\r\n",
        "        plt.plot(np.zeros_like(self.story_weights)+base_line, \"--\", color=\"gray\")\r\n",
        "        plt.show() \r\n",
        "        print('Peak count:',len(self.story_peaks))          \r\n",
        "\r\n",
        "\r\n",
        "        # story skeleton 추출\r\n",
        "        skel_text = \"\"\r\n",
        "        for k in self.story_peaks:\r\n",
        "            #print(k,term_weight[k],word_table[k])\r\n",
        "            skel_text += self.term_table[k]+' '  \r\n",
        "\r\n",
        "        print('Frame text:',skel_text)\r\n",
        "        print('')\r\n",
        "        print(f'Similarity : {self.s_discriminator.similiraty([skel_text.strip()],self.org_text_emb)[0]}')      \r\n",
        "\r\n",
        "        for index, word in zip(range(len(self.org_term_set)),self.org_term_set):\r\n",
        "            self.term_table[index] = word\r\n",
        "        '''    \r\n",
        "        print('Token table of origin text')\r\n",
        "        print('---------------------------------------------')\r\n",
        "        print(' Code     Score        Token              ')\r\n",
        "        print('')\r\n",
        "        for k in self.term_table.keys(): \r\n",
        "            print( f'  {str(k).ljust(5)}   {str(round(self.story_weights[k],4)).ljust(8)}  {self.term_table[k]}')\r\n",
        "\r\n",
        "        print('---------------------------------------------') \r\n",
        "        '''\r\n",
        "    def get_org_sample(self, num):\r\n",
        "        return self.org_sentences[np.random.choice(len(self.org_sentences), num)]\r\n",
        "\r\n",
        "    def get_source_embedded_code(self):\r\n",
        "        return self.org_text_emb"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6m8KIRU2ijn9"
      },
      "source": [
        "from functools import reduce\r\n",
        "\r\n",
        "class Generator(nn.Module):\r\n",
        "    \"\"\"\r\n",
        "        Simple Generator w/ MLP\r\n",
        "    \"\"\"\r\n",
        "    def __init__(self, input_size=1024):\r\n",
        "        super(Generator, self).__init__()\r\n",
        "        self.layer = nn.Sequential(\r\n",
        "            nn.Linear(input_size, input_size*2),\r\n",
        "            nn.LeakyReLU(0.2),\r\n",
        "            \r\n",
        "            nn.Linear(input_size*2, input_size*2),\r\n",
        "            nn.LeakyReLU(0.2),\r\n",
        "            #nn.Linear(input_size*4, input_size*2),\r\n",
        "            #nn.LeakyReLU(0.2),\r\n",
        "\r\n",
        "            nn.Linear(input_size*2, input_size),\r\n",
        "            #nn.BatchNorm1d(term_length*4),\r\n",
        "            ##nn.Tanh() # -1 ~ 1\r\n",
        "        )\r\n",
        "    '''\r\n",
        "    def forward(self, x, story_peaks, bias):\r\n",
        "        #biased_noise = torch.randn(N,_NOISE_DIM)\r\n",
        "        # stroy peak에 해당하는 term에게 평균값에 해당하는 bias를 추가 한다.\r\n",
        "                 \r\n",
        "        y_ = self.layer(x)\r\n",
        "        y_[:,story_peaks] += bias\r\n",
        "        y_ = nn.Sigmoid()(y_)\r\n",
        "        #reduce(torch.add, [y_,bias]) / 2\r\n",
        "        return y_\r\n",
        "    '''\r\n",
        "\r\n",
        "\r\n",
        "    def forward(self, x, bias):\r\n",
        "        #biased_noise = torch.randn(N,_NOISE_DIM)\r\n",
        "        # stroy peak에 해당하는 term에게 평균값에 해당하는 bias를 추가 한다.\r\n",
        "                 \r\n",
        "        y_ = self.layer(x)\r\n",
        "        y = torch.add(y_,bias)\r\n",
        "        y = nn.Sigmoid()(y)\r\n",
        "\r\n",
        "        return y, y_\r\n",
        "\r\n",
        "    '''    \r\n",
        "    def forward(self, x):\r\n",
        "        #biased_noise = torch.randn(N,_NOISE_DIM)\r\n",
        "        # stroy peak에 해당하는 term에게 평균값에 해당하는 bias를 추가 한다.\r\n",
        "                 \r\n",
        "        y_ = self.layer(x)\r\n",
        "        #y = torch.add(y_,bias)\r\n",
        "        y = nn.Sigmoid()(y_)\r\n",
        "\r\n",
        "        return y, y_    \r\n",
        "    '''        "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLVGMAA_qn01"
      },
      "source": [
        "import random\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "\r\n",
        "class Summarizer:\r\n",
        "\r\n",
        "    def __init__(self,g_discriminator,s_discriminator):\r\n",
        "        self.g_discriminator = g_discriminator\r\n",
        "        self.s_discriminator = s_discriminator\r\n",
        "\r\n",
        "    def ready(self,source):\r\n",
        "        self.source = source  \r\n",
        "        #self.source.analysis_frame_terms(self.s_discriminator)\r\n",
        "        self.generator = Generator(input_size=self.source.org_source_length)\r\n",
        "\r\n",
        "        return self\r\n",
        "\r\n",
        "    def summarize(self,epochs=10,batch_size=2,frame_expansion_ratio = 0.8):\r\n",
        "        self.frame_expansion_ratio = frame_expansion_ratio\r\n",
        "        history = self.__train(epochs,batch_size)\r\n",
        "\r\n",
        "        plt.figure(figsize=(12, 6))\r\n",
        "        plt.plot(history['gen_g_loss'],label='generator grammar loss')\r\n",
        "        plt.plot(history['gen_s_loss'],label='generator similarity loss')\r\n",
        "        if 'dis_loss' in history:\r\n",
        "            plt.plot(history['dis_loss'],label='discriminator grammar loss')\r\n",
        "        plt.legend()\r\n",
        "        plt.show()\r\n",
        "\r\n",
        "        return self\r\n",
        "\r\n",
        "    # text의 생성 for torch\r\n",
        "    def __text_gen2(self, noise, gen_length):\r\n",
        "        gtext = []\r\n",
        "        sorted_noise, i = torch.sort(noise, descending=True)\r\n",
        "        order, i = torch.sort(i[:gen_length], descending=False)\r\n",
        "        #print(len(order))\r\n",
        "        assert len(order) == gen_length\r\n",
        "        order = order.cpu().detach().numpy()\r\n",
        "        for k in order:\r\n",
        "            gtext.append((self.source.term_table[k],k))\r\n",
        "        return gtext\r\n",
        "\r\n",
        "    def __discrete_gradient(self,weights,gen_length,use_gpu=False, verbose=0):\r\n",
        "        fake_gen_out = torch.zeros(weights.shape).to(device)\r\n",
        "        fake_sim_out = torch.zeros(weights.shape).to(device)\r\n",
        "\r\n",
        "        real_text = self.source.get_org_sample(weights.shape[0])\r\n",
        "        fake_outs = []\r\n",
        "        real_outs = []\r\n",
        "        apply_order = []\r\n",
        "        for i, noise in enumerate(weights):\r\n",
        "            gtext = self.__text_gen2(noise,gen_length)\r\n",
        "            tw = \"\"\r\n",
        "            tk = []\r\n",
        "            fake_scores = []\r\n",
        "            for (w,k) in gtext:\r\n",
        "                tw += w + ' '\r\n",
        "                tk.append(k)\r\n",
        "                if w.endswith('.'):\r\n",
        "                    fake_outs.append(tw.strip())\r\n",
        "                    real_outs.append(real_text[i])\r\n",
        "                    apply_order.append((i,tk))\r\n",
        "                    tw = \"\"\r\n",
        "                    tk = []\r\n",
        "                    \r\n",
        "            if len(tk) > 0:\r\n",
        "                fake_outs.append(tw.strip())\r\n",
        "                real_outs.append(real_text[i])\r\n",
        "                apply_order.append((i,tk))\r\n",
        "\r\n",
        "        D_z_loss, fake_gmr_out=self.g_discriminator.transfer_learning(fake_outs,train_for = False)\r\n",
        "        D_x_loss, real_gmr_out=self.g_discriminator.transfer_learning(real_outs,train_for = True)   # not use of 'real_gmr_out'\r\n",
        "\r\n",
        "        f_sim_out = self.s_discriminator.similiraty(fake_outs,self.source.org_text_emb)\r\n",
        "\r\n",
        "        #if use_gpu:\r\n",
        "        #    apply_order = torch.FloatTensor(apply_order).to(device)  \r\n",
        "        \r\n",
        "        #print(fake_dis_out)\r\n",
        "        \r\n",
        "        for j, (i,tk) in enumerate(apply_order):\r\n",
        "            #fake_gen_out[i,tk] += fake_dis_out[j].numpy() --> 이거는 tf 용...\r\n",
        "            #fake_gen_out[i,tk] += fake_dis_out[j] #.cpu().detach().numpy()\r\n",
        "            # \r\n",
        "            fake_gen_out[i,tk] += torch.sigmoid(fake_gmr_out[j,1])\r\n",
        "            fake_sim_out[i,tk] += f_sim_out[j]\r\n",
        "            \r\n",
        "        return fake_gen_out, fake_sim_out, D_z_loss, D_x_loss\r\n",
        "\r\n",
        "\r\n",
        "    def __train(self, epochs=10,batch_size=10):\r\n",
        "        # In the Deepmind paper they use RMSProp however then Adam optimizer\r\n",
        "        # improves training time\r\n",
        "        #generator_optimizer = tf.keras.optimizers.Adam(1e-4)\r\n",
        "        # This method returns a helper function to compute cross entropy loss\r\n",
        "        #cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\r\n",
        "\r\n",
        "        # Set the seed value all over the place to make this reproducible.\r\n",
        "        seed_val = 10\r\n",
        "\r\n",
        "        random.seed(seed_val)\r\n",
        "        np.random.seed(seed_val)\r\n",
        "        torch.manual_seed(seed_val)\r\n",
        "        torch.cuda.manual_seed_all(seed_val)\r\n",
        "        \r\n",
        "        criterion = nn.BCELoss()\r\n",
        "        #D_opt = torch.optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\r\n",
        "        G_opt = torch.optim.Adam(self.generator.parameters(), lr=3e-4)\r\n",
        "        D1_opt = AdamW(self.g_discriminator.discriminator.parameters(),\r\n",
        "                        lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\r\n",
        "                        eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\r\n",
        "                        )\r\n",
        "\r\n",
        "        \r\n",
        "        gen_length = len(self.source.story_peaks) + int(len(self.source.story_peaks)*self.frame_expansion_ratio)\r\n",
        "        pb = ProgressBar(epochs,prefix='Train...')\r\n",
        "        gen_gmr_loss_history = []\r\n",
        "        gen_sim_loss_history = []\r\n",
        "        dis_loss_history = []    \r\n",
        "\r\n",
        "        #model 들은 cuda로 보낸다.\r\n",
        "        self.g_discriminator.discriminator.to(device)\r\n",
        "        self.g_discriminator.discriminator.eval()\r\n",
        "\r\n",
        "        self.generator.to(device)      # gpu 메모리 용량 땜에 디짐....    \r\n",
        "        self.generator.eval()\r\n",
        "\r\n",
        "        initial_bias = 0.5\r\n",
        "        for i in range(epochs):\r\n",
        "\r\n",
        "            noise = torch.randn(batch_size,self.source.org_source_length).to(device)\r\n",
        "            bias = torch.zeros_like(noise).to(device)\r\n",
        "            bias[:,self.source.story_peaks] += initial_bias\r\n",
        "            #with tf.GradientTape() as tape:\r\n",
        "            #print(noise)\r\n",
        "\r\n",
        "            #bias = 0\r\n",
        "            with torch.no_grad():        \r\n",
        "                sw, sw0 = self.generator(noise,bias)\r\n",
        "            initial_bias = torch.max(sw0)\r\n",
        "            self.g_discriminator.discriminator.train()          #discriminator는 evaluation 모드로 전환\r\n",
        "            fake_gmr_out, fake_sim_out, D_z_loss, D_x_loss = self.__discrete_gradient(sw,gen_length)\r\n",
        "            \r\n",
        "            D_loss = D_x_loss + D_z_loss      \r\n",
        "\r\n",
        "            self.g_discriminator.discriminator.zero_grad()\r\n",
        "            D_loss.backward()\r\n",
        "            D1_opt.step()\r\n",
        "            self.g_discriminator.discriminator.eval()\r\n",
        "            #print('D1_opt.step()')\r\n",
        "            if True:\r\n",
        "                self.generator.train()     #generator는 train 모드로 전환\r\n",
        "                noise = torch.randn(batch_size,self.source.org_source_length).to(device)\r\n",
        "                bias = torch.zeros_like(noise).to(device)\r\n",
        "                bias[:,self.source.story_peaks] += initial_bias\r\n",
        "                #bias = 0\r\n",
        "                sw, sw0  = self.generator(noise,bias)\r\n",
        "                initial_bias = torch.max(sw0)\r\n",
        "                self.last_bias_max = torch.max(sw0)\r\n",
        "                with torch.no_grad():                \r\n",
        "                    fake_gmr_out, fake_sim_out, D_z_loss, D_x_loss = self.__discrete_gradient(sw,gen_length)\r\n",
        "                \r\n",
        "                # --> 이게 엄밀히 말하면.. D(G(z)) 은 아닌데... 이걸 수학적으로 어떻게 해석해야 하는가???\r\n",
        "                # grammar loss function...\r\n",
        "                G_g_loss = criterion(sw,fake_gmr_out)\r\n",
        "                # similarity loss function...\r\n",
        "                G_s_loss = criterion(sw,fake_sim_out)\r\n",
        "\r\n",
        "                G_loss = G_g_loss + G_s_loss\r\n",
        "\r\n",
        "                self.generator.zero_grad()\r\n",
        "                G_loss.backward()\r\n",
        "                #print('backward:')\r\n",
        "                G_opt.step()\r\n",
        "                self.generator.eval()\r\n",
        "            #print('step:')\r\n",
        "            gen_gmr_loss_history.append(G_g_loss.cpu().detach().numpy())\r\n",
        "            gen_sim_loss_history.append(G_s_loss.cpu().detach().numpy())\r\n",
        "            dis_loss_history.append(D_loss.cpu().detach().numpy())\r\n",
        "\r\n",
        "            pb.printProgress(+1,f'{i+1}/{epochs} epochs, Generator / grammar loss:{G_g_loss}   similarity loss:{G_s_loss},   Discriminator grammar_loss:{D_loss}        ')\r\n",
        "            \r\n",
        "            \r\n",
        "        self.generator.eval()\r\n",
        "        self.g_discriminator.discriminator.eval()\r\n",
        "        \r\n",
        "        plt.figure(figsize=(12, 6))\r\n",
        "        plt.plot(sw0[0].cpu().detach().numpy(),label='before activation weights')\r\n",
        "        plt.plot(sw[0].cpu().detach().numpy(),label='after activation weights')\r\n",
        "        plt.legend()        \r\n",
        "        plt.show()\r\n",
        "\r\n",
        "        return  {'gen_g_loss':gen_gmr_loss_history,'gen_s_loss':gen_sim_loss_history,'dis_loss':dis_loss_history }\r\n",
        "\r\n",
        "    def get_summary(self, count):\r\n",
        "        texts = []\r\n",
        "        self.generator.cpu()\r\n",
        "        self.generator.eval()\r\n",
        "        gen_length = len(self.source.story_peaks) + int(len(self.source.story_peaks)*self.frame_expansion_ratio)\r\n",
        "        noise = torch.randn(count,self.source.org_source_length)\r\n",
        "        bias = torch.zeros_like(noise)\r\n",
        "        #bias = torch.randn(1,self.source.org_source_length)\r\n",
        "        bias[:,self.source.story_peaks] += self.last_bias_max.cpu().detach().numpy()\r\n",
        "        #bias = 0\r\n",
        "        with torch.no_grad():\r\n",
        "            sw,sw0 = self.generator(noise,bias)\r\n",
        "\r\n",
        "        for noise in sw:\r\n",
        "            gtext = self.__text_gen2(noise,gen_length)\r\n",
        "            text = ' '.join([w for (w,k) in gtext])\r\n",
        "            #print(text)\r\n",
        "            texts.append(text)\r\n",
        "        return texts"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yuWcED5mruy",
        "outputId": "35c6ee46-4380-4083-c1b2-eb5a79c1e75c"
      },
      "source": [
        "use_pretrained_model = True\r\n",
        "\r\n",
        "if use_pretrained_model:\r\n",
        "    g_discriminator = Grammar_Discriminator(input_dir = './drive/MyDrive/Colab Notebooks/summary/grammar_check_model')\r\n",
        "else:\r\n",
        "    urls = ['https://raw.githubusercontent.com/dolmani38/Summary/master/data/korean_sample.txt',\r\n",
        "            'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-ABC%20%EC%82%B4%EC%9D%B8%EC%82%AC%EA%B1%B4.txt',\r\n",
        "            'https://raw.githubusercontent.com/dolmani38/Summary/master/data/%EC%95%A0%EA%B1%B0%EC%84%9C%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8B%B0-%EA%B7%B8%EB%A6%AC%EA%B3%A0%20%EC%95%84%EB%AC%B4%EB%8F%84%20%EC%97%86%EC%97%88%EB%8B%A4.txt']\r\n",
        "    sentences,labels = collect_training_dataset_for_grammar_discriminator(source_urls=urls)\r\n",
        "    g_discriminator = Grammar_Discriminator()\r\n",
        "    g_discriminator.set_dataset(sentences,labels)\r\n",
        "    g_discriminator.train(epochs=2)\r\n",
        "    g_discriminator.save_model()\r\n",
        "\r\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxVKp7eW5hBd",
        "outputId": "b4e743c2-d538-4999-dbde-8c20c90bd6d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "s_discriminator = Similarity_Discriminator()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating Similarity_Discriminator object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IV0k7A30XPHR"
      },
      "source": [
        "org_text = \"\"\"나무꾼이 나무를 하다가 숲 속에서 도망치는 사슴을 만났는데, 이 사슴이 사냥꾼이 쫓아오고 있으니 자신을 숨겨달라고 말했다. 말하는 사슴을 신기하게 여긴 나무꾼이 사슴을 숨겨줬고, 뒤쫓아 온 사냥꾼을 다른 방향으로 보내서 구해주었다.\r\n",
        "사슴은 은혜를 갚겠다고 하면서, 나무꾼에게 선녀들이 하늘에서 내려와서 목욕하는 선녀탕이라는 샘과 선녀들이 목욕을 하러 오는 시기, 선녀의 옷을 훔쳐 그를 아내로 삼도록 하는 꾀를 나무꾼에게 가르쳐 주었다. 나무꾼은 반신반의 하면서도 사슴이 가르쳐준 시기에 선녀들이 목욕을 하러 내려온다는 샘으로 찾아가 몸을 숨겼다. 그렇게 잠시간 기다리자 과연, 선녀들이 하늘에서 내려와 날개옷을 벗고 선녀탕에서 목욕을 하는 것이었다. 나무꾼은 사슴이 가르쳐준 대로 날개옷을 하나 훔쳤다.\r\n",
        "날개옷이 없어진 탓에 한 명의 선녀는 하늘로 올라가지 못했으며 다른 선녀들은 날개옷이 없는 선녀를 내버려두고 하늘로 돌아갔다. 이때 나무꾼이 홀로 남은 선녀에게 자신의 아내가 되어달라고 하자, 하늘나라로 올라가지 못하게 된 선녀는 할 수 없이 나무꾼에게 의탁하게 되었다.\r\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SntCD-e-LOr7"
      },
      "source": [
        "org_text = \"\"\"옛날 어느 집에 귀여운 여자 아기가 태어났어요.\r\n",
        "아기는 무럭무럭 자라서, 예쁘고 마음씨 고운 소녀가 되었어요.\r\n",
        "그러던 어느날, 소녀의 어머니가 병이들어 그만 세상을 떠나고 말았어요.\r\n",
        "소녀의 아버지는 홀로 남은 소녀가 걱정되었어요.\r\n",
        "그래서 얼마 후 새어머니를 맞이했어요.\r\n",
        "새어머니는 소녀보다 나이가 위인 두 딸을 데리고 왔어요.\r\n",
        "그러나 새어머니와 언니들은 성질이 고약한 심술쟁이들이었어요.\r\n",
        "새어머니는 소녀가 자기 딸들보다 예쁘고 착한 게 못마땅했어요.\r\n",
        "그런데 이번에는 아버지마저 돌아가셨어요.\r\n",
        "소녀는 하녀처럼 하루 종일 쓸고, 닦고, 집안일을 도맡아 했어요.\r\n",
        "해도 해도 끝이 없는 집안일이 힘들어 지칠때면\r\n",
        "난롯가에 앉아서 잠시 쉬곤 했지요.\r\n",
        "\"엄마, 저애를 신데렐라라고 불러야겠어요.\"\r\n",
        "\"온통 재투성이잖아요. 호호호!\" 두 언니는 소녀를 놀려 댔어요.\r\n",
        "어느 날, 왕궁에서 무도회가 열렸어요.\r\n",
        "신데렐라의 집에도 초대장이 왔어요.\r\n",
        "새어머니는 언니들을 데리고 무도회장으로 떠났어요.\r\n",
        "신데렐라도 무도회에 가고 싶었어요.\r\n",
        "혼자 남은 신데렐라는 훌쩍훌쩍 울기 시작했어요.\r\n",
        "\"신데렐라, 너도 무도회에 가고 싶니?\"\r\n",
        "신데렐라가 고개를 들어보니, 마법사 할머니가 빙그레 웃고 있었어요.\r\n",
        "\"내가 너를 무도회에 보내주마 호박 한개와 생쥐 두마리, 도마뱀을 구해 오렴.\"\r\n",
        "마법사 할머니가 주문을 외웠어요.\r\n",
        "그리고 지팡이로 호박을 건드리자, 호박이 화려한 황금 마차로 변했어요.\r\n",
        "이번에는 생쥐와 도마뱀을 건드렸어요.\r\n",
        "그랬더니 생쥐는 흰말로, 도마뱀은 멋진 마부로 변했답니다.\r\n",
        "신데렐라의 옷도 구슬 장식이 반짝이는 예쁜 드레스로 바뀌웠어요.\r\n",
        "\"신데렐라, 발을 내밀어 보거라.\"\r\n",
        "할머니는 신데렐라에게 반짝반짝 빛나는 유리 구두를 신겨 주었어요.\r\n",
        "\"신데렐라, 밤 열두시가 되면 모든게 처음대로 돌아간단다. 황금 마차는 호박으로, 흰말은 생쥐로, 마부는 도마뱀으로 변하게 돼. 그러니까 반드시 밤 열두 시가 되기 전에 돌아와야 해. 알겠지?\"\r\n",
        "왕자님도 아름다운 신데렐라에게 마음을 빼았겼어요.\r\n",
        "왕자님은 무도회장에 모인 다른 아가씨들은 쳐다보지도 않고,신데렐라하고만 춤을 추었어요.\r\n",
        "신데렐라는 왕자님과 춤을 추느라 시간 가는 줄도 몰랐어요.\r\n",
        "땡, 땡, 땡...... 벽시계가 열두 시를 알리는 소리에 신데렐라는 화들짝 놀랐어요.\r\n",
        "신데렐라가 허둥지둥 왕궁을 빠져나가는데,\r\n",
        "유리 구두 한 짝이 벗겨졌어요.\r\n",
        "하지만 구두를 주울 틈이 없었어요.\r\n",
        "신데렐라를 뛰쫓아오던 왕자님은 층계에서 유리 구두 한 짝을 주웠어요.\r\n",
        "왕자님은 유리 구두를 가지고 임금님께 가서 말했어요.\r\n",
        "\"이 유리 구두의 주인과 결혼하겠어요.\"\r\n",
        "그래서 신하들은 유리 구두의 주인을 찾아 온 나라를 돌아다녔어요.\r\n",
        "언니들은 발을 오므려도 보고, 구두를 늘려도 보았지만 한눈에 보기에도 유리 구두는 너무 작았어요.\r\n",
        "그때, 신데렐라가 조용히 다가와 말했어요.\r\n",
        "\"저도 한번 신어 볼 수 있나요?\"\r\n",
        "신데렐라는 신하게 건넨 유리 구두를 신었어요,\r\n",
        "유리 구두는 신데렐라의 발에 꼭 맞았어요.\r\n",
        "신하들은 신데렐라를 왕궁으로 데리고 갔어요.\r\n",
        "그 뒤 신데렐라는 왕자님과 결혼하여 오래오래 행복하게 살았대요.\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mtx-xJovOCDe"
      },
      "source": [
        "source = Source(org_text)\r\n",
        "source.analysis_frame_terms(s_discriminator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trcC9GJV00Yo"
      },
      "source": [
        "summarizer = Summarizer(g_discriminator,s_discriminator)\r\n",
        "#source = Source(org_text)\r\n",
        "summarizer.ready(source)\r\n",
        "summarizer.summarize(epochs=30,batch_size=2,frame_expansion_ratio = 1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2x0r-ASXe4Oy"
      },
      "source": [
        "summarizer.get_summary(4)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
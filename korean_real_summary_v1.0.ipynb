{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "korean_real_summary_no_morp_v0.1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dolmani38/Summary/blob/master/korean_real_summary_v1.0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdM3q73ReHxs"
      },
      "source": [
        "# **Korean Summarizer Using Multiple Discriminators**\n",
        "\n",
        "참조 : https://www.kaggle.com/shivamb/beginners-guide-to-text-generation-using-lstms\n",
        "\n",
        "참조 : https://github.com/williamSYSU/TextGAN-PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlvsCFJaeHxt",
        "outputId": "be0f624a-0075-4dcd-ce18-87c852d923d5"
      },
      "source": [
        "!pip install sentence-transformers==0.3.0\n",
        "!pip install transformers==3.0.2\n",
        "!pip install wikipedia\n",
        "!pip install konlpy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers==0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/23/833e0620753a36cb2f18e2e4a4f72fd8c49c123c3f07744b69f8a592e083/sentence-transformers-0.3.0.tar.gz (61kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.4MB/s \n",
            "\u001b[?25hCollecting transformers>=3.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 12.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.0) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.0) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.0) (1.19.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.0) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.0) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.0) (3.2.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (20.8)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 43.2MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 46.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (0.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (2.23.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.1->sentence-transformers==0.3.0) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.1->sentence-transformers==0.3.0) (3.7.4.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers==0.3.0) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers==0.3.0) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers>=3.0.2->sentence-transformers==0.3.0) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=3.0.2->sentence-transformers==0.3.0) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (2020.12.5)\n",
            "Building wheels for collected packages: sentence-transformers, sacremoses\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.3.0-cp36-none-any.whl size=86754 sha256=bdfc43d8691bf50a903f4f18ecf75ac438ee97fbdf384970cef1fe08693ab546\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/23/85/85d6a9a6c68f0625a1ecdaad903bb0a78df058c10cf74f9de4\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=4ce631f26286e89f480b3e45b86779250bf8e3cbd81c149e1e3d03fa8e42d845\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sentence-transformers sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers, sentence-transformers\n",
            "Successfully installed sacremoses-0.0.43 sentence-transformers-0.3.0 tokenizers-0.9.4 transformers-4.1.1\n",
            "Collecting transformers==3.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 13.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.0.43)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (4.41.1)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 50.3MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 48.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (1.19.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (20.8)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.0.2) (2.4.7)\n",
            "Installing collected packages: tokenizers, sentencepiece, transformers\n",
            "  Found existing installation: tokenizers 0.9.4\n",
            "    Uninstalling tokenizers-0.9.4:\n",
            "      Successfully uninstalled tokenizers-0.9.4\n",
            "  Found existing installation: transformers 4.1.1\n",
            "    Uninstalling transformers-4.1.1:\n",
            "      Successfully uninstalled transformers-4.1.1\n",
            "Successfully installed sentencepiece-0.1.94 tokenizers-0.8.1rc1 transformers-3.0.2\n",
            "Collecting wikipedia\n",
            "  Downloading https://files.pythonhosted.org/packages/67/35/25e68fbc99e672127cc6fbb14b8ec1ba3dfef035bf1e4c90f78f24a80b7d/wikipedia-1.4.0.tar.gz\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-cp36-none-any.whl size=11686 sha256=049aea38bb0f967fb297ed65e3dacff726a5ba2179991d5b7b0b0fcfd5477df1\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/2a/18/4e471fd96d12114d16fe4a446d00c3b38fb9efcb744bd31f4a\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n",
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 168kB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/21/9e2c0dbf9df856e6392a1aec1d18006c60b175aa4e31d351e8278a8a63c0/JPype1-1.2.0-cp36-cp36m-manylinux2010_x86_64.whl (453kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 52.7MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 11.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.19.4)\n",
            "Collecting tweepy>=3.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/67/c3/6bed87f3b1e5ed2f34bd58bf7978e308c86e255193916be76e5a5ce5dfca/tweepy-3.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Installing collected packages: JPype1, colorama, beautifulsoup4, tweepy, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "  Found existing installation: tweepy 3.6.0\n",
            "    Uninstalling tweepy-3.6.0:\n",
            "      Successfully uninstalled tweepy-3.6.0\n",
            "Successfully installed JPype1-1.2.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2 tweepy-3.10.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Em1oCkJceHxz"
      },
      "source": [
        "# keras module for building LSTM \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "from tensorflow.python.framework import tensor_shape\n",
        "import keras.utils as ku \n",
        "\n",
        "# set seeds for reproducability\n",
        "from tensorflow.random import set_seed\n",
        "from numpy.random import seed\n",
        "set_seed(2)\n",
        "seed(1)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string, os \n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFdj0QfSeHx1"
      },
      "source": [
        "# 학습을 위한 데이터 준비"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94NlJEeHeHx3"
      },
      "source": [
        "네이버 뉴스에서 아무거나 하나 Text를 얻어옴\n",
        "\n",
        "이것을 '요약' 목표"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "lz5XtC9MeHx5"
      },
      "source": [
        "org_text = \"\"\"\n",
        "주호영 국민의힘 원내대표는 22일 고위공직자범죄수사처(공수처)법 개정과 가덕도 신공항 건설 등을 밀어붙이고 있는 문재인 정권과 더불어민주당을 향해 \"이제 끝이 보인다\"며 \"짓밟힌 풀들이 아우성 치는 국민적 저항에 직면할 것\"이라고 경고했다.\n",
        "주 원내대표는 이날 자신의 페이스북에 \"문재인 정권이 공수처법 개정을 위한 '군사작전'에 돌입하겠다고 엄포를 놓고 있다\"며 \"정의당을 끌어들이기 위해 꼼수 선거법에 묶어 '패스트트랙'이라는 불법·탈법으로 만들어낸 공수처법을 시행도 해보지 않고 고치려 하는 것\"이라고 지적했다.\n",
        "이어 주 원내대표는 \"야당 원내대표인 제게 문재인 대통령은 사람 좋아보이는 표정으로 '공수처는 야당의 동의 없이는 절대 출범할 수 없는 것'이라고 얘기했고, 야당이 유엔 안보리 상임이사국처럼 공수처장 임명에 '비토권'을 행사할 수 있는데 무얼 걱정하느냐고, 여당 사람들이 우리를 속였다\"며 \"거짓말이라는 비난을 개의치 않는 사람들\"이라고 꼬집었다.\n",
        "주 원내대표는 \"이해찬 전 민주당 대표가 얘기한 '민주당 20년 집권'의 토대가 올해 안에 완성된다\"며 \"탈원전과 동남권 신공항은 문 대통령이 대선 공약으로 내건 사업이니 여기에 불법이 있었다고 시비를 거는 것은 민주주의를 부정하는 것이라고 청와대 출신 윤건영 민주당 의원이 윽박지른다. 이제 '민주주의 없는 민주당'이 법위에 군림하는 '반민주'를 거리낌없이 획책하는 것\"이라고 언급했다.\n",
        "그러면서 주 원내대표는 \"표를 얻기 위해 나라 곳간을 다 허물어뜨렸고, 재정 운용에서 신중함은 사라졌다\"며 \"괴물 공수처가 출범하면 공무원 누구나 대통령과 권력이 지시하는 범죄행위에 거리낌 없이 가담할 것이다. 청와대와 권부 요직에 앉아 불법으로 각종 이권을 챙기는 권력자들에 대한 사건이 불거져도 공수처가 사건을 가져가 버리면 그만\"이라고 우려했다.\n",
        "주 원내대표는 \"문 대통령은 제게 '공수처는 고위 공직자들을 처벌하는 것인데 왜 야당이 반대하는지 이해할 수 없다'고 했는데, 그런 분이 청와대와 대통령 주변을 감시하는 특별감찰관은 취임 이후 지금까지 왜 임명하지 않았는가\"라며 \"공수처는 권력형 비리의 쓰레기 하치장, 종말 처리장이 될 것\"이라고 비판했다.\n",
        "문재인 정부를 향해 주 원내대표는 \"문 대통령과 그 사도들은 법치가 미치지 않는 무오류의 화신이 될 것\"이라며 \"오류를 인정하지 않는 존재가 바로 신이며 그 아래에는 자신들의 지도자를 목숨바쳐 지킴으로서 정의를 실현하겠다는 추종자들로 넘쳐 난다. 공수처는 지도자의 신성을 인정하지 않는 세력을 정죄하는 수단으로 전락할 것\"이라고 질타했다.\n",
        "주 원내대표는 \"저도 법조인이지만 대통령과 공수처장이 마음대로 검사들과 수사관들을 임명하는 이 끔찍한 사법기구가 어떤 일을 할지 두렵기만 하다\"며 \"공수처는 검찰과 경찰 위에 있는 사법기구로, 헌법과 법으로 독립성을 보장하는 검찰총장을 이렇게 핍박하는 정권이 공수처를 어떻게 운영할지 불을 보듯 뻔한 일\"이라고 예측했다.\n",
        "그러면서 주 원내대표는 \"추미애 법무장관을 앞장 세워 윤석열 검찰의 권력 비리 수사를 저지하려다가 난관에 봉착하자 무슨 수를 써서라도 공수처를 출범시키려 한다. 공수처장 자리에는 추미애보다 더 한 막무가내 내 편을 앉힐 게 분명한 것\"이라며 \"문 정권의 파렴치와 오만함을 최전선에서 온 몸으로 겪어온 저로서는 민주당이 내일부터 국회에서 보일 행태가 환히 보인다. 180석의 민주당이 또 군사작전을 개시하면 그걸 누가 막겠는가\"라고 성토했다.\n",
        "주 원내대표는 \"공수처법을 막을 힘이 우리 야당에게는 없다. 삭발하고 장외투쟁해 봐야 눈 하나 깜짝할 사람들이 아닌 것\"이라며 \"대란대치(大亂大治), 세상을 온통 혼돈 속으로 밀어넣고 그걸 권력 유지에 이용한다는 게 이 정권의 통치기술\"이라고 규탄했다.\n",
        "아울러 주 원내대표는 \"권력은 바람, 국민은 풀이다. 바람이 불면 청보리 밭의 보리가 눕는다\"며 \"권력은 풀들이 다시는 일어서지 못하도록 풀을 짓밟지만 풀들은 다시 일어난다. 시인 김수영은 '바람보다 먼저 눕지만, 바람보다 먼저 일어나는' 민초의 힘을 노래했다\"고 말했다.\n",
        "마지막으로 주 원내대표는 \"문재인 정권은 이제 곧 국회에서 광장에서 짓밟힌 풀들이 일어서서 아우성치는 모습을 지켜보게 될 것\"이라며 \"대란대치를 끝장내려는 국민적 저항에 직면할 것\"이라고 거듭 강조했다.\n",
        "\"\"\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-k66tHNeHx6"
      },
      "source": [
        "한국어 문법체계에 따라 요약문을 생성하기 위해 한국어 문장 샘플을 준비\n",
        "\n",
        "'한글 위키백과'에서 임의의 문장을 수집 함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "oi6AfKSzeHx7"
      },
      "source": [
        "#한국어 위키백과에서 스크랩핑\n",
        "\n",
        "import wikipedia as wiki\n",
        "wiki.set_lang('ko')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekFwbQVxeHx7",
        "outputId": "a93bf723-535e-492d-d641-b7aa50a454c0"
      },
      "source": [
        "# '전래동화' 라는 keyword로 100개 page의 Text를 취득\n",
        "\n",
        "def __search_from_wiki(question,max_rank):\n",
        "    results = wiki.search(question,results=max_rank)\n",
        "    print(results)\n",
        "    contents = []\n",
        "    for result in results:\n",
        "        try:\n",
        "            page = wiki.page(result)\n",
        "            #print(f\"Top wiki result: {page}\")\n",
        "            text = page.content\n",
        "            ln = len(text)\n",
        "            print(f'Collecting page : {page} , text length {str(ln)}')\n",
        "            #if ln < 4000:\n",
        "            #  contents.append(text)\n",
        "            #else:\n",
        "            #  contents.append(text[0:4000])\n",
        "            contents.append(text)\n",
        "        except Exception as ex:\n",
        "          print(ex)\n",
        "    return contents\n",
        "\n",
        "\n",
        "ko_grammar_set_raw = __search_from_wiki(\"전래동화\", 100)\n",
        "\n",
        "print(f'전체 수집한 Page Count : {len(ko_grammar_set_raw)}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['동화', '신 전래동화', '꾸러기 수비대', '아동 문학', '호시조라 미유키', '한국의 사찰', '해와 달이 된 오누이', '잠자는 숲속의 미녀', '거북', '이한갈', '정은찬', '옛날 옛적에 (애니메이션)', '김기두 (배우)', '계룡선녀전 (드라마)', '동요', '이상훈 (1976년)', '최홍일', '육진수 (격투기 선수)', '제비', '장석현 (연예인)', '유다미', '밀교 (불교)', '최지웅', '박재훈 (배우)', '재희', '한다은', '정미남', '안젤리나 다닐로바', '도깨비', '기탄교육', '국지용', '의왕백운호수축제', '박민경', '정정아', '콩딱쿵! 이야기 주머니', '윤기원 (배우)', '선녀와 나무꾼', '남생이', '콩쥐팥쥐 (동음이의)', '대장화홍련전', '토끼전', '지대한', '이설구', '빨간 자전거', '미녀와 야수', '이서휴게소', '은비까비의 옛날옛적에', '콩쥐팥쥐전', '김덕현 (배우)', '아시리아인', '티베트', '백조의 호수', '홍석연', '서예', '도교', '켈트 다신교', '금도끼 은도끼', '제네시스 (밴드)', '장화, 홍련 (동음이의)', '허구 국가', '노상현', '타이완의 문화', '도깨비 (동음이의)', '골디락스', '자와어', '선녀강림', '손춘익', '안동국제탈춤페스티벌', '윌리엄 버틀러 예이츠', '성덕대왕신종', '모모타로', '고려', '한국 문학', '라푼젤 (영화)', '토비트', '조선 후기의 문학', '송월동 동화마을', '계룡선녀전', '홍석천', '이집트', '응우옌 왕조', '아시아의 역사', '안지환', '외래어', '프랑스인', '스키타이족', '메이플 월드', '네버랜드', '인도네시아', '프랑크인', '이탈리아', '일본 제국', 'MBC 창작동요제', '드랑 나흐 오스텐', '김환영 (작가)', '바이킹', '조선', '원나라', '오윤 (화가)', '앱북']\n",
            "Collecting page : <WikipediaPage '동화'> , text length 685\n",
            "Collecting page : <WikipediaPage '신 전래동화'> , text length 156\n",
            "Collecting page : <WikipediaPage '꾸러기 수비대'> , text length 1554\n",
            "Collecting page : <WikipediaPage '아동 문학'> , text length 678\n",
            "Collecting page : <WikipediaPage '호시조라 미유키'> , text length 1601\n",
            "Collecting page : <WikipediaPage '한국의 사찰'> , text length 885\n",
            "Collecting page : <WikipediaPage '해와 달이 된 오누이'> , text length 392\n",
            "Collecting page : <WikipediaPage '잠자는 숲속의 미녀'> , text length 2037\n",
            "Collecting page : <WikipediaPage '거북'> , text length 1381\n",
            "Collecting page : <WikipediaPage '이한갈'> , text length 776\n",
            "Collecting page : <WikipediaPage '정은찬'> , text length 1096\n",
            "Collecting page : <WikipediaPage '옛날 옛적에 (애니메이션)'> , text length 1080\n",
            "Collecting page : <WikipediaPage '김기두 (배우)'> , text length 1767\n",
            "Collecting page : <WikipediaPage '계룡선녀전 (드라마)'> , text length 1416\n",
            "Collecting page : <WikipediaPage '동요'> , text length 1429\n",
            "Collecting page : <WikipediaPage '이상훈 (1976년)'> , text length 831\n",
            "Collecting page : <WikipediaPage '최홍일'> , text length 1917\n",
            "Collecting page : <WikipediaPage '육진수 (격투기 선수)'> , text length 749\n",
            "Collecting page : <WikipediaPage '제비'> , text length 1125\n",
            "Collecting page : <WikipediaPage '장석현 (연예인)'> , text length 576\n",
            "Collecting page : <WikipediaPage '유다미'> , text length 409\n",
            "Collecting page : <WikipediaPage '밀교 (불교)'> , text length 4379\n",
            "Collecting page : <WikipediaPage '최지웅'> , text length 712\n",
            "Collecting page : <WikipediaPage '박재훈 (배우)'> , text length 1962\n",
            "Collecting page : <WikipediaPage '재희'> , text length 1229\n",
            "Collecting page : <WikipediaPage '한다은'> , text length 634\n",
            "Collecting page : <WikipediaPage '정미남'> , text length 970\n",
            "Collecting page : <WikipediaPage '안젤리나 다닐로바'> , text length 879\n",
            "Collecting page : <WikipediaPage '도깨비'> , text length 3171\n",
            "Collecting page : <WikipediaPage '기탄교육'> , text length 245\n",
            "Collecting page : <WikipediaPage '국지용'> , text length 420\n",
            "Collecting page : <WikipediaPage '의왕백운호수축제'> , text length 272\n",
            "Collecting page : <WikipediaPage '박민경'> , text length 885\n",
            "Collecting page : <WikipediaPage '정정아'> , text length 1119\n",
            "Collecting page : <WikipediaPage '콩딱쿵! 이야기 주머니'> , text length 122\n",
            "Collecting page : <WikipediaPage '윤기원 (배우)'> , text length 2555\n",
            "Collecting page : <WikipediaPage '선녀와 나무꾼'> , text length 173\n",
            "Collecting page : <WikipediaPage '남생이'> , text length 502\n",
            "\"콩쥐팥쥐 (동음이의)\" may refer to: \n",
            "콩쥐팥쥐\n",
            "콩쥐팥쥐 (1967년 영화)\n",
            "콩쥐팥쥐 (1958년 영화)\n",
            "Collecting page : <WikipediaPage '대장화홍련전'> , text length 216\n",
            "Collecting page : <WikipediaPage '토끼전'> , text length 3931\n",
            "Collecting page : <WikipediaPage '지대한'> , text length 3296\n",
            "Collecting page : <WikipediaPage '이설구'> , text length 3021\n",
            "Collecting page : <WikipediaPage '빨간 자전거'> , text length 2955\n",
            "Collecting page : <WikipediaPage '미녀와 야수'> , text length 2977\n",
            "Collecting page : <WikipediaPage '정안알밤휴게소'> , text length 808\n",
            "Collecting page : <WikipediaPage '은비까비의 옛날옛적에'> , text length 940\n",
            "Collecting page : <WikipediaPage '콩쥐팥쥐전'> , text length 2912\n",
            "Collecting page : <WikipediaPage '김덕현 (배우)'> , text length 1986\n",
            "Collecting page : <WikipediaPage '아시리아인'> , text length 2793\n",
            "Collecting page : <WikipediaPage '티베트'> , text length 5093\n",
            "Collecting page : <WikipediaPage '백조의 호수'> , text length 2130\n",
            "Collecting page : <WikipediaPage '홍석연'> , text length 3008\n",
            "Collecting page : <WikipediaPage '서예'> , text length 10102\n",
            "Collecting page : <WikipediaPage '도교'> , text length 6686\n",
            "Collecting page : <WikipediaPage '켈트 다신교'> , text length 4308\n",
            "Collecting page : <WikipediaPage '금도끼 은도끼'> , text length 1259\n",
            "Collecting page : <WikipediaPage '제네시스 (밴드)'> , text length 5457\n",
            "\"장화, 홍련 (동음이의)\" may refer to: \n",
            "장화홍련전\n",
            "장화, 홍련\n",
            "장화, 홍련\n",
            "Collecting page : <WikipediaPage '허구 국가'> , text length 2702\n",
            "Collecting page : <WikipediaPage '노상현'> , text length 351\n",
            "Collecting page : <WikipediaPage '타이완의 문화'> , text length 1581\n",
            "\"도깨비 (동음이의)\" may refer to: \n",
            "도깨비\n",
            "도깨비\n",
            "눈물을 마시는 새\n",
            "레인보우 식스 시즈\n",
            "도깨비가 간다\n",
            "도깨비가 간다\n",
            "Tokebi\n",
            "도깨비 (DokeV)\n",
            "Collecting page : <WikipediaPage '골디락스'> , text length 1243\n",
            "Collecting page : <WikipediaPage '자와어'> , text length 3770\n",
            "Collecting page : <WikipediaPage '선녀강림'> , text length 1856\n",
            "Collecting page : <WikipediaPage '손춘익'> , text length 1687\n",
            "Collecting page : <WikipediaPage '안동국제탈춤페스티벌'> , text length 2123\n",
            "Collecting page : <WikipediaPage '윌리엄 버틀러 예이츠'> , text length 12057\n",
            "Collecting page : <WikipediaPage '성덕대왕신종'> , text length 1934\n",
            "Collecting page : <WikipediaPage '모모타로'> , text length 1980\n",
            "Collecting page : <WikipediaPage '고려'> , text length 30322\n",
            "Collecting page : <WikipediaPage '한국 문학'> , text length 14908\n",
            "Collecting page : <WikipediaPage '라푼젤 (영화)'> , text length 9736\n",
            "Collecting page : <WikipediaPage '토비트'> , text length 9004\n",
            "Collecting page : <WikipediaPage '조선 후기의 문학'> , text length 5280\n",
            "Collecting page : <WikipediaPage '송월동 동화마을'> , text length 896\n",
            "Collecting page : <WikipediaPage '계룡선녀전'> , text length 2268\n",
            "Collecting page : <WikipediaPage '홍석천'> , text length 8717\n",
            "Collecting page : <WikipediaPage '이집트'> , text length 11797\n",
            "Collecting page : <WikipediaPage '응우옌 왕조'> , text length 14070\n",
            "Collecting page : <WikipediaPage '아시아의 역사'> , text length 11513\n",
            "Collecting page : <WikipediaPage '안지환'> , text length 12107\n",
            "Collecting page : <WikipediaPage '외래어'> , text length 5000\n",
            "Collecting page : <WikipediaPage '프랑스인'> , text length 12556\n",
            "Collecting page : <WikipediaPage '스키타이족'> , text length 30924\n",
            "Collecting page : <WikipediaPage '메이플 월드'> , text length 5895\n",
            "Collecting page : <WikipediaPage '네버랜드'> , text length 8303\n",
            "Collecting page : <WikipediaPage '인도네시아'> , text length 30606\n",
            "Collecting page : <WikipediaPage '프랑크인'> , text length 23128\n",
            "Collecting page : <WikipediaPage '이탈리아'> , text length 42736\n",
            "Collecting page : <WikipediaPage '일본 제국'> , text length 15259\n",
            "Collecting page : <WikipediaPage 'MBC 창작동요제'> , text length 1724\n",
            "Collecting page : <WikipediaPage '드랑 나흐 오스텐'> , text length 4331\n",
            "Collecting page : <WikipediaPage '김환영 (작가)'> , text length 4105\n",
            "Collecting page : <WikipediaPage '바이킹'> , text length 22255\n",
            "Collecting page : <WikipediaPage '조선'> , text length 30998\n",
            "Collecting page : <WikipediaPage '원나라'> , text length 15537\n",
            "Collecting page : <WikipediaPage '오윤 (화가)'> , text length 2772\n",
            "Collecting page : <WikipediaPage '앱북'> , text length 4534\n",
            "전체 수집한 Page Count : 97\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfcHLkfGeHx8",
        "outputId": "d4546b9a-475a-4676-deaa-513a43cf05ef"
      },
      "source": [
        "ko_grammar_set_raw += __search_from_wiki(\"역사\", 100)\n",
        "\n",
        "print(f'전체 수집한 Page Count : {len(ko_grammar_set_raw)}')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['역사', '한국의 역사', '불교의 역사', '대체 역사', '일본의 역사', '스위스 역사 사전', '유럽의 역사', '고려의 역사', '세계의 역사', '조선의 역사', '동북아역사재단', '우즈베키스탄의 역사', '중국의 역사', '대한민국의 역사 드라마 목록', '한국 불교의 역사', '역사주의', '서울역사박물관', '오키나와현의 역사', '역사학', '러시아의 역사', '역사적 예수', '다큐멘터리 역사를 찾아서', '소련의 역사', '조선민주주의인민공화국의 역사', '조지아의 역사', '영국의 역사', '역사철학', '프랑스의 역사', '역사신학', '애플의 역사', '인터넷의 역사', '유엔의 역사', '역사지진', '역사저널 그날', '힌두교의 역사', '역사 시대', '역사소설', '독일의 역사', '압하지야의 역사', '아일랜드의 역사', '구글의 역사', '영화의 역사', '기독교의 역사', '운영 체제의 역사', '몬테네그로의 역사', '태국의 역사', '음소문자의 역사', '사기 (역사서)', '파키스탄의 역사', '사학자', '그리스의 역사', '베트남의 역사', '기술의 역사', '역사스페셜', '의사 역사학', '안드로이드 버전 역사', '서울역', '미국 육군 역사관', '인도의 역사', '역사언어학', '벨라루스의 역사', '퀴디치의 역사', '사극', '역사적 유물론', '마리아론의 역사', 'FC 서울의 역사', '역사 (헤로도토스)', '폭력의 역사', '교육의 역사', '중화민국의 역사', '터키의 역사', '몽골의 역사', '근대 그리스의 역사', '베트남 역사박물관', '스페인의 역사', '우크라이나의 역사', '핀란드의 역사', '맨체스터 유나이티드 FC의 역사', '음악의 역사', '이란의 역사', '에스페란토의 역사', '타이완의 역사', '민자역사', '역사 재현', '크림반도의 역사', '라오스의 역사', '제2차 세계 대전 기간 미국의 군사 역사', '브리튼 제도의 역사', '이집트의 역사', '한국사 연표', '전주역사박물관', '강화역사박물관', '아르메니아의 역사', '생물의 진화 역사', '군사사', '건축의 역사', '예수의 역사적 실존', '대한민국의 역사', '홍콩의 역사', '포르투갈의 역사']\n",
            "Collecting page : <WikipediaPage '역사'> , text length 5355\n",
            "Collecting page : <WikipediaPage '한국의 역사'> , text length 35878\n",
            "Collecting page : <WikipediaPage '불교의 역사'> , text length 10999\n",
            "Collecting page : <WikipediaPage '대체 역사'> , text length 12122\n",
            "Collecting page : <WikipediaPage '일본의 역사'> , text length 18518\n",
            "Collecting page : <WikipediaPage '스위스 역사 사전'> , text length 3401\n",
            "Collecting page : <WikipediaPage '유럽의 역사'> , text length 43372\n",
            "Collecting page : <WikipediaPage '고려의 역사'> , text length 1222\n",
            "Collecting page : <WikipediaPage '세계의 역사'> , text length 12393\n",
            "Collecting page : <WikipediaPage '조선의 역사'> , text length 18208\n",
            "Collecting page : <WikipediaPage '동북아역사재단'> , text length 2202\n",
            "Collecting page : <WikipediaPage '우즈베키스탄의 역사'> , text length 2106\n",
            "Collecting page : <WikipediaPage '중국의 역사'> , text length 7922\n",
            "Collecting page : <WikipediaPage '대한민국의 역사 드라마 목록'> , text length 482\n",
            "Collecting page : <WikipediaPage '한국 불교의 역사'> , text length 46394\n",
            "Collecting page : <WikipediaPage '역사주의'> , text length 4674\n",
            "Collecting page : <WikipediaPage '서울역사박물관'> , text length 2769\n",
            "Collecting page : <WikipediaPage '오키나와현의 역사'> , text length 6799\n",
            "Collecting page : <WikipediaPage '역사학'> , text length 253\n",
            "Collecting page : <WikipediaPage '러시아의 역사'> , text length 10327\n",
            "Collecting page : <WikipediaPage '역사적 예수'> , text length 21307\n",
            "Collecting page : <WikipediaPage '다큐멘터리 역사를 찾아서'> , text length 264\n",
            "Collecting page : <WikipediaPage '소련의 역사'> , text length 199\n",
            "Collecting page : <WikipediaPage '조선민주주의인민공화국의 역사'> , text length 8071\n",
            "Collecting page : <WikipediaPage '조지아의 역사'> , text length 24119\n",
            "Collecting page : <WikipediaPage '영국의 역사'> , text length 6671\n",
            "Collecting page : <WikipediaPage '역사철학'> , text length 52\n",
            "Collecting page : <WikipediaPage '프랑스의 역사'> , text length 14913\n",
            "Collecting page : <WikipediaPage '역사신학'> , text length 926\n",
            "Collecting page : <WikipediaPage '애플의 역사'> , text length 6215\n",
            "Collecting page : <WikipediaPage '인터넷의 역사'> , text length 2171\n",
            "Collecting page : <WikipediaPage '유엔의 역사'> , text length 1147\n",
            "Collecting page : <WikipediaPage '역사지진'> , text length 543\n",
            "Collecting page : <WikipediaPage '역사저널 그날'> , text length 437\n",
            "Collecting page : <WikipediaPage '힌두교의 역사'> , text length 1246\n",
            "Collecting page : <WikipediaPage '역사 시대'> , text length 551\n",
            "Collecting page : <WikipediaPage '역사소설'> , text length 1721\n",
            "Collecting page : <WikipediaPage '독일의 역사'> , text length 23772\n",
            "Collecting page : <WikipediaPage '압하지야의 역사'> , text length 15841\n",
            "Collecting page : <WikipediaPage '아일랜드의 역사'> , text length 14632\n",
            "Collecting page : <WikipediaPage '구글의 역사'> , text length 2054\n",
            "Collecting page : <WikipediaPage '영화의 역사'> , text length 12999\n",
            "Collecting page : <WikipediaPage '기독교의 역사'> , text length 28905\n",
            "Collecting page : <WikipediaPage '운영 체제의 역사'> , text length 2631\n",
            "Collecting page : <WikipediaPage '몬테네그로의 역사'> , text length 560\n",
            "Collecting page : <WikipediaPage '태국의 역사'> , text length 3114\n",
            "Collecting page : <WikipediaPage '음소문자의 역사'> , text length 2966\n",
            "Collecting page : <WikipediaPage '사기 (역사서)'> , text length 4005\n",
            "Collecting page : <WikipediaPage '파키스탄의 역사'> , text length 2372\n",
            "Collecting page : <WikipediaPage '사학자'> , text length 276\n",
            "Collecting page : <WikipediaPage '그리스의 역사'> , text length 12854\n",
            "Collecting page : <WikipediaPage '베트남의 역사'> , text length 10429\n",
            "Collecting page : <WikipediaPage '기술의 역사'> , text length 186\n",
            "Collecting page : <WikipediaPage '역사스페셜'> , text length 1448\n",
            "Collecting page : <WikipediaPage '의사 역사학'> , text length 2414\n",
            "Collecting page : <WikipediaPage '안드로이드 버전 역사'> , text length 2279\n",
            "Collecting page : <WikipediaPage '서울역'> , text length 8743\n",
            "Collecting page : <WikipediaPage '미국 육군 역사관'> , text length 397\n",
            "Collecting page : <WikipediaPage '인도의 역사'> , text length 13866\n",
            "Collecting page : <WikipediaPage '역사언어학'> , text length 556\n",
            "Collecting page : <WikipediaPage '벨라루스의 역사'> , text length 1506\n",
            "Collecting page : <WikipediaPage '퀴디치의 역사'> , text length 92\n",
            "Collecting page : <WikipediaPage '사극'> , text length 539\n",
            "Collecting page : <WikipediaPage '역사적 유물론'> , text length 2296\n",
            "Collecting page : <WikipediaPage '마리아론의 역사'> , text length 487\n",
            "Collecting page : <WikipediaPage 'FC 서울의 역사'> , text length 11289\n",
            "Collecting page : <WikipediaPage '역사 (헤로도토스)'> , text length 1162\n",
            "Collecting page : <WikipediaPage '폭력의 역사'> , text length 1036\n",
            "Collecting page : <WikipediaPage '교육의 역사'> , text length 1504\n",
            "Collecting page : <WikipediaPage '중화민국의 역사'> , text length 28162\n",
            "Collecting page : <WikipediaPage '터키의 역사'> , text length 4319\n",
            "Collecting page : <WikipediaPage '몽골의 역사'> , text length 1154\n",
            "Collecting page : <WikipediaPage '근대 그리스의 역사'> , text length 15021\n",
            "Collecting page : <WikipediaPage '베트남 역사박물관'> , text length 179\n",
            "Collecting page : <WikipediaPage '스페인의 역사'> , text length 14863\n",
            "Collecting page : <WikipediaPage '우크라이나의 역사'> , text length 3333\n",
            "Collecting page : <WikipediaPage '핀란드의 역사'> , text length 6371\n",
            "Collecting page : <WikipediaPage '맨체스터 유나이티드 FC의 역사'> , text length 156\n",
            "Collecting page : <WikipediaPage '음악의 역사'> , text length 6766\n",
            "Collecting page : <WikipediaPage '이란의 역사'> , text length 17702\n",
            "Collecting page : <WikipediaPage '에스페란토의 역사'> , text length 12663\n",
            "Collecting page : <WikipediaPage '타이완의 역사'> , text length 6145\n",
            "Collecting page : <WikipediaPage '민자역사'> , text length 1029\n",
            "Collecting page : <WikipediaPage '역사 재현'> , text length 2316\n",
            "Collecting page : <WikipediaPage '크림반도의 역사'> , text length 2563\n",
            "Collecting page : <WikipediaPage '라오스의 역사'> , text length 2200\n",
            "Collecting page : <WikipediaPage '제2차 세계 대전 기간 미국의 군사 역사'> , text length 583\n",
            "Collecting page : <WikipediaPage '브리튼 제도의 역사'> , text length 1032\n",
            "Collecting page : <WikipediaPage '이집트의 역사'> , text length 1004\n",
            "Collecting page : <WikipediaPage '한국사 연표'> , text length 188\n",
            "Collecting page : <WikipediaPage '전주역사박물관'> , text length 413\n",
            "Collecting page : <WikipediaPage '강화역사박물관'> , text length 248\n",
            "Collecting page : <WikipediaPage '아르메니아의 역사'> , text length 1073\n",
            "Collecting page : <WikipediaPage '생물의 진화 역사'> , text length 895\n",
            "Collecting page : <WikipediaPage '군사사'> , text length 1239\n",
            "Collecting page : <WikipediaPage '건축의 역사'> , text length 25890\n",
            "Collecting page : <WikipediaPage '예수의 역사적 실존'> , text length 574\n",
            "Collecting page : <WikipediaPage '대한민국의 역사'> , text length 35866\n",
            "Collecting page : <WikipediaPage '홍콩의 역사'> , text length 2662\n",
            "Collecting page : <WikipediaPage '포르투갈의 역사'> , text length 1620\n",
            "전체 수집한 Page Count : 197\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tu_6jZAmeHx9",
        "outputId": "247f0c8a-d8c7-4212-9ade-78c6ee6b824f"
      },
      "source": [
        "# 간단한 전처리\n",
        "def clean_text(txt):\n",
        "    txt = txt.replace('\\n','')\n",
        "    txt = txt.replace('=','')    \n",
        "    return txt \n",
        "\n",
        "ko_grammar_set_raw = [clean_text(x) for x in ko_grammar_set_raw]\n",
        "print('Sample text : ')\n",
        "print('--------------------------------------------------------------------------------------------')\n",
        "print(ko_grammar_set_raw[120])\n",
        "print('--------------------------------------------------------------------------------------------')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample text : \n",
            "--------------------------------------------------------------------------------------------\n",
            "한반도는 제2차 세계 대전 이후 미국과 소련의 군정기를 거쳐 조선민주주의인민공화국(북한)과 대한민국(남한)으로 나뉘게 되었다. 1948년 이전의 한반도의 역사에 관해서는 한국의 역사를 참조하도록 한다. 이 문서는 조선민주주의인민공화국의 역사에 관한 내용을 다루고 있다. 형성기 1945년 8월 15일 광복 이후, 38도선 이북에서는 소련군이 1948년 9월 9일 조선민주주의인민공화국 수립 이전까지, 38선 이남에서는 미군이 1945년 9월 8일부터 1948년 8월 15일 대한민국(남한) 정부수립까지 한반도를 다스린 기간을 군정기라 부른다. 이 기간 동안 남한에서는 수많은 정치 단체들이 조직되어 이합집산하였고, 모스크바 3상회의로 신탁통치안을 놓고 찬반으로 나뉘어 좌우익이 심각하게 대립하였다. 해방 직후 38선 이북에서는 민족주의 계열의 조만식을 위원장으로 하는 평남 건국준비위원회가 결성되었으나, 소련의 김일성 지지와 지원으로 평남건준위는 유명무실화되었고, 조만식을 비롯한 민족주의계 인사들은 김일성에 의해 숙청되었다. 1946년 2월 8일 김일성을 위원장으로 하는 북조선 임시 인민 위원회가 구성되어 사실상 정부의 구실을 하였다. 그리고서는 무상분배 토지개혁 단행, 국유화 등 개혁을 했고, 이 후 공산주의 지배 체제를 확고히 한 뒤에는 수상을 김일성으로 하고 부수상을 박헌영으로하는  1948년 9월 9일 조선민주주의인민공화국이 수립되었다. 초기에 대체로 정치 구성원은 갑산파의 김일성, 남로당파 박헌영, 연안파 김두봉, 소련파 등으로 이뤄져 연립내각체제에 있었다. 곧, 부수상 겸 외무상에 박헌영, 부수상에 홍명희, 부수상 겸 산업상에 박헌영, 국가기획위원장에 정준택, 민족보위상에 최용건, 국가검열상(국방상)에 김원봉, 내상(內相)에 박일우, 농림상에 박문규, 상상(商相)에 장지우, 교통상에 주영하, 재정상에 최창익, 교육상에 백남운, 체신상에 김정규, 사법상에 리승엽, 문화선전상에 허정숙, 노동상에 허성택, 보건상에 이병남, 도시경영상에 이영, 무임소상에 이극로를 각각 임명하였다.소련군 태평양 함대에 입대하여 웅진, 나진, 청진 등 상륙작전에 활약하여 적기훈장을 수여하는 등 조선민주주의인민공화국 정권 수립 이후 문화선전성 제1부상(차관)을 지내기도 했던 조선민주주의인민공화국의 대표적 소련파 정상진은 \"북한은 해방 후 20년 역사를 위조했다\"며 \"일본군을 내몬 뒤 소련 정부에서 북한 정권 수립을 지원하라는 명령을 받았어요. 해방 후 원산항(港)에 귀국하는 김일성을 맞으러 간 것도 접니다. 소련은 고려인을 통해 북한에 영향력을 행사하려 했어요. 1945년부터 1965년까지 북한에서 발표된 모든 연설문은 다 소련에서 작성됐어요.\"라고 폭로했다. 한국 전쟁의 사전 준비 김일성은 조선로동당의 일당독재를 기반으로 반대파(민족주의자, 종교인 특히 북조선 수립 이전 융성하던 개신교, 지주, 마름, 기업가, 기술자 등을 포함한 상당수의 북조선 인민)에 대한 철저한 숙청을 통하여 북조선 내 정치적 입지를 강하게 다지고 있었다. 또한, 북조선은 최고인민회의 간부회의를 통하여 통일을 위하여 점령군의 즉각적인 철수를 요구하자, 소련은 이에 호응하여 북한에 있는 모든 군대를 그 이듬해(1949년) 1월까지 모두 철수시키겠다고 발표했다.(1948년 9월 20일)  북조선 주둔 소련군은 1948년 12월 시베리아로 철수하였다. 그후 그들은 미군도 남한으로부터 철수할 것을 요구하고 소련군이 먼저 철수한 점을 내세워 정치적인 선전공세를 펴기 시작하자 미국은 당황하기 시작했다.그러나, 한편으로 소련 정부는 모스크바에서 비밀리에 군수뇌회담을 개최하여 철군이후의 구체적인 계획을 수립하고 '특별군사사절단'을 통해서 집행하기로 하였다. 이 회의에는  김일성 등 북조선과 중화인민공화국의 고위 군부 대표도 참석하였다. 모스크바 계획이라고 불리는 전쟁준비 계획은 만주에 있던 조선인 의용군 부대를 북한으로 귀국시켜 5개 사단을 갖게 하고 이외에 8개의 전방사단과 우수한 장비를 보유한 8개의 예비사단, 그리고 500대의 탱크를 보유하는 2개의 기갑사단을 갖게 하는 것이었다. 한국전쟁(6·25전쟁) 조선민주주의인민공화국은 대한민국(남한)을 공산화하기 위한 전쟁을 준비하고, 마침내 1950년 6월 25일 새벽 4시에 38도선 전역에 걸쳐서 선전포고도 없이 남한을 공격하여 한국 전쟁(6·25전쟁)을 일으켰다.이후 3년간 계속된 전쟁은 한반도에 엄청난 피해를 안겨다 주었다. 수많은 사람들이 죽거나 다치고 전 국토가 초토화되어 대부분의 산업 시설이 파괴되었다. 이와 동시에 대한민국과 조선민주주의인민공화국 간에는 적대 감정이 팽배하게 되어 분단이 더욱 고착화되었다.그러나 전쟁의 기원,원인과 책임소재와 관련하여 여러 학설이 충돌하고 있다.전쟁 중이던 1952년 12월 '원수'의 호칭이 제정되었고, 1953년 2월 8일 인민군 창건 5주년에 김일성이 공화국 최초의 원수가 되어 '김일성 장군'에서 '김일성 원수'로 불리게 되었다. 1953년 5월부터는 《김일성선집》이 간행되기 시작하였다. 1953년 7월 28일 김일성은 공화국영웅 칭호와 국기훈장 제일급을 수여받았다.김일성은 1953년 7월 28일 라디오 방송 연설에서 전쟁이 \"조선 인민의 승리\"로 끝났다고 주장하였다. 그의 해석에 따르자면, \"미국의 제국주의자들이 이승만 괴뢰 도당을 부추겨 일으킨 전쟁에서 조선 인민과 인민군은 영용하게 싸워 승리했다\"는 것으로 해서 일방적인 승리라고 확신을 하였다. 김일성의 권력 장악 전쟁 직후 패전의 책임을 물어 1950년대 초반즈음에 박헌영을 비롯한 남로당계가 숙청당했다. 그리고 1950년 중반 무렵에 최창익을 비롯한 연안파 세력들이 김일성 체제에 비판하였는데, 이를 8월 종파 사건이라 한다. 8월 종파 사건을 계기로 소련파 및 연안파가 숙청당하고, 이때 중·소 분쟁을 계기로 주체 사상이 등장하였다. 60년대에는 경제 건설을 우선시하는 갑산파 온건세력들이 숙청 당했고, 1967년 주체 사상이 지도 이념화되었다. 1972년에는 대한민국과 7.4 남북 공동성명을 맺었다. 이어서 김일성은 사회주의 헌법을 제정하여 주체사상을 확립하였는데 이는 국가 원수가 수상제에서 주석제로 바뀌었고, 주석의 권한은 입법, 사법, 행정의 모든 권력이 장악하여 김일성 유일 체제를 확립하게 하였다. 그리고 이때 '주체 사상'을 헌법에 최초로 규범화하였다. 주체 사상의 설립과 체제 경쟁 1960년대부터 대한민국(남한)·미국·일본의 안보 체제 구축과 국제 정세의 악화로 위기에 놓인 조선민주주의인민공화국은 국방 건설을 위하여 이른바 4대 군사 노선을 채택하여 군수 공업 발전에 박차를 가하였다. 아울러 김일성과 조선로동당의 장기 독재를 강화하기 위해 이른바 주체 로선을 강조하였다. 대남 정책에서는 겉으로는 평화적인 남북 연방제 통일 방안을 제시하고, 내면으로는 남한에 통일혁명당을 조직하여 주체 사상에 입각한 내부 혁명을 부추겼으며, 무장 군인을 파견하여 무력 도발을 일으켰다. 김정일 후계 체제의 확립 1980년대에 김정일 후계 체제가 공식화되면서, 1990년대 김일성의 사후 김정일 중심 체제가 강화되어갔다. 1992년 헌법 개정을 통해서 주석의 권한을 축소하는 대신, 군사 관련 기능 및 권한을 국방위원회로 통합하여 김정일 체제가 별다른 파벌 분쟁없이, 공고해져 갔다. 그리고 1998년 헌법 개정을 한번더 하면서, 주석제가 폐지되고, 국방위원장의 권한이 강화되어 완전히 확립되었다. 6·15 남북 공동선언 이후 한국의 김대중 대통령과 북한의 김정일 국방위원장이 합의하여 발표한 6·15 남북 공동선언의 내용은 다음과 같다.남과 북은 나라의 통일문제를 그 주인인 우리 민족끼리 서로 힘을 합쳐 자주적으로 해결해 나가기로 하였다.남과 북은 나라의 통일을 위한 남측의 연합 제안과 북측의 낮은 단계의 연방제안이 서로 공통성이 있다고 인정하고 앞으로 이 방향에서 통일을 지향시켜 나가기로 하였다.남과 북은 올해 8·15에 즈음하여 흩어진 가족, 친척 방문단을 교환하며 비전향 장기수 문제를 해결하는 등 인도적 문제를 조속히 풀어 나가기로 하였다.남과 북은 경제협력을 통하여 민족경제를 균형적으로 발전시키고 사회·문화·체육·보건·환경 등 제반 분야의 협력과 교류를 활성화하여 서로의 신뢰를 다져 나가기로 하였다.남과 북은 이상과 같은 합의사항을 조속히 실천에 옮기기 위하여 빠른 시일 안에 당국 사이의 대화를 개최하기로 하였다. 경제 1946년 조선민주주의인민공화국 정부가 공식수립이전인 북조선 인민위원회가 설립했을때 무상분배,무상몰수 토지개혁 및 주요산업등 국유화를 단행했다. 1950년대에는 한국전쟁으로 인하여 피폐해진 국토와 전후 복구사업을 재건하기 위해 3개년 계획과 5개년 개발계획, 천리마운동등을 전개하여 나아갔다. 이무렵에는 모든 농지의 협동 농장화와 생산력 증대를 위한 노동 강화운동, 사상,기술,문화의 혁명등을 추진해 나아갔다.그러나 1980년대에 들어서면서 공산주의 체제의 모순과 한계가 드러남에 따라 생산력 저하와 공산주의 사회주의권 국가들의 몰락으로 인하여 교역 상대국이 상실되었고, 오늘날 에너지와 원자재,식량등 심각한 경제고를 겪고있다. 1990년대 이후 들어서는 매년 마이너스 경제 성장율을 보이고있다. 이에 조선민주주의인민공화국정부 내에서는 이러한 위기를 해결하고자 1984년에 합영법제정, 1994년에는 신합영법개정하여 1991년에는 나진,선봉 자유무역지대 설치, 2002년에는 신의주 경제 특구 설치, 개성공단 설치등 함으로써 일부지역에만 자유무역지대를 설치하고 '제한적 개방'을 표방하고 있다.하지만 겉으로는 달리, 북한의 경제는 침체되어가고 있었다.결국, 서울올림픽에 대응하여 주최한 행사로 인하여 북한의 경제는 큰 타격을 입었고, 고난의 행군 까지 가버리는 사태가 발생한다.이로써, 북한의 경제정책은 실패했다는 것이 증명되었다.2000년대 들어서, 북한은 외자 유치를 위해 필사의 노력을 다하고 있다.나진, 선봉지역에, 신의주 공업 자유지대, 개성공단등, 중국과 대한민국의 자본 유치를 위해 노력하고 있다.하지만, 불확실성 때문에 투자를 꺼려하는 사람들이 많다. 문화  연혁 1945년 8월 8일: 소비에트 연방(소련)이 일본 (日本)에 선전 포고. 일제 강점기에 놓여 있었던 한반도 북부를 군사 점거하였다.1945년 8월 15일 이후: 포츠담 선언에 따라, 북위 38선이북의 5도에 소련군이 진주. 진주 지역에서 각 급 인민위원회를 설치되었다.1945년 10월 10일: 조선공산당 발족. (조선노동당 창건일)1945년 11월 23일: 신의주 반공학생사건 발생1946년 2월 8일: 소련 군정의 지시에 따라, 사회주의 국가의 건국 준비를 위한 북조선림시인민위원회가 건설되었다.1946년 2월 8일 소비에트 연방(소련) 점령군 사령부는 북조선임시인민위원회를 발족시켜 북조선에서 사실상의 단독 정부로 가능케 했다. 위원장에는 김일성, 부위원장에는 김두봉, 서기장에는 강양욱이 선출되었다.1946년 11월 3일 총선거를 실시하여, 453명의 여성을 포함한 3,458명의 도·시·군 인민위원회의 대의원을 선출하였다.1948년 2월 6일 북조선인민회의 제4차 회의에서 헌법제정위원회가 작성한 헌법 초안을 검토해 조선인민군의 창설을 결정하였다.1948년 2월 8일 조선인민군의 창설을 발표하였다.1948년 4월 27일 북조선인민회의 특별회의에서 조선민주주의인민공화국 헌법 초안이 승인되었다. 소비에트 연방의 군정치 이후의 조선민주주의인민공화국 1948년 2월: 조선인민군이 창설되다.1948년 8월: 최고인민회의의 대의원 선거가 실시되었다.1948년 9월 9일: 최고인민회의가 헌법을 채택, 북위 38선 이북에 마르크스-레닌주의를 이념으로 한 '조선민주주의인민공화국'  수립. 김일성이 수상에 취임.1949년: 주요 정당 단체를 결집한 조국통일민주주의전선 결성.1949년 6월: 조선노동당 성립, 김일성이 중앙위원장에 취임.1950년 6월 25일: 조선인민군이 한반도의 적화 통일을 목표로 대한민국에 침공, 대한민국 국군과 교전 상태가 되어 한국 전쟁이 시작되다.전쟁 중 오기섭, 박헌영, 김두봉, 허가이 등이 김일성의 독재를 위해 숙청되다.1956년 8월 29일: 소련파의 박창옥, 연안파의 최장익 등이 김일성에게 도전하지만 숙청되다.1968년 1월 23일: 푸에블로 호 납치사건 발생. 미해군 정보수집함 푸에블로 호를 해상에서 나포한 사건1972년: 김일성의 정적 숙청이 완료되고 조선민주주의인민공화국 사회주의헌법이 제정되다.1973년: 후계자론이 최초로 거론되고 김정일이 전면에 나서기 시작하다.1977년: 조선민주주의인민공화국 사회주의헌법에서 국가의 공식 이념이 마르크스-레닌주의에서 주체사상으로 바뀌다.1988년 11월 7일: 조선민주주의인민공화국에서 제13차 세계청년학생축전이 평양에서 개최, 평양에서 가장 큰 건물인 유경호텔이 건설되기 시작됨(1995년에 건설이 중단됨).1991년 9월 17일: 조선민주주의인민공화국과 대한민국이 UN 동시 가입.1994년 7월 8일: 조선민주주의인민공화국의 초대 주석 김일성이 사망하다. 김일성 사망 이후의 조선민주주의인민공화국 2000년 6월 13일: 6·15 남북 공동선언2002년 4월 1일: 매스게임과 집단공연인 아리랑이 메이데이스타디움(5月 1日 競技場)에서 5개월간 공연되다.2002년 12월 1일: 조선노동당기관지 로동신문이 편집체제를 변경하다.2003년 10월 1일: 평양직할시 보통강구역에 대한민국과 조선민주주의인민공화국이 합작하여 만든 체육관이 건설되다.2004년 7월 19일: 제7차 남북군사회담을 통해 조선민주주의인민공화국 정부가 남한 주민에게 방송하는 구국의 소리 방송을 운영을 중단할 것을 합의했다.2004년 4월 15일: 평양볼링장에서 \"제8차 볼링선수권대회\"를 개최하다. (김일성 국가 주석이 평양볼링장을 방문한 8돌을 맞이하여)2005년 8월 15일: 경기도 고양시에서 8.15 광복 60돌을 맞이하여 열린 남북축구대회가 벌어지다.2005년 9월 15일: 평양직할시의 중구역의 아파트들이 너무 낡고, 붕괴될 가능성이 높아 재개발을 하기 시작함. 이와 같은 날에 보통강구역의 천리마거리, 중구역의 해방산거리를 재개발하기 시작함.2005년 10월 10일: 중화인민공화국(중국)과 조선민주주의인민공화국이 합작하여 대안친선유리공장이 만들어지다.2005년 10월 29일: 중화인민공화국(중국)의 주석이며, 중국공산당 총서기인 후진타오주석이 평안북도 대안군을 방문하다.2005년 11월 1일: 조선민주주의인민공화국의 금융기관인 \"동북아시아은행\"에서 신용카드를 출시하다.2005년 11월 29일: 조선민주주의인민공화국의 조선중앙방송에서 인민생활공채(복권)에서 당첨된 번호들을 발표하다.2006년 1월 3일 조선민주여성동맹의 김정숙 위원장에게 이집트 문화 및 국제문명대화를 위한 전문동맹이 상과 업적칭송증서를 김정일 조선노동당 총비서에게 이집트 문화 및 국제문명대화를 위한 전문동맹이 상과 업적칭송증서를 전달하다. (1월 3일 조선중앙통신)2006년 1월 4일 조선로동당 기관지 \"로동신문\"이 논설을 통하여 \"테러와의 전쟁\"을 벌이고 있는 미국의 실태에 대하여 강력하게 비판했다.2006년 1월 6일 조선컴퓨터센터에서 운영하고 있는 인터넷 웹사이트 \"내나라\"가 온라인 쇼핑코너를 마련하다.2006년 1월 13일 김정일 조선노동당 총비서는 중화인민공화국(중국)을 비공식 방문하여 중화인민공화국의 국가주석인 후진타오를 만나 정상회담을 벌였다.2012년 4월 15일 김정은이 \"김일성 탄생 100돐\"을 맞으며 실시한 열병식에서 기념육성연설을 하다.2012년 10월 12일 \"모란봉악단\"을 창설하여 시험공연을 실시. 외국곡을 전자음악으로 연주하고 이색적인 공연을 펼치다.2012년 12월 17일 김정은 제1위원장의 지시에 따라 인공로켓 \"은하 3호\"를 평안북도 철산군 동창리 로켓기지에서 발사하다.2013년 1월 9일 김정은 제1위원장이 \"신년사\"를 집필, 발표하여 경제발전에 대한 여러가지 공약들을 제시하다. 참고자료 강준만, 《한국현대사산책》〈1940년대편 2권〉(인물과사상사, 2004) 149쪽.2000년 6월 15일 이후의 자료들은 조선민주주의인민공화국의 조선중앙통신과 대한민국의 연합뉴스의 보도를 분석하여 작성한 것임. 각주 \n",
            "--------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqJHLPaReHx-"
      },
      "source": [
        "문장으로 잘라 낸다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_b8WKqYXeHx_",
        "outputId": "2a0e5263-0b4a-443a-c713-d086b4db3c80"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erJc7g-VeHyA",
        "outputId": "ef990f40-1089-4ae5-9741-aabdb813ffec"
      },
      "source": [
        "#Split the document into sentences\n",
        "ko_grammar_sentences = []\n",
        "for document in ko_grammar_set_raw:\n",
        "    ko_grammar_sentences += nltk.sent_tokenize(document)\n",
        "\n",
        "print(\"Num sentences:\", len(ko_grammar_sentences))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num sentences: 13542\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "7M2yM5kFeHyC",
        "outputId": "68b89675-ee56-4a6f-aef4-e05afacbce67"
      },
      "source": [
        "ko_grammar_sentences[11000]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1970년대 무지부르는 파키스탄 정부에 의해 체포되고 AL당은 활동이 금지되었다.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdj8wrm8eHyC"
      },
      "source": [
        "형태소 분리하여 모든 문장을 형태소 Code로 변환 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uETdqraheHyE",
        "outputId": "bc381963-1f55-4ce1-ca8d-3d47e330b4a2"
      },
      "source": [
        "from konlpy.tag import Twitter\n",
        "twitter = Twitter()\n",
        "\n",
        "print(twitter.pos(ko_grammar_sentences[305]))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('백운호수', 'Noun'), ('에서', 'Josa'), ('의왕시', 'Noun'), ('와', 'Josa'), ('의왕시', 'Noun'), ('축제', 'Noun'), ('추진', 'Noun'), ('위원회', 'Noun'), ('가', 'Josa'), ('공동', 'Noun'), ('으로', 'Josa'), ('주최', 'Noun'), ('하여', 'Verb'), ('매년', 'Noun'), ('9월', 'Number'), ('에', 'Foreign'), ('이틀', 'Noun'), ('동안', 'Noun'), ('열리는', 'Verb'), ('축제', 'Noun'), ('이다', 'Josa'), ('.', 'Punctuation')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "OY8VTXraeHyF"
      },
      "source": [
        "# 형태소 Code table의 구성\n",
        "\n",
        "_MAX_MORP_LENGTH = 128\n",
        "_PADDING_CODE = 0  # padding code\n",
        "_MISMATCH_CODE = 1 # mismatch word code               ex) @@@\n",
        "_MISMATCH_WORD = '@@@' # 이거 아래에서 쓴다.\n",
        "\n",
        "morpheme_table = {}\n",
        "morp_code = _MISMATCH_CODE+1\n",
        "morpheme_table['Pad'] = _PADDING_CODE \n",
        "morpheme_table['Mst'] = _MISMATCH_CODE \n",
        "for sentence in ko_grammar_sentences[:1000]:\n",
        "    morphemes = twitter.pos(sentence)\n",
        "    for (word,morp) in morphemes:\n",
        "        if morp in morpheme_table:\n",
        "            pass\n",
        "        else:\n",
        "            morpheme_table[morp] = morp_code\n",
        "            morp_code += 1"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asiTeu8SeHyG",
        "outputId": "165d2475-7d35-4785-f9db-61baea8815b7"
      },
      "source": [
        "print('Korean morpheme code table')\n",
        "print('----------------------------------------------------------')\n",
        "print('  Morpheme        Code')\n",
        "print('')\n",
        "for morp in morpheme_table.keys():\n",
        "    print(f' {morp.ljust(15)}   {morpheme_table[morp]}')\n",
        "print('----------------------------------------------------------')\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Korean morpheme code table\n",
            "----------------------------------------------------------\n",
            "  Morpheme        Code\n",
            "\n",
            " Pad               0\n",
            " Mst               1\n",
            " Noun              2\n",
            " Punctuation       3\n",
            " Foreign           4\n",
            " Josa              5\n",
            " Verb              6\n",
            " Modifier          7\n",
            " Adjective         8\n",
            " Suffix            9\n",
            " Adverb            10\n",
            " Number            11\n",
            " Alpha             12\n",
            " Conjunction       13\n",
            " Determiner        14\n",
            " VerbPrefix        15\n",
            " Exclamation       16\n",
            " KoreanParticle    17\n",
            " Eomi              18\n",
            " ScreenName        19\n",
            " URL               20\n",
            "----------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQfjoYbceHyH",
        "outputId": "15a51ab3-0576-4af3-8151-b78218213f24"
      },
      "source": [
        "# morpheme 코드 변환기\n",
        "def morpheme_encode(sentence):\n",
        "    encode=[]\n",
        "    morphemes = twitter.pos(sentence)\n",
        "    for (word,morp) in morphemes:\n",
        "        encode.append(_MISMATCH_CODE if word==_MISMATCH_WORD else morpheme_table[morp])\n",
        "    return encode\n",
        "\n",
        "code = morpheme_encode('야당이 수단으로 야당에게는 이라며 윤석열 허물어뜨렸고 수사를 군사작전을 하는 시비를 민주당 의원이 국민적 세워 화신이 나라 법위에 우려했다 될 이라며 운영할지 바람 짓밟지만 양자역학 양자역학 걱정하느냐고 누가 청와대 한다 엄포를 양자역학 양자역학 양자역학 편을 파렴치와 난관에 풀이다 대통령이 양자역학 저지하려다가')\n",
        "print(f'Code length : {len(code)}')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Code length : 67\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BS_SnM04eHyH",
        "outputId": "1cf44ca4-92c4-41e8-be11-a8c79511e485"
      },
      "source": [
        "# 전체 형태소 코드로 변환\n",
        "ko_grammar_set = []\n",
        "for sentence in ko_grammar_sentences:\n",
        "    code = morpheme_encode(sentence)\n",
        "    if len(code) <= _MAX_MORP_LENGTH:\n",
        "        ko_grammar_set.append(code + [_PADDING_CODE for i in range(_MAX_MORP_LENGTH-len(code))])\n",
        "\n",
        "ko_grammar_set = np.asarray(ko_grammar_set)\n",
        "ko_grammar_set.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13343, 128)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNq3STdueHyI"
      },
      "source": [
        "# Dataset 전체 구성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiSdmi1beHyI",
        "outputId": "63f20a16-98fe-4a92-8048-7cdf313b86f1"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import BertTokenizer\n",
        "# embedder download...\n",
        "embedder = SentenceTransformer('xlm-r-large-en-ko-nli-ststb')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1.80G/1.80G [00:56<00:00, 31.9MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "e1FGpEuJeHyJ"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BATCH_COUNT = 100"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhOs5GKNeHyK",
        "outputId": "ea810fa3-b517-48bb-c595-46c88070a4b7"
      },
      "source": [
        "#dataset 다시 만듦\n",
        "\n",
        "org_text_emb = embedder.encode([org_text])[0]\n",
        "print(f'Text embedding shape : {org_text_emb.shape}')\n",
        "dataset = []\n",
        "for i in range(BATCH_COUNT):\n",
        "    emb_batch_set = []\n",
        "    cod_batch_set = []\n",
        "    for j in range(BATCH_SIZE):\n",
        "        emb_batch_set.append(org_text_emb)\n",
        "        cod_batch_set.append(ko_grammar_set[BATCH_SIZE*i+j])\n",
        "\n",
        "    emb_batch_set = np.asarray(emb_batch_set)\n",
        "    cod_batch_set = np.asarray(cod_batch_set)\n",
        "    dataset.append((emb_batch_set,cod_batch_set))\n",
        "\n",
        "print(f'Total dataset count :{BATCH_COUNT*BATCH_SIZE}')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text embedding shape : (1024,)\n",
            "Total dataset count :6400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxfQxFQKeHyL",
        "outputId": "d8267677-3e20-4538-c1b2-8f9af4c7af24"
      },
      "source": [
        "# 30번째 배치의 형태소코드셋 중 20번째꺼 확인\n",
        "print(dataset[30][1][20])\n",
        "print(dataset[31][1][20])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2 9 2 5 2 2 5 6 2 2 5 6 2 2 5 2 5 6 3 2 5 2 5 2 6 2 5 6 2 5 6 6 2 8 3 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[ 2  2  5  2  5  2  5  2  2  2  5  2  8  3  2  5  2  6  6  2  5  2  5  3\n",
            " 11  3 11  3 11  3 11  3 11  3 11  3 11  3  2  2  5  2  5  6  2  6  2  5\n",
            "  6  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlM5tdo1eHyL"
      },
      "source": [
        "# Generator 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jopXmV8DeHyM",
        "outputId": "f8c64613-6d38-488b-f2f1-7b3c67b548a5"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([org_text])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(f'Total token count of origin text : {total_words}')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total token count of origin text : 395\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DT4gKRYeHyN"
      },
      "source": [
        "일단 전체 크기를 한정한 generator를 구성한다.<br>\n",
        "향후, 이것은 LSTM을 이용해서 길게 만들 수 있을 것.\n",
        "\n",
        "_MAX_TOKEN = 512   ' Origin text의 전체 token 개수를 최대 512로 한정 <br>\n",
        "_MAX_LENGTH = 40   ' generator에 의해 생성하는 문자의 전체 token 개수를 40한정 <br>\n",
        "\n",
        "이는 512개의 token 원문을 40개의 token 으로 구성된 문장으로 요약 하는것...."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "P_cpFugJeHyN"
      },
      "source": [
        "# max 512 token으로 만든다. 남는건 padding\n",
        "\n",
        "_MAX_TOKEN = 512\n",
        "_MAX_LENGTH = 40\n",
        "_NOISE_DIM = 100\n",
        "\n",
        "word_keys = []\n",
        "word_values = []\n",
        "\n",
        "for word,index in tokenizer.word_index.items():\n",
        "    word_keys.append(index)\n",
        "    word_values.append(word)\n",
        "\n",
        "current_token_len = len(word_keys)\n",
        "\n",
        "if current_token_len > _MAX_TOKEN:\n",
        "    word_keys = word_keys[:_MAX_TOKEN]\n",
        "    word_values = word_values[:_MAX_TOKEN]\n",
        "else:\n",
        "    for i in range(current_token_len+1,_MAX_TOKEN+1):\n",
        "        word_keys.append(i)\n",
        "        word_values.append(_MISMATCH_WORD)\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jg3YCjGJeHyO",
        "outputId": "dec940b9-bbc6-4918-9345-a703090c0700"
      },
      "source": [
        "print('Token table of origin text')\n",
        "print('---------------------------------------------')\n",
        "print(' Code         Token')\n",
        "print('')\n",
        "for k in word_keys:\n",
        "  print( f'  {str(k).ljust(8)}    {word_values[k-1]}')\n",
        "print('---------------------------------------------')\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token table of origin text\n",
            "---------------------------------------------\n",
            " Code         Token\n",
            "\n",
            "  1           원내대표는\n",
            "  2           주\n",
            "  3           것\n",
            "  4           이라고\n",
            "  5           며\n",
            "  6           문재인\n",
            "  7           않는\n",
            "  8           문\n",
            "  9           이라며\n",
            "  10          이제\n",
            "  11          풀들이\n",
            "  12          수\n",
            "  13          대통령과\n",
            "  14          공수처는\n",
            "  15          될\n",
            "  16          있는\n",
            "  17          향해\n",
            "  18          보인다\n",
            "  19          짓밟힌\n",
            "  20          국민적\n",
            "  21          저항에\n",
            "  22          직면할\n",
            "  23          정권이\n",
            "  24          위해\n",
            "  25          공수처법을\n",
            "  26          제게\n",
            "  27          대통령은\n",
            "  28          '공수처는\n",
            "  29          없는\n",
            "  30          야당이\n",
            "  31          공수처장\n",
            "  32          사람들이\n",
            "  33          민주당\n",
            "  34          그러면서\n",
            "  35          공수처가\n",
            "  36          청와대와\n",
            "  37          왜\n",
            "  38          그\n",
            "  39          인정하지\n",
            "  40          이\n",
            "  41          공수처를\n",
            "  42          권력\n",
            "  43          게\n",
            "  44          정권의\n",
            "  45          민주당이\n",
            "  46          국회에서\n",
            "  47          그걸\n",
            "  48          권력은\n",
            "  49          먼저\n",
            "  50          주호영\n",
            "  51          국민의힘\n",
            "  52          22일\n",
            "  53          고위공직자범죄수사처\n",
            "  54          공수처\n",
            "  55          법\n",
            "  56          개정과\n",
            "  57          가덕도\n",
            "  58          신공항\n",
            "  59          건설\n",
            "  60          등을\n",
            "  61          밀어붙이고\n",
            "  62          정권과\n",
            "  63          더불어민주당을\n",
            "  64          끝이\n",
            "  65          아우성\n",
            "  66          치는\n",
            "  67          경고했다\n",
            "  68          이날\n",
            "  69          자신의\n",
            "  70          페이스북에\n",
            "  71          공수처법\n",
            "  72          개정을\n",
            "  73          위한\n",
            "  74          '군사작전'에\n",
            "  75          돌입하겠다고\n",
            "  76          엄포를\n",
            "  77          놓고\n",
            "  78          있다\n",
            "  79          정의당을\n",
            "  80          끌어들이기\n",
            "  81          꼼수\n",
            "  82          선거법에\n",
            "  83          묶어\n",
            "  84          '패스트트랙'이라는\n",
            "  85          불법·탈법으로\n",
            "  86          만들어낸\n",
            "  87          시행도\n",
            "  88          해보지\n",
            "  89          않고\n",
            "  90          고치려\n",
            "  91          하는\n",
            "  92          지적했다\n",
            "  93          이어\n",
            "  94          야당\n",
            "  95          원내대표인\n",
            "  96          사람\n",
            "  97          좋아보이는\n",
            "  98          표정으로\n",
            "  99          야당의\n",
            "  100         동의\n",
            "  101         없이는\n",
            "  102         절대\n",
            "  103         출범할\n",
            "  104         것'이라고\n",
            "  105         얘기했고\n",
            "  106         유엔\n",
            "  107         안보리\n",
            "  108         상임이사국처럼\n",
            "  109         임명에\n",
            "  110         '비토권'을\n",
            "  111         행사할\n",
            "  112         있는데\n",
            "  113         무얼\n",
            "  114         걱정하느냐고\n",
            "  115         여당\n",
            "  116         우리를\n",
            "  117         속였다\n",
            "  118         거짓말이라는\n",
            "  119         비난을\n",
            "  120         개의치\n",
            "  121         사람들\n",
            "  122         꼬집었다\n",
            "  123         이해찬\n",
            "  124         전\n",
            "  125         대표가\n",
            "  126         얘기한\n",
            "  127         '민주당\n",
            "  128         20년\n",
            "  129         집권'의\n",
            "  130         토대가\n",
            "  131         올해\n",
            "  132         안에\n",
            "  133         완성된다\n",
            "  134         탈원전과\n",
            "  135         동남권\n",
            "  136         신공항은\n",
            "  137         대통령이\n",
            "  138         대선\n",
            "  139         공약으로\n",
            "  140         내건\n",
            "  141         사업이니\n",
            "  142         여기에\n",
            "  143         불법이\n",
            "  144         있었다고\n",
            "  145         시비를\n",
            "  146         거는\n",
            "  147         것은\n",
            "  148         민주주의를\n",
            "  149         부정하는\n",
            "  150         것이라고\n",
            "  151         청와대\n",
            "  152         출신\n",
            "  153         윤건영\n",
            "  154         의원이\n",
            "  155         윽박지른다\n",
            "  156         '민주주의\n",
            "  157         민주당'이\n",
            "  158         법위에\n",
            "  159         군림하는\n",
            "  160         '반민주'를\n",
            "  161         거리낌없이\n",
            "  162         획책하는\n",
            "  163         언급했다\n",
            "  164         표를\n",
            "  165         얻기\n",
            "  166         나라\n",
            "  167         곳간을\n",
            "  168         다\n",
            "  169         허물어뜨렸고\n",
            "  170         재정\n",
            "  171         운용에서\n",
            "  172         신중함은\n",
            "  173         사라졌다\n",
            "  174         괴물\n",
            "  175         출범하면\n",
            "  176         공무원\n",
            "  177         누구나\n",
            "  178         권력이\n",
            "  179         지시하는\n",
            "  180         범죄행위에\n",
            "  181         거리낌\n",
            "  182         없이\n",
            "  183         가담할\n",
            "  184         것이다\n",
            "  185         권부\n",
            "  186         요직에\n",
            "  187         앉아\n",
            "  188         불법으로\n",
            "  189         각종\n",
            "  190         이권을\n",
            "  191         챙기는\n",
            "  192         권력자들에\n",
            "  193         대한\n",
            "  194         사건이\n",
            "  195         불거져도\n",
            "  196         사건을\n",
            "  197         가져가\n",
            "  198         버리면\n",
            "  199         그만\n",
            "  200         우려했다\n",
            "  201         고위\n",
            "  202         공직자들을\n",
            "  203         처벌하는\n",
            "  204         것인데\n",
            "  205         반대하는지\n",
            "  206         이해할\n",
            "  207         없다'고\n",
            "  208         했는데\n",
            "  209         그런\n",
            "  210         분이\n",
            "  211         대통령\n",
            "  212         주변을\n",
            "  213         감시하는\n",
            "  214         특별감찰관은\n",
            "  215         취임\n",
            "  216         이후\n",
            "  217         지금까지\n",
            "  218         임명하지\n",
            "  219         않았는가\n",
            "  220         라며\n",
            "  221         권력형\n",
            "  222         비리의\n",
            "  223         쓰레기\n",
            "  224         하치장\n",
            "  225         종말\n",
            "  226         처리장이\n",
            "  227         비판했다\n",
            "  228         정부를\n",
            "  229         사도들은\n",
            "  230         법치가\n",
            "  231         미치지\n",
            "  232         무오류의\n",
            "  233         화신이\n",
            "  234         오류를\n",
            "  235         존재가\n",
            "  236         바로\n",
            "  237         신이며\n",
            "  238         아래에는\n",
            "  239         자신들의\n",
            "  240         지도자를\n",
            "  241         목숨바쳐\n",
            "  242         지킴으로서\n",
            "  243         정의를\n",
            "  244         실현하겠다는\n",
            "  245         추종자들로\n",
            "  246         넘쳐\n",
            "  247         난다\n",
            "  248         지도자의\n",
            "  249         신성을\n",
            "  250         세력을\n",
            "  251         정죄하는\n",
            "  252         수단으로\n",
            "  253         전락할\n",
            "  254         질타했다\n",
            "  255         저도\n",
            "  256         법조인이지만\n",
            "  257         공수처장이\n",
            "  258         마음대로\n",
            "  259         검사들과\n",
            "  260         수사관들을\n",
            "  261         임명하는\n",
            "  262         끔찍한\n",
            "  263         사법기구가\n",
            "  264         어떤\n",
            "  265         일을\n",
            "  266         할지\n",
            "  267         두렵기만\n",
            "  268         하다\n",
            "  269         검찰과\n",
            "  270         경찰\n",
            "  271         위에\n",
            "  272         사법기구로\n",
            "  273         헌법과\n",
            "  274         법으로\n",
            "  275         독립성을\n",
            "  276         보장하는\n",
            "  277         검찰총장을\n",
            "  278         이렇게\n",
            "  279         핍박하는\n",
            "  280         어떻게\n",
            "  281         운영할지\n",
            "  282         불을\n",
            "  283         보듯\n",
            "  284         뻔한\n",
            "  285         일\n",
            "  286         예측했다\n",
            "  287         추미애\n",
            "  288         법무장관을\n",
            "  289         앞장\n",
            "  290         세워\n",
            "  291         윤석열\n",
            "  292         검찰의\n",
            "  293         비리\n",
            "  294         수사를\n",
            "  295         저지하려다가\n",
            "  296         난관에\n",
            "  297         봉착하자\n",
            "  298         무슨\n",
            "  299         수를\n",
            "  300         써서라도\n",
            "  301         출범시키려\n",
            "  302         한다\n",
            "  303         자리에는\n",
            "  304         추미애보다\n",
            "  305         더\n",
            "  306         한\n",
            "  307         막무가내\n",
            "  308         내\n",
            "  309         편을\n",
            "  310         앉힐\n",
            "  311         분명한\n",
            "  312         파렴치와\n",
            "  313         오만함을\n",
            "  314         최전선에서\n",
            "  315         온\n",
            "  316         몸으로\n",
            "  317         겪어온\n",
            "  318         저로서는\n",
            "  319         내일부터\n",
            "  320         보일\n",
            "  321         행태가\n",
            "  322         환히\n",
            "  323         180석의\n",
            "  324         또\n",
            "  325         군사작전을\n",
            "  326         개시하면\n",
            "  327         누가\n",
            "  328         막겠는가\n",
            "  329         라고\n",
            "  330         성토했다\n",
            "  331         막을\n",
            "  332         힘이\n",
            "  333         우리\n",
            "  334         야당에게는\n",
            "  335         없다\n",
            "  336         삭발하고\n",
            "  337         장외투쟁해\n",
            "  338         봐야\n",
            "  339         눈\n",
            "  340         하나\n",
            "  341         깜짝할\n",
            "  342         아닌\n",
            "  343         대란대치\n",
            "  344         大亂大治\n",
            "  345         세상을\n",
            "  346         온통\n",
            "  347         혼돈\n",
            "  348         속으로\n",
            "  349         밀어넣고\n",
            "  350         유지에\n",
            "  351         이용한다는\n",
            "  352         통치기술\n",
            "  353         규탄했다\n",
            "  354         아울러\n",
            "  355         바람\n",
            "  356         국민은\n",
            "  357         풀이다\n",
            "  358         바람이\n",
            "  359         불면\n",
            "  360         청보리\n",
            "  361         밭의\n",
            "  362         보리가\n",
            "  363         눕는다\n",
            "  364         다시는\n",
            "  365         일어서지\n",
            "  366         못하도록\n",
            "  367         풀을\n",
            "  368         짓밟지만\n",
            "  369         풀들은\n",
            "  370         다시\n",
            "  371         일어난다\n",
            "  372         시인\n",
            "  373         김수영은\n",
            "  374         '바람보다\n",
            "  375         눕지만\n",
            "  376         바람보다\n",
            "  377         일어나는'\n",
            "  378         민초의\n",
            "  379         힘을\n",
            "  380         노래했다\n",
            "  381         고\n",
            "  382         말했다\n",
            "  383         마지막으로\n",
            "  384         정권은\n",
            "  385         곧\n",
            "  386         광장에서\n",
            "  387         일어서서\n",
            "  388         아우성치는\n",
            "  389         모습을\n",
            "  390         지켜보게\n",
            "  391         대란대치를\n",
            "  392         끝장내려는\n",
            "  393         거듭\n",
            "  394         강조했다\n",
            "  395         @@@\n",
            "  396         @@@\n",
            "  397         @@@\n",
            "  398         @@@\n",
            "  399         @@@\n",
            "  400         @@@\n",
            "  401         @@@\n",
            "  402         @@@\n",
            "  403         @@@\n",
            "  404         @@@\n",
            "  405         @@@\n",
            "  406         @@@\n",
            "  407         @@@\n",
            "  408         @@@\n",
            "  409         @@@\n",
            "  410         @@@\n",
            "  411         @@@\n",
            "  412         @@@\n",
            "  413         @@@\n",
            "  414         @@@\n",
            "  415         @@@\n",
            "  416         @@@\n",
            "  417         @@@\n",
            "  418         @@@\n",
            "  419         @@@\n",
            "  420         @@@\n",
            "  421         @@@\n",
            "  422         @@@\n",
            "  423         @@@\n",
            "  424         @@@\n",
            "  425         @@@\n",
            "  426         @@@\n",
            "  427         @@@\n",
            "  428         @@@\n",
            "  429         @@@\n",
            "  430         @@@\n",
            "  431         @@@\n",
            "  432         @@@\n",
            "  433         @@@\n",
            "  434         @@@\n",
            "  435         @@@\n",
            "  436         @@@\n",
            "  437         @@@\n",
            "  438         @@@\n",
            "  439         @@@\n",
            "  440         @@@\n",
            "  441         @@@\n",
            "  442         @@@\n",
            "  443         @@@\n",
            "  444         @@@\n",
            "  445         @@@\n",
            "  446         @@@\n",
            "  447         @@@\n",
            "  448         @@@\n",
            "  449         @@@\n",
            "  450         @@@\n",
            "  451         @@@\n",
            "  452         @@@\n",
            "  453         @@@\n",
            "  454         @@@\n",
            "  455         @@@\n",
            "  456         @@@\n",
            "  457         @@@\n",
            "  458         @@@\n",
            "  459         @@@\n",
            "  460         @@@\n",
            "  461         @@@\n",
            "  462         @@@\n",
            "  463         @@@\n",
            "  464         @@@\n",
            "  465         @@@\n",
            "  466         @@@\n",
            "  467         @@@\n",
            "  468         @@@\n",
            "  469         @@@\n",
            "  470         @@@\n",
            "  471         @@@\n",
            "  472         @@@\n",
            "  473         @@@\n",
            "  474         @@@\n",
            "  475         @@@\n",
            "  476         @@@\n",
            "  477         @@@\n",
            "  478         @@@\n",
            "  479         @@@\n",
            "  480         @@@\n",
            "  481         @@@\n",
            "  482         @@@\n",
            "  483         @@@\n",
            "  484         @@@\n",
            "  485         @@@\n",
            "  486         @@@\n",
            "  487         @@@\n",
            "  488         @@@\n",
            "  489         @@@\n",
            "  490         @@@\n",
            "  491         @@@\n",
            "  492         @@@\n",
            "  493         @@@\n",
            "  494         @@@\n",
            "  495         @@@\n",
            "  496         @@@\n",
            "  497         @@@\n",
            "  498         @@@\n",
            "  499         @@@\n",
            "  500         @@@\n",
            "  501         @@@\n",
            "  502         @@@\n",
            "  503         @@@\n",
            "  504         @@@\n",
            "  505         @@@\n",
            "  506         @@@\n",
            "  507         @@@\n",
            "  508         @@@\n",
            "  509         @@@\n",
            "  510         @@@\n",
            "  511         @@@\n",
            "  512         @@@\n",
            "---------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "bZgNu7SYeHyP"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (Input,\n",
        "                                     Dense, \n",
        "                                     BatchNormalization, \n",
        "                                     LeakyReLU,\n",
        "                                     Softmax,\n",
        "                                     Reshape, \n",
        "                                     Conv2DTranspose,\n",
        "                                     Conv2D,\n",
        "                                     Dropout,\n",
        "                                     Flatten,\n",
        "                                     Concatenate,\n",
        "                                     Lambda)\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "6yLFO_w3eHyP"
      },
      "source": [
        "# tensor용 Hash table 구성 \n",
        "keys=tf.constant(word_keys,tf.int32)\n",
        "values=tf.constant(word_values, tf.string)\n",
        "# build a lookup table\n",
        "word_table = tf.lookup.StaticHashTable(\n",
        "    initializer=tf.lookup.KeyValueTensorInitializer(keys,values),\n",
        "    default_value=tf.constant(' '),\n",
        "    name=\"token_table\"\n",
        ")"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6Vj9qaaeHyQ"
      },
      "source": [
        "noise를 token_table을 통해 text로 변환하는 변환기 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "SI2OgqhqeHyR"
      },
      "source": [
        "# noise 생성\n",
        "w = tf.random.normal([2,_MAX_LENGTH,_MAX_TOKEN])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "72FRz_bDeHyR"
      },
      "source": [
        "import sys\n",
        "\n",
        "def to_text(w):\n",
        "\n",
        "    #tf.print(w)\n",
        "    texts = []\n",
        "    try:\n",
        "        for z in w:\n",
        "            text = \"\"\n",
        "            #code = []\n",
        "            for v in z:\n",
        "                try:\n",
        "                    #print(v)\n",
        "                    key = tf.argmax(v,output_type=tf.int32) #tf.constant(,dtype=tf.int32)\n",
        "                    text += word_table.lookup(key).numpy().decode('utf-8') + ' '\n",
        "                except Exception as ex:\n",
        "                    tf.print(ex,sys.exc_info())\n",
        "                    text += '[   ] '                  \n",
        "            texts.append(text)\n",
        "\n",
        "    except Exception as ex:\n",
        "        tf.print(ex,sys.exc_info())\n",
        "\n",
        "    return tf.constant(texts,dtype=tf.string)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywMSg5noeHyS",
        "outputId": "d206cea1-f662-45e3-c798-9df6fca4858c"
      },
      "source": [
        "# to_text 함수의 test\n",
        "e = to_text(w)\n",
        "for t in e:\n",
        "  print(t.numpy().decode('utf-8'))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "검찰과 @@@ 앞장 그만 @@@ @@@ 사도들은 22일 보인다 @@@ 고 @@@ 주변을 독립성을 @@@ @@@ 야당의 재정 깜짝할 출범하면 @@@ @@@ 청와대와 @@@ @@@ 수단으로 불법이 '민주당 추미애보다 장외투쟁해 @@@ 여당 개시하면 @@@ 대선 얘기한 상임이사국처럼 밀어넣고 거는 감시하는 \n",
            "@@@ 자신의 표정으로 @@@ 오만함을 그러면서 '공수처는 국민적 @@@ @@@ 것인데 @@@   것'이라고 막무가내 거는 지도자를 @@@ 한다 추미애보다 @@@ @@@ @@@ @@@ 행사할 될 @@@ 불법·탈법으로 사람들이 마지막으로 개정과 정부를 @@@ 정부를 @@@ 없다'고 무슨 향해 사람들 성토했다 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGwa9iaPeHyT"
      },
      "source": [
        "생성된 text의 embedding 변환기 구현<br>\n",
        "embedding은 org_text의 embedding과 비교하여 원문과 유사하게 민들기 위한 목적"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "_D7qcRmQeHyX"
      },
      "source": [
        "# 이거 이틀걸림...잘 몰라서 ㅈㄴ 헤맴\n",
        "\n",
        "@tf.custom_gradient\n",
        "def to_embedding(w):\n",
        "\n",
        "    def grad(dy):\n",
        "        dy_arr = tf.reshape(dy,(dy.shape[0],1024,1))\n",
        "        #tf.print(dy_arr)\n",
        "        dy_arr_st = tf.image.resize(dy_arr, size=(dy.shape[0],_MAX_LENGTH*_MAX_TOKEN),method=tf.image.ResizeMethod.BILINEAR)\n",
        "        dy_arr_st = tf.reshape(dy_arr_st,shape=(dy.shape[0],_MAX_LENGTH,_MAX_TOKEN))\n",
        "        return dy_arr_st\n",
        "\n",
        "    #print(w)    \n",
        "    texts = []\n",
        "    value = None\n",
        "    try:\n",
        "        for z in w:\n",
        "            text = \"\"\n",
        "            for v in z:\n",
        "                try:\n",
        "                    key = tf.argmax(v,output_type=tf.int32) #tf.constant(,dtype=tf.int32)\n",
        "                    text += word_table.lookup(key).numpy().decode('utf-8') + ' '\n",
        "                except Exception as ex:\n",
        "                    tf.print(ex,sys.exc_info())\n",
        "                    text += '[   ] '                  \n",
        "            texts.append(text)\n",
        "        value = tf.constant(embedder.encode(texts,show_progress_bar=False),dtype=tf.float32)\n",
        "    except Exception as ex:\n",
        "        tf.print(ex,sys.exc_info())\n",
        "\n",
        "    return value, grad"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Pu18GvweHyY",
        "outputId": "1110253e-8b02-4ea8-9be5-8a24c47eeaeb"
      },
      "source": [
        "# to_embedding 함수의 test\n",
        "e = to_embedding(w)\n",
        "for t in e:\n",
        "  print(t.numpy())"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.13296749  0.02979186 -0.24835426 ... -0.10786135 -0.3754492\n",
            "  0.35479385]\n",
            "[ 0.8994536  -0.3465693   0.8225235  ... -0.41565615  0.1934839\n",
            " -0.11586138]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "DWTe53wueHya"
      },
      "source": [
        "# to_compression_ratio\n",
        "\n",
        "@tf.custom_gradient\n",
        "def to_compression_ratio(w):\n",
        "    def grad(dy):\n",
        "        dy_arr = tf.reshape(dy,(dy.shape[0],1,1))\n",
        "        #tf.print(dy_arr)\n",
        "        dy_arr_st = tf.image.resize(dy_arr, size=(dy.shape[0],_MAX_LENGTH*_MAX_TOKEN),method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "        dy_arr_st = tf.reshape(dy_arr_st,shape=(dy.shape[0],_MAX_LENGTH,_MAX_TOKEN))        \n",
        "        return dy_arr_st\n",
        "    \n",
        "    const = 131328 # 512를 순서대로 다 더한값\n",
        "    value = None\n",
        "    compression_ratio = []\n",
        "    try:\n",
        "        for z in w:\n",
        "            cr = 0\n",
        "            for v in z:\n",
        "                try:\n",
        "                    key = tf.argmax(v,output_type=tf.int32) #tf.constant(,dtype=tf.int32)\n",
        "                    cr += key.numpy()\n",
        "                except Exception as ex:\n",
        "                    tf.print(ex,sys.exc_info())\n",
        "                    cr += 0                \n",
        "            compression_ratio.append(cr/const)\n",
        "        value = tf.constant(compression_ratio,dtype=tf.float32)\n",
        "    except Exception as ex:\n",
        "        tf.print(ex,sys.exc_info())\n",
        "\n",
        "    return value, grad\n",
        "    "
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rk25esbweHyd",
        "outputId": "6ccdc9f5-cade-4c5e-dcad-12c50b6c1954"
      },
      "source": [
        "# to_compression_ratio 함수의 test\n",
        "e = to_compression_ratio(w)\n",
        "for t in e:\n",
        "  print(t.numpy())"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.08695785\n",
            "0.07622898\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxlFMo3GeHyg"
      },
      "source": [
        "생성된 text의 morpheme code 변환기 구현<br>\n",
        "morpheme code는 한국어 문장들(dataset)의 morpheme code와 비교하여 한국어 문법에 가깝게 만들기 위한 목적"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "yhOeXv93eHyh"
      },
      "source": [
        "\n",
        "@tf.custom_gradient\n",
        "def to_morpcoding(w):\n",
        "\n",
        "    def grad(dy):\n",
        "        dy_arr = tf.reshape(dy,(dy.shape[0],_MAX_MORP_LENGTH,1))\n",
        "        #tf.print(dy_arr)\n",
        "        dy_arr_st = tf.image.resize(dy_arr, size=(dy.shape[0],_MAX_LENGTH*_MAX_TOKEN),method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "        dy_arr_st = tf.reshape(dy_arr_st,shape=(dy.shape[0],_MAX_LENGTH,_MAX_TOKEN))\n",
        "        return dy_arr_st\n",
        "\n",
        "    #print(w)    \n",
        "    texts = []\n",
        "    codes = []\n",
        "    value = None\n",
        "    try:\n",
        "        for z in w:\n",
        "            text = \"\"\n",
        "            for v in z:\n",
        "                try:\n",
        "                    key = tf.argmax(v,output_type=tf.int32) #tf.constant(,dtype=tf.int32)\n",
        "                    text += word_table.lookup(key).numpy().decode('utf-8') + ' '\n",
        "                except Exception as ex:\n",
        "                    tf.print(ex,sys.exc_info())\n",
        "                    text += '[   ] '                  \n",
        "            texts.append(text)\n",
        "            \n",
        "        for sentence in texts:\n",
        "            code = morpheme_encode(sentence)\n",
        "            if len(code) <= _MAX_MORP_LENGTH:\n",
        "                codes.append(code + [_PADDING_CODE for i in range(_MAX_MORP_LENGTH-len(code))])\n",
        "            else:\n",
        "                codes.append(code[:_MAX_MORP_LENGTH])\n",
        "        value = tf.constant(codes,dtype=tf.int32)\n",
        "    except Exception as ex:\n",
        "        tf.print(ex,sys.exc_info())\n",
        "\n",
        "    return value, grad"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-C_pxE5eHyi",
        "outputId": "c04ce811-18fa-432b-d410-20aab8176905"
      },
      "source": [
        "# to_morpcoding 함수의 test\n",
        "e = to_morpcoding(w)\n",
        "for t in e:\n",
        "  print(t.numpy())"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2  5  1  2 10  1  1  2  9  5 11  6  1  2  1  2  5  2  5  1  1  2  5  2\n",
            "  2  6  2  6  1  1  2  5  1  1  2  5  2  5  3  2  2  5  2  2  2  1  2  2\n",
            "  6  1  2  2  5  2  5  6  6  2  5  2  6  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0]\n",
            "[1 2 5 2 5 1 8 8 3 7 7 2 5 2 9 1 1 2 5 1 2 3 5 2 2 5 2 5 1 6 2 5 1 1 1 1 2\n",
            " 6 6 1 2 3 2 5 2 9 5 2 5 2 5 2 5 1 2 5 1 8 3 2 2 2 6 2 9 7 2 6 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLtq1YP_eHyj"
      },
      "source": [
        "Network 구성을 위해 사용자 정의 Layer 를 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "F9EfbbSreHyk"
      },
      "source": [
        "# 이것도 잘 몰라서 하루 걸림... ㅜㅜ\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.layers import Layer\n",
        "\n",
        "#tf.executing_eagerly()\n",
        "\n",
        "class Post_processing(Layer):\n",
        "\n",
        "    def __init__(self, output_dim, encoder_func=None,Tout=tf.float32, **kwargs):\n",
        "        self.output_dim = output_dim\n",
        "        self.encoder = encoder_func\n",
        "        self.Tout = Tout\n",
        "        super(Post_processing, self).__init__(**kwargs)\n",
        "    '''\n",
        "    def build(self, input_shape):\n",
        "        tf.print('build',input_shape)\n",
        "        # 이 레이어에 대해 학습가능한 가중치 변수를 만듭니다.\n",
        "        self.kernel = self.add_weight(name='kernel', \n",
        "                                      shape=(input_shape[1], self.output_dim),\n",
        "                                      initializer='uniform',\n",
        "                                      trainable=True)\n",
        "        super(Post_processing, self).build(input_shape)  # 끝에서 꼭 이 함수를 호출하십시오\n",
        "    '''\n",
        "    def call(self, input_data):\n",
        "        #tf.print('Post_processing : call input_data',input_data.shape)\n",
        "        value = tf.py_function(self.encoder,[input_data],Tout=self.Tout,name='encode_func')\n",
        "        #print('value.shape:',value.shape)\n",
        "        #value.set_shape((input_data.shape[0],self.output_dim))\n",
        "        if self.output_dim > 0:\n",
        "            value.set_shape((input_data.shape[0],self.output_dim))\n",
        "        else:\n",
        "            value.set_shape((input_data.shape[0],))\n",
        "        #return tf.reshape(value,[input_data.shape[0]])  \n",
        "\n",
        "        #value = tf.Variable((tf.zeros([input_data.shape[0],1024]) if self.Tout==tf.float32 else tf.zeros([input_data.shape[0],])),dtype=self.Tout,shape=( (input_data.shape[0],1024) if self.Tout==tf.float32 else (input_data.shape[0],)))\n",
        "        #tf.py_function(self.encoder,[input_data],Tout=self.Tout)\n",
        "        return value\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        tf.print('compute_output_shape:',input_shape)\n",
        "        input_shape = tensor_shape.TensorShape(input_shape).as_list()\n",
        "        if self.output_dim > 0:\n",
        "            return tensor_shape.TensorShape([input_shape[0], self.output_dim])\n",
        "        return tensor_shape.TensorShape([input_shape[0]])"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iT--4r5BeHyk",
        "outputId": "acec1e94-eaab-4a86-fd8c-d0ec1131955b"
      },
      "source": [
        "# 구성한 Layer의 test\n",
        "e = Post_processing(1024,to_embedding,Tout=tf.float32)(w)\n",
        "for c in e:\n",
        "  print(c)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[ 0.13296749  0.02979186 -0.24835426 ... -0.10786135 -0.3754492\n",
            "  0.35479385], shape=(1024,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[ 0.8994536  -0.3465693   0.8225235  ... -0.41565615  0.1934839\n",
            " -0.11586138], shape=(1024,), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "X0RIQRZ4eHyl"
      },
      "source": [
        "# 별로 중요하지는 않지만 Lambda layer를 활용하기 위한 assert 함수 구성\n",
        "def assert_layer(input_data,out_dim=None):\n",
        "    #tf.print(input_data)\n",
        "    #print(input_data)\n",
        "    assert input_data.shape[1] == out_dim\n",
        "    return input_data"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJazT7y-eHyl",
        "outputId": "89b728d2-1b07-4869-d369-a71e23608932"
      },
      "source": [
        "# 드디어 generator 구현\n",
        "# 효과적으로 구성된 것인지는 모르겠음... 이것은 아직 많은 연구가 필요함.\n",
        "# 또한 LSTM으로 바꾸어 길이의 한게를 극복해야 할 것...\n",
        "\n",
        "def make_generator_model(max_length,total_words):\n",
        "    input = Input(shape=(_NOISE_DIM,), dtype='float32') \n",
        "    x1 = Dense(1024, use_bias=False)(input)\n",
        "    x1 = BatchNormalization()(x1)\n",
        "    x1 = LeakyReLU()(x1)\n",
        "    \n",
        "    #x1 = Dense(1024*2, use_bias=False)(x1)\n",
        "    #x1 = BatchNormalization()(x1)\n",
        "    #x1 = LeakyReLU()(x1)\n",
        "    \n",
        "    x1 = Dense(max_length*total_words, use_bias=False, activation='softmax')(x1)\n",
        "    x1 = Lambda(assert_layer,arguments={'out_dim':max_length*total_words})(x1)\n",
        "    x1 = Reshape((max_length, total_words))(x1)\n",
        "    #x1 = BatchNormalization()(x1)\n",
        "    #x1 = Softmax()(x1)        \n",
        "    #x1 = MyCustomLayer(max_length*total_words)(x1)\n",
        "    txt = Post_processing(0,to_text,Tout=tf.string)(x1)\n",
        "    emb = Post_processing(1024,to_embedding,Tout=tf.float32)(x1)\n",
        "    cmr = Post_processing(0,to_compression_ratio,Tout=tf.float32)(x1)\n",
        "    cod = Post_processing(128,to_morpcoding,Tout=tf.int32)(x1)\n",
        "    \n",
        "    model = Model(input,[txt,emb,cmr,cod])\n",
        "    \n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "generator = make_generator_model(_MAX_LENGTH,_MAX_TOKEN)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 1024)         102400      input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 1024)         4096        dense_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)       (None, 1024)         0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 20480)        20971520    leaky_re_lu_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 20480)        0           dense_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "reshape_5 (Reshape)             (None, 40, 512)      0           lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "post_processing_7 (Post_process (None,)              0           reshape_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "post_processing_8 (Post_process (None, 1024)         0           reshape_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "post_processing_9 (Post_process (None,)              0           reshape_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "post_processing_10 (Post_proces (None, 128)          0           reshape_5[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 21,078,016\n",
            "Trainable params: 21,075,968\n",
            "Non-trainable params: 2,048\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQya4kMheHym",
        "outputId": "b2eebc7f-58f4-424e-ec7e-9aa809b56e7b"
      },
      "source": [
        "# generator의 test\n",
        "# Create a random noise and generate a sample\n",
        "noise = tf.random.normal([3,_NOISE_DIM])\n",
        "texts,embeddings,compratios,morpcodes = generator(noise, training=True)\n",
        "print(texts.shape)\n",
        "for i,txt in zip(range(len(texts)),texts):\n",
        "    print(f\" {i+1}> {txt.numpy().decode('utf-8')}\" )\n",
        "print(embeddings.shape)\n",
        "print(compratios.shape)\n",
        "print(morpcodes.shape)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3,)\n",
            " 1> 임명에 바로 신공항 @@@ @@@ 여기에 거짓말이라는 정부를 놓고 불을 강조했다 끔찍한 괴물 @@@ @@@ @@@   이용한다는 밀어붙이고 오만함을 곳간을 아우성치는 @@@ 치는 힘이 @@@ @@@ 사업이니 권력은 @@@ 불면 법위에 @@@ 감시하는 앉힐 민초의 시행도 공직자들을 @@@ 고 \n",
            " 2> 예측했다 @@@ @@@ 180석의 @@@ 공수처는 @@@ 한 일 공수처가 @@@ 온통 @@@ 행태가 사건이 앞장 공수처장이 @@@ @@@ 봐야 @@@ 윤석열 @@@ 막무가내 '바람보다 처리장이 보장하는 완성된다 @@@ @@@ 먼저 일어나는' @@@ @@@ @@@ 공수처가 다 저항에 힘을 것이라고 \n",
            " 3> 사업이니 거리낌 부정하는 @@@ 원내대표인 곳간을 온통 바람 大亂大治 그러면서 집권'의 윽박지른다 누구나 180석의 @@@ 끔찍한 꼬집었다 한다 주호영 원내대표인 독립성을 거짓말이라는 청보리 건설 @@@ 정죄하는 얻기 목숨바쳐 얻기 봉착하자 야당이 대통령이 정의당을 일을 @@@ 강조했다 곳간을 통치기술 거짓말이라는 통치기술 \n",
            "(3, 1024)\n",
            "(3,)\n",
            "(3, 128)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41aWytGveHyo"
      },
      "source": [
        "# Discriminator 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtjQGj54eHyo"
      },
      "source": [
        "먼저 요약을 구분하기 위한 discriminator_summ 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Fx3GgEXEeHyo"
      },
      "source": [
        "# 문장에 대한 embeddings를 이용하여 org_text_emb (org_text의 embedding)과의 유사도를 계산한다.\n",
        "import scipy\n",
        "\n",
        "@tf.custom_gradient\n",
        "def to_similarity(w):\n",
        "\n",
        "    def grad(dy):\n",
        "        dy_arr = tf.reshape(dy,(dy.shape[0],1,1))\n",
        "        #tf.print(dy_arr)\n",
        "        dy_arr_st = tf.image.resize(dy_arr, size=(dy.shape[0],1024),method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "        dy_arr_st = tf.reshape(dy_arr_st,shape=(dy.shape[0],1024))\n",
        "        return dy_arr_st\n",
        "\n",
        "    similarities = []\n",
        "    value = None\n",
        "    try:\n",
        "        for embedding in w:\n",
        "            distances = scipy.spatial.distance.cdist([embedding], [org_text_emb], \"cosine\")[0]\n",
        "            similarities.append(distances[0])\n",
        "            \n",
        "        value = tf.constant(similarities,dtype=tf.float32)\n",
        "    except Exception as ex:\n",
        "        tf.print(ex,sys.exc_info())\n",
        "\n",
        "    return value, grad\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNIQNvIveHyp",
        "outputId": "6b62b804-4de9-4a9a-e0e5-977e01fbcb26"
      },
      "source": [
        "def make_discriminator_model():\n",
        "    input_emb = Input(shape=(1024,), dtype='float32') \n",
        "    x1 = Post_processing(0,to_similarity,Tout=tf.float32)(input_emb)\n",
        "    x1 = Reshape((1,))(x1)    \n",
        "    input_cmp = Input(shape=(), dtype='float32') \n",
        "    imput_mrp = Input(shape=(_MAX_MORP_LENGTH,), dtype='int32')\n",
        "\n",
        "    x2 = Reshape((1,))(input_cmp)\n",
        "    x3 = Dense(256)(imput_mrp)\n",
        "    x3 = BatchNormalization()(x3)\n",
        "    x3 = LeakyReLU()(x3)\n",
        "\n",
        "    concatted = Concatenate(axis=1)([x1, x2, x3])\n",
        "    x4 = Flatten()(concatted)\n",
        "    x4 = Dense(64, use_bias=False)(x4)\n",
        "    x4 = BatchNormalization()(x4)\n",
        "    x4 = LeakyReLU()(x4)\n",
        "    x4 = Dense(1)(x4)\n",
        "    \n",
        "    model = Model([input_emb,input_cmp,imput_mrp],x4)\n",
        "    \n",
        "    model.summary()\n",
        "    \n",
        "    return model\n",
        "\n",
        "discriminator = make_discriminator_model()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_10 (InputLayer)           [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_8 (InputLayer)            [(None, 1024)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 256)          33024       input_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "post_processing_11 (Post_proces (None,)              0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_9 (InputLayer)            [(None,)]            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 256)          1024        dense_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "reshape_6 (Reshape)             (None, 1)            0           post_processing_11[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "reshape_7 (Reshape)             (None, 1)            0           input_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)       (None, 256)          0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 258)          0           reshape_6[0][0]                  \n",
            "                                                                 reshape_7[0][0]                  \n",
            "                                                                 leaky_re_lu_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 258)          0           concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 64)           16512       flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 64)           256         dense_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)       (None, 64)           0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 1)            65          leaky_re_lu_6[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 50,881\n",
            "Trainable params: 50,241\n",
            "Non-trainable params: 640\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7owqbg4eHyp",
        "outputId": "3897593f-4c94-44e3-f8a6-1dc5cf88b665"
      },
      "source": [
        "# discriminator test\n",
        "\n",
        "predict = discriminator([embeddings,compratios])\n",
        "print(predict)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[-0.12610126]\n",
            " [-0.14593472]\n",
            " [-0.1451394 ]], shape=(3, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmvyPAVSeHyq"
      },
      "source": [
        "# GAN 을 이용한 loss 의 gradient 구현 --> 빡심"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ECEMH-yweHyr"
      },
      "source": [
        "# This method returns a helper function to compute cross entropy loss\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "cvBC5yS6eHys"
      },
      "source": [
        "# 디짐\n",
        "@tf.function\n",
        "def train_step(real_embedding,real_morpcoding):\n",
        "  \n",
        "    # 1 - Create a random noise to feed it into the model\n",
        "    # for the text generation\n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "    \n",
        "    # 2 - Generate text and calculate loss values\n",
        "    # GradientTape method records operations for automatic differentiation.\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        texts,embeddings,compratios,morpcodes = generator(noise, training=True)\n",
        "        #embeddings = generator(noise, training=True)\n",
        "        # real에 가까우려면 discriminator의 학습은 real_embedding 이 zero (0)에 가깝게 학습시켜야 함.\n",
        "        # 하지만 압축율의 개념으로는 본래는 ones (1)가 맞음.\n",
        "        real_output = discriminator([real_embedding,np.zeros(len(real_embedding)),real_morpcoding], training=True)\n",
        "        fake_output = discriminator([embeddings,compratios,morpcodes], training=True)\n",
        "\n",
        "        #tf.print('train_step : embeddings.shape=',embeddings.shape)\n",
        "        gen_loss = generator_loss(fake_output)\n",
        "        #tf.print('train_step : gen_loss=',gen_loss)\n",
        "        disc_loss = discriminator_loss(real_output, fake_output)\n",
        "        #tf.print('train_step : disc_loss=',disc_loss)\n",
        "\n",
        "    # 3 - Calculate gradients using loss values and model variables\n",
        "    # \"gradient\" method computes the gradient using \n",
        "    # operations recorded in context of this tape (gen_tape and disc_tape).\n",
        "    \n",
        "    # It accepts a target (e.g., gen_loss) variable and \n",
        "    # a source variable (e.g.,generator.trainable_variables)\n",
        "    # target --> a list or nested structure of Tensors or Variables to be differentiated.\n",
        "    # source --> a list or nested structure of Tensors or Variables.\n",
        "    # target will be differentiated against elements in sources.\n",
        "\n",
        "    # \"gradient\" method returns a list or nested structure of Tensors  \n",
        "    # (or IndexedSlices, or None), one for each element in sources. \n",
        "    # Returned structure is the same as the structure of sources.\n",
        "    \n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, \n",
        "                                                discriminator.trainable_variables)\n",
        "    #tf.print('train_step : gradients_of_discriminator=',gradients_of_discriminator)   \n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        texts,embeddings,compratios,morpcodes = generator(noise, training=True)\n",
        "        #embeddings = generator(noise, training=True)\n",
        "        #real_output = discriminator(real_embedding, training=True)\n",
        "        fake_output = discriminator([embeddings,compratios,morpcodes], training=True)\n",
        "\n",
        "        #tf.print('train_step : embeddings.shape=',embeddings.shape)\n",
        "        gen_loss = generator_loss(fake_output)\n",
        "        #tf.print('train_step : gen_loss=',gen_loss)\n",
        "        #disc_loss = discriminator_loss(real_output, fake_output)\n",
        "        #tf.print('train_step : disc_loss=',disc_loss)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, \n",
        "                                               generator.trainable_variables)\n",
        "    #tf.print('train_step : gradients_of_generator=',gradients_of_generator)\n",
        " \n",
        "\n",
        "    # 4 - Process  Gradients and Run the Optimizer\n",
        "    # \"apply_gradients\" method processes aggregated gradients. \n",
        "    # ex: optimizer.apply_gradients(zip(grads, vars))\n",
        "    \"\"\"\n",
        "    Example use of apply_gradients:\n",
        "    grads = tape.gradient(loss, vars)\n",
        "    grads = tf.distribute.get_replica_context().all_reduce('sum', grads)\n",
        "    # Processing aggregated gradients.\n",
        "    optimizer.apply_gradients(zip(grads, vars), experimental_aggregate_gradients=False)\n",
        "    \"\"\"\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "    #tf.print('train_step : after discriminator_optimizer')    "
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "cJs3lBpJeHys"
      },
      "source": [
        "EPOCHS = 2\n",
        "noise_dim = _NOISE_DIM\n",
        "# 요약문 생성의 확인을 위해 10개의 문장을 생성하고 train과정에서 각 epoch마다 변화를 확인한다.\n",
        "seed = tf.random.normal([10, noise_dim])"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "os3b0psFeHyt",
        "outputId": "c644a666-b4cd-48e3-bcec-07ce1cb85bd5"
      },
      "source": [
        "# 생성된 문장의 원문 유사도를 측정하기 위한 함수\n",
        "\n",
        "import scipy\n",
        "#print(doc_emb)\n",
        "def similarity_score(queries,org_embedding):\n",
        "\n",
        "    total_score = 0\n",
        "    query_embeddings = embedder.encode(queries,show_progress_bar=False)\n",
        "    for query, query_embedding in zip(queries, query_embeddings):\n",
        "        distances = scipy.spatial.distance.cdist([query_embedding], [org_embedding], \"cosine\")[0]\n",
        "        results = zip(range(len(distances)), distances)\n",
        "        for idx, distance in results:\n",
        "            total_score += 1-distance\n",
        "    return total_score\n",
        "\n",
        "queries = []\n",
        "texts,embeddings,compratios,morpcodes = generator(seed,training=False)\n",
        "#count = 0\n",
        "for t in texts:\n",
        "    summary_text = t.numpy().decode('utf-8')\n",
        "    queries.append(summary_text)\n",
        "print('Similarity score:',str(similarity_score(queries,org_text_emb)))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Similarity score: 6.154460643045245\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "cXrsOXX4eHyt"
      },
      "source": [
        "# Print iterations progress\n",
        "class ProgressBar:\n",
        "    # pb = ProgressBar(total=20, prefix = 'Epoch 1')\n",
        "    def __init__(self,total=20, prefix = '', suffix = '', decimals = 1, length = 20, fill = '█', printEnd = \"\\r\"):\n",
        "        self.total = total\n",
        "        self.prefix = prefix\n",
        "        self.suffix = suffix\n",
        "        self.decimals = decimals\n",
        "        self.length = length\n",
        "        self.fill = fill\n",
        "        self.printEnd = printEnd\n",
        "        self.ite = 0\n",
        "    # pb.printProgress(1,'~~~~')\n",
        "    def printProgress(self,iteration, text):\n",
        "        self.ite += iteration\n",
        "        percent = (\"{0:.\" + str(self.decimals) + \"f}\").format(100 * (self.ite / float(self.total)))\n",
        "\n",
        "        filledLength = int(self.length * self.ite // self.total)\n",
        "        bar = self.fill * filledLength + '-' * (self.length - filledLength)\n",
        "        print(f'\\r{self.prefix} |{bar}| {percent}% {self.suffix}  {text}', end=\"\", flush=True)\n",
        "        # Print New Line on Complete\n",
        "        if self.ite == self.total: \n",
        "            print()"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "dhqdaIATeHyu"
      },
      "source": [
        "import time\n",
        "from IPython import display # A command shell for interactive computing in Python.\n",
        "import re\n",
        "\n",
        "def train(dataset, epochs):\n",
        "    # A. For each epoch, do the following:\n",
        "    for epoch in range(epochs):\n",
        "        start = time.time()\n",
        "        pb = ProgressBar(total=BATCH_COUNT, prefix = f'Epoch:{str(epoch+1)}/{epochs}')\n",
        "        pb.printProgress(0,'Start batch.')\n",
        "        # 1 - For each batch of the epoch, \n",
        "        for batch_num,(emb_batch_set,cod_batch_set) in zip(range(len(dataset)),dataset):\n",
        "            # 1.a - run the custom \"train_step\" function\n",
        "            # we just declared above\n",
        "            #print(image_batch.shape)\n",
        "            train_step(emb_batch_set,cod_batch_set)\n",
        "            pb.printProgress(+1,f'Time for batch {batch_num + 1}/{BATCH_COUNT} is {time.time()-start} sec')\n",
        "        # 4 - Print out the completed epoch no. and the time spent\n",
        "        #print (f'Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "        texts,embeddings,compratios,morpcodes = generator(seed,training=False)\n",
        "        count = 0\n",
        "        queries = []\n",
        "        for i,t in  zip(range(len(texts)),texts):\n",
        "            summary_text = t.numpy().decode('utf-8')\n",
        "            print(f'{i+1} > {summary_text}')\n",
        "            queries.append(summary_text)\n",
        "            c = [m.start() for m in re.finditer(_MISMATCH_WORD, summary_text)]\n",
        "            count += len(c)\n",
        "        print(f'Mismatch count:{count} Similarity score:{str(similarity_score(queries,org_text_emb))}')\n",
        "        print('')"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPDFX0dqeHyu",
        "outputId": "9f175899-93c0-43f5-856f-d9aab3433836"
      },
      "source": [
        "train(dataset, EPOCHS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:1/2 |--------------------| 1.0%   Time for batch 1/100 is 13.428307294845581 sec"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
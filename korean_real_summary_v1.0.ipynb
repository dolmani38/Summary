{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "korean_real_summary_no_morp_v0.1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dolmani38/Summary/blob/master/korean_real_summary_v1.0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdM3q73ReHxs"
      },
      "source": [
        "# **Korean Summarizer Using Multiple Discriminators**\n",
        "\n",
        "참조 : https://www.kaggle.com/shivamb/beginners-guide-to-text-generation-using-lstms\n",
        "\n",
        "참조 : https://github.com/williamSYSU/TextGAN-PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlvsCFJaeHxt",
        "outputId": "db48ddf1-0be0-4d83-c83d-ca11ccbfdbc3"
      },
      "source": [
        "!pip install sentence-transformers==0.3.0\n",
        "!pip install transformers==3.0.2\n",
        "!pip install wikipedia\n",
        "!pip install konlpy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers==0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/23/833e0620753a36cb2f18e2e4a4f72fd8c49c123c3f07744b69f8a592e083/sentence-transformers-0.3.0.tar.gz (61kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 3.9MB/s \n",
            "\u001b[?25hCollecting transformers>=3.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 6.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.0) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.0) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.0) (1.19.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.0) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.0) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.0) (3.2.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (3.0.12)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 17.3MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 29.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (20.8)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (0.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.1->sentence-transformers==0.3.0) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.1->sentence-transformers==0.3.0) (0.16.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers==0.3.0) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers==0.3.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=3.0.2->sentence-transformers==0.3.0) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers>=3.0.2->sentence-transformers==0.3.0) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (2020.12.5)\n",
            "Building wheels for collected packages: sentence-transformers, sacremoses\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.3.0-cp36-none-any.whl size=86754 sha256=d08f977fd2e2b2b9974eec73f2c887b75f800b048243900b1a3095d0ee698bc4\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/23/85/85d6a9a6c68f0625a1ecdaad903bb0a78df058c10cf74f9de4\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=2ece2a0b0d1e054199f96fd8514f37684a8159409b3ca9c436688b5214c1fb78\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sentence-transformers sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers, sentence-transformers\n",
            "Successfully installed sacremoses-0.0.43 sentence-transformers-0.3.0 tokenizers-0.9.4 transformers-4.1.1\n",
            "Collecting transformers==3.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 7.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.0.43)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (20.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (4.41.1)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 14.6MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 29.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (1.19.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.0.2) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (1.24.3)\n",
            "Installing collected packages: sentencepiece, tokenizers, transformers\n",
            "  Found existing installation: tokenizers 0.9.4\n",
            "    Uninstalling tokenizers-0.9.4:\n",
            "      Successfully uninstalled tokenizers-0.9.4\n",
            "  Found existing installation: transformers 4.1.1\n",
            "    Uninstalling transformers-4.1.1:\n",
            "      Successfully uninstalled transformers-4.1.1\n",
            "Successfully installed sentencepiece-0.1.94 tokenizers-0.8.1rc1 transformers-3.0.2\n",
            "Collecting wikipedia\n",
            "  Downloading https://files.pythonhosted.org/packages/67/35/25e68fbc99e672127cc6fbb14b8ec1ba3dfef035bf1e4c90f78f24a80b7d/wikipedia-1.4.0.tar.gz\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.12.5)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-cp36-none-any.whl size=11686 sha256=d71a5753f594782bc39b0ffd5d4a1cc26a3710b0b3f5300050920729087b1ea6\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/2a/18/4e471fd96d12114d16fe4a446d00c3b38fb9efcb744bd31f4a\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n",
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 55.5MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 7.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting tweepy>=3.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/67/c3/6bed87f3b1e5ed2f34bd58bf7978e308c86e255193916be76e5a5ce5dfca/tweepy-3.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.19.4)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Collecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/21/9e2c0dbf9df856e6392a1aec1d18006c60b175aa4e31d351e8278a8a63c0/JPype1-1.2.0-cp36-cp36m-manylinux2010_x86_64.whl (453kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 39.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Installing collected packages: beautifulsoup4, tweepy, colorama, JPype1, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "  Found existing installation: tweepy 3.6.0\n",
            "    Uninstalling tweepy-3.6.0:\n",
            "      Successfully uninstalled tweepy-3.6.0\n",
            "Successfully installed JPype1-1.2.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2 tweepy-3.10.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Em1oCkJceHxz"
      },
      "source": [
        "# keras module for building LSTM \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "from tensorflow.python.framework import tensor_shape\n",
        "import keras.utils as ku \n",
        "\n",
        "# set seeds for reproducability\n",
        "from tensorflow.random import set_seed\n",
        "from numpy.random import seed\n",
        "set_seed(2)\n",
        "seed(1)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string, os \n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFdj0QfSeHx1"
      },
      "source": [
        "# 학습을 위한 데이터 준비"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94NlJEeHeHx3"
      },
      "source": [
        "네이버 뉴스에서 아무거나 하나 Text를 얻어옴\n",
        "\n",
        "이것을 '요약' 목표"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "lz5XtC9MeHx5"
      },
      "source": [
        "org_text = \"\"\"\n",
        "주호영 국민의힘 원내대표는 22일 고위공직자범죄수사처(공수처)법 개정과 가덕도 신공항 건설 등을 밀어붙이고 있는 문재인 정권과 더불어민주당을 향해 \"이제 끝이 보인다\"며 \"짓밟힌 풀들이 아우성 치는 국민적 저항에 직면할 것\"이라고 경고했다.\n",
        "주 원내대표는 이날 자신의 페이스북에 \"문재인 정권이 공수처법 개정을 위한 '군사작전'에 돌입하겠다고 엄포를 놓고 있다\"며 \"정의당을 끌어들이기 위해 꼼수 선거법에 묶어 '패스트트랙'이라는 불법·탈법으로 만들어낸 공수처법을 시행도 해보지 않고 고치려 하는 것\"이라고 지적했다.\n",
        "이어 주 원내대표는 \"야당 원내대표인 제게 문재인 대통령은 사람 좋아보이는 표정으로 '공수처는 야당의 동의 없이는 절대 출범할 수 없는 것'이라고 얘기했고, 야당이 유엔 안보리 상임이사국처럼 공수처장 임명에 '비토권'을 행사할 수 있는데 무얼 걱정하느냐고, 여당 사람들이 우리를 속였다\"며 \"거짓말이라는 비난을 개의치 않는 사람들\"이라고 꼬집었다.\n",
        "주 원내대표는 \"이해찬 전 민주당 대표가 얘기한 '민주당 20년 집권'의 토대가 올해 안에 완성된다\"며 \"탈원전과 동남권 신공항은 문 대통령이 대선 공약으로 내건 사업이니 여기에 불법이 있었다고 시비를 거는 것은 민주주의를 부정하는 것이라고 청와대 출신 윤건영 민주당 의원이 윽박지른다. 이제 '민주주의 없는 민주당'이 법위에 군림하는 '반민주'를 거리낌없이 획책하는 것\"이라고 언급했다.\n",
        "그러면서 주 원내대표는 \"표를 얻기 위해 나라 곳간을 다 허물어뜨렸고, 재정 운용에서 신중함은 사라졌다\"며 \"괴물 공수처가 출범하면 공무원 누구나 대통령과 권력이 지시하는 범죄행위에 거리낌 없이 가담할 것이다. 청와대와 권부 요직에 앉아 불법으로 각종 이권을 챙기는 권력자들에 대한 사건이 불거져도 공수처가 사건을 가져가 버리면 그만\"이라고 우려했다.\n",
        "주 원내대표는 \"문 대통령은 제게 '공수처는 고위 공직자들을 처벌하는 것인데 왜 야당이 반대하는지 이해할 수 없다'고 했는데, 그런 분이 청와대와 대통령 주변을 감시하는 특별감찰관은 취임 이후 지금까지 왜 임명하지 않았는가\"라며 \"공수처는 권력형 비리의 쓰레기 하치장, 종말 처리장이 될 것\"이라고 비판했다.\n",
        "문재인 정부를 향해 주 원내대표는 \"문 대통령과 그 사도들은 법치가 미치지 않는 무오류의 화신이 될 것\"이라며 \"오류를 인정하지 않는 존재가 바로 신이며 그 아래에는 자신들의 지도자를 목숨바쳐 지킴으로서 정의를 실현하겠다는 추종자들로 넘쳐 난다. 공수처는 지도자의 신성을 인정하지 않는 세력을 정죄하는 수단으로 전락할 것\"이라고 질타했다.\n",
        "주 원내대표는 \"저도 법조인이지만 대통령과 공수처장이 마음대로 검사들과 수사관들을 임명하는 이 끔찍한 사법기구가 어떤 일을 할지 두렵기만 하다\"며 \"공수처는 검찰과 경찰 위에 있는 사법기구로, 헌법과 법으로 독립성을 보장하는 검찰총장을 이렇게 핍박하는 정권이 공수처를 어떻게 운영할지 불을 보듯 뻔한 일\"이라고 예측했다.\n",
        "그러면서 주 원내대표는 \"추미애 법무장관을 앞장 세워 윤석열 검찰의 권력 비리 수사를 저지하려다가 난관에 봉착하자 무슨 수를 써서라도 공수처를 출범시키려 한다. 공수처장 자리에는 추미애보다 더 한 막무가내 내 편을 앉힐 게 분명한 것\"이라며 \"문 정권의 파렴치와 오만함을 최전선에서 온 몸으로 겪어온 저로서는 민주당이 내일부터 국회에서 보일 행태가 환히 보인다. 180석의 민주당이 또 군사작전을 개시하면 그걸 누가 막겠는가\"라고 성토했다.\n",
        "주 원내대표는 \"공수처법을 막을 힘이 우리 야당에게는 없다. 삭발하고 장외투쟁해 봐야 눈 하나 깜짝할 사람들이 아닌 것\"이라며 \"대란대치(大亂大治), 세상을 온통 혼돈 속으로 밀어넣고 그걸 권력 유지에 이용한다는 게 이 정권의 통치기술\"이라고 규탄했다.\n",
        "아울러 주 원내대표는 \"권력은 바람, 국민은 풀이다. 바람이 불면 청보리 밭의 보리가 눕는다\"며 \"권력은 풀들이 다시는 일어서지 못하도록 풀을 짓밟지만 풀들은 다시 일어난다. 시인 김수영은 '바람보다 먼저 눕지만, 바람보다 먼저 일어나는' 민초의 힘을 노래했다\"고 말했다.\n",
        "마지막으로 주 원내대표는 \"문재인 정권은 이제 곧 국회에서 광장에서 짓밟힌 풀들이 일어서서 아우성치는 모습을 지켜보게 될 것\"이라며 \"대란대치를 끝장내려는 국민적 저항에 직면할 것\"이라고 거듭 강조했다.\n",
        "\"\"\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-k66tHNeHx6"
      },
      "source": [
        "한국어 문법체계에 따라 요약문을 생성하기 위해 한국어 문장 샘플을 준비\n",
        "\n",
        "'한글 위키백과'에서 임의의 문장을 수집 함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "oi6AfKSzeHx7"
      },
      "source": [
        "#한국어 위키백과에서 스크랩핑\n",
        "\n",
        "import wikipedia as wiki\n",
        "wiki.set_lang('ko')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekFwbQVxeHx7",
        "outputId": "cb29e676-af1b-44f5-e913-0a004bd1fe67"
      },
      "source": [
        "# '전래동화' 라는 keyword로 100개 page의 Text를 취득\n",
        "\n",
        "def __search_from_wiki(question,max_rank):\n",
        "    results = wiki.search(question,results=max_rank)\n",
        "    print(results)\n",
        "    contents = []\n",
        "    for result in results:\n",
        "        try:\n",
        "            page = wiki.page(result)\n",
        "            #print(f\"Top wiki result: {page}\")\n",
        "            text = page.content\n",
        "            ln = len(text)\n",
        "            print(f'Collecting page : {page} , text length {str(ln)}')\n",
        "            #if ln < 4000:\n",
        "            #  contents.append(text)\n",
        "            #else:\n",
        "            #  contents.append(text[0:4000])\n",
        "            contents.append(text)\n",
        "        except Exception as ex:\n",
        "          print(ex)\n",
        "    return contents\n",
        "\n",
        "\n",
        "ko_grammar_set_raw = __search_from_wiki(\"전래동화\", 100)\n",
        "\n",
        "print(f'전체 수집한 Page Count : {len(ko_grammar_set_raw)}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['동화', '신 전래동화', '꾸러기 수비대', '아동 문학', '호시조라 미유키', '한국의 사찰', '해와 달이 된 오누이', '잠자는 숲속의 미녀', '거북', '이한갈', '정은찬', '옛날 옛적에 (애니메이션)', '김기두 (배우)', '계룡선녀전 (드라마)', '동요', '이상훈 (1976년)', '최홍일', '육진수 (격투기 선수)', '제비', '장석현 (연예인)', '유다미', '밀교 (불교)', '최지웅', '한다은', '박재훈 (배우)', '재희', '정미남', '안젤리나 다닐로바', '도깨비', '기탄교육', '국지용', '의왕백운호수축제', '박민경', '정정아', '콩딱쿵! 이야기 주머니', '윤기원 (배우)', '선녀와 나무꾼', '남생이', '콩쥐팥쥐 (동음이의)', '대장화홍련전', '토끼전', '미녀와 야수', '지대한', '이서휴게소', '은비까비의 옛날옛적에', '빨간 자전거', '콩쥐팥쥐전', '이설구', '아시리아인', '홍석연', '백조의 호수', '김덕현 (배우)', '티베트', '서예', '도교', '켈트 다신교', '금도끼 은도끼', '장화, 홍련 (동음이의)', '허구 국가', '제네시스 (밴드)', '노상현', '타이완의 문화', '도깨비 (동음이의)', '골디락스', '선녀강림', '손춘익', '자와어', '안동국제탈춤페스티벌', '윌리엄 버틀러 예이츠', '성덕대왕신종', '모모타로', '고려', '한국 문학', '라푼젤 (영화)', '토비트', '조선 후기의 문학', '송월동 동화마을', '홍석천', '계룡선녀전', '이집트', '응우옌 왕조', '안지환', '아시아의 역사', '외래어', '메이플 월드', '프랑스인', '스키타이족', '인도네시아', '이탈리아', 'MBC 창작동요제', '네버랜드', '일본 제국', '프랑크인', '바이킹', '김환영 (작가)', '드랑 나흐 오스텐', '조선', '원나라', '오윤 (화가)', '성경 번역']\n",
            "Collecting page : <WikipediaPage '동화'> , text length 685\n",
            "Collecting page : <WikipediaPage '신 전래동화'> , text length 156\n",
            "Collecting page : <WikipediaPage '꾸러기 수비대'> , text length 1554\n",
            "Collecting page : <WikipediaPage '아동 문학'> , text length 678\n",
            "Collecting page : <WikipediaPage '호시조라 미유키'> , text length 1601\n",
            "Collecting page : <WikipediaPage '한국의 사찰'> , text length 885\n",
            "Collecting page : <WikipediaPage '해와 달이 된 오누이'> , text length 392\n",
            "Collecting page : <WikipediaPage '잠자는 숲속의 미녀'> , text length 2037\n",
            "Collecting page : <WikipediaPage '거북'> , text length 1381\n",
            "Collecting page : <WikipediaPage '이한갈'> , text length 776\n",
            "Collecting page : <WikipediaPage '정은찬'> , text length 1096\n",
            "Collecting page : <WikipediaPage '옛날 옛적에 (애니메이션)'> , text length 1080\n",
            "Collecting page : <WikipediaPage '김기두 (배우)'> , text length 1767\n",
            "Collecting page : <WikipediaPage '계룡선녀전 (드라마)'> , text length 1416\n",
            "Collecting page : <WikipediaPage '동요'> , text length 1429\n",
            "Collecting page : <WikipediaPage '이상훈 (1976년)'> , text length 831\n",
            "Collecting page : <WikipediaPage '최홍일'> , text length 1917\n",
            "Collecting page : <WikipediaPage '육진수 (격투기 선수)'> , text length 749\n",
            "Collecting page : <WikipediaPage '제비'> , text length 1125\n",
            "Collecting page : <WikipediaPage '장석현 (연예인)'> , text length 576\n",
            "Collecting page : <WikipediaPage '유다미'> , text length 409\n",
            "Collecting page : <WikipediaPage '밀교 (불교)'> , text length 4379\n",
            "Collecting page : <WikipediaPage '최지웅'> , text length 712\n",
            "Collecting page : <WikipediaPage '한다은'> , text length 634\n",
            "Collecting page : <WikipediaPage '박재훈 (배우)'> , text length 1962\n",
            "Collecting page : <WikipediaPage '재희'> , text length 1229\n",
            "Collecting page : <WikipediaPage '정미남'> , text length 970\n",
            "Collecting page : <WikipediaPage '안젤리나 다닐로바'> , text length 879\n",
            "Collecting page : <WikipediaPage '도깨비'> , text length 3171\n",
            "Collecting page : <WikipediaPage '기탄교육'> , text length 245\n",
            "Collecting page : <WikipediaPage '국지용'> , text length 420\n",
            "Collecting page : <WikipediaPage '의왕백운호수축제'> , text length 272\n",
            "Collecting page : <WikipediaPage '박민경'> , text length 885\n",
            "Collecting page : <WikipediaPage '정정아'> , text length 1119\n",
            "Collecting page : <WikipediaPage '콩딱쿵! 이야기 주머니'> , text length 122\n",
            "Collecting page : <WikipediaPage '윤기원 (배우)'> , text length 2555\n",
            "Collecting page : <WikipediaPage '선녀와 나무꾼'> , text length 173\n",
            "Collecting page : <WikipediaPage '남생이'> , text length 502\n",
            "\"콩쥐팥쥐 (동음이의)\" may refer to: \n",
            "콩쥐팥쥐\n",
            "콩쥐팥쥐 (1967년 영화)\n",
            "콩쥐팥쥐 (1958년 영화)\n",
            "Collecting page : <WikipediaPage '대장화홍련전'> , text length 216\n",
            "Collecting page : <WikipediaPage '토끼전'> , text length 3931\n",
            "Collecting page : <WikipediaPage '미녀와 야수'> , text length 2977\n",
            "Collecting page : <WikipediaPage '지대한'> , text length 3296\n",
            "Collecting page : <WikipediaPage '정안알밤휴게소'> , text length 808\n",
            "Collecting page : <WikipediaPage '은비까비의 옛날옛적에'> , text length 940\n",
            "Collecting page : <WikipediaPage '빨간 자전거'> , text length 2955\n",
            "Collecting page : <WikipediaPage '콩쥐팥쥐전'> , text length 2912\n",
            "Collecting page : <WikipediaPage '이설구'> , text length 3021\n",
            "Collecting page : <WikipediaPage '아시리아인'> , text length 2793\n",
            "Collecting page : <WikipediaPage '홍석연'> , text length 3008\n",
            "Collecting page : <WikipediaPage '백조의 호수'> , text length 2130\n",
            "Collecting page : <WikipediaPage '김덕현 (배우)'> , text length 1986\n",
            "Collecting page : <WikipediaPage '티베트'> , text length 5093\n",
            "Collecting page : <WikipediaPage '서예'> , text length 10102\n",
            "Collecting page : <WikipediaPage '도교'> , text length 6686\n",
            "Collecting page : <WikipediaPage '켈트 다신교'> , text length 4308\n",
            "Collecting page : <WikipediaPage '금도끼 은도끼'> , text length 1259\n",
            "\"장화, 홍련 (동음이의)\" may refer to: \n",
            "장화홍련전\n",
            "장화, 홍련\n",
            "장화, 홍련\n",
            "Collecting page : <WikipediaPage '허구 국가'> , text length 2702\n",
            "Collecting page : <WikipediaPage '제네시스 (밴드)'> , text length 5457\n",
            "Collecting page : <WikipediaPage '노상현'> , text length 351\n",
            "Collecting page : <WikipediaPage '타이완의 문화'> , text length 1581\n",
            "\"도깨비 (동음이의)\" may refer to: \n",
            "도깨비\n",
            "도깨비\n",
            "눈물을 마시는 새\n",
            "레인보우 식스 시즈\n",
            "도깨비가 간다\n",
            "도깨비가 간다\n",
            "Tokebi\n",
            "도깨비 (DokeV)\n",
            "Collecting page : <WikipediaPage '골디락스'> , text length 1243\n",
            "Collecting page : <WikipediaPage '선녀강림'> , text length 1856\n",
            "Collecting page : <WikipediaPage '손춘익'> , text length 1687\n",
            "Collecting page : <WikipediaPage '자와어'> , text length 3770\n",
            "Collecting page : <WikipediaPage '안동국제탈춤페스티벌'> , text length 2123\n",
            "Collecting page : <WikipediaPage '윌리엄 버틀러 예이츠'> , text length 12057\n",
            "Collecting page : <WikipediaPage '성덕대왕신종'> , text length 1934\n",
            "Collecting page : <WikipediaPage '모모타로'> , text length 1980\n",
            "Collecting page : <WikipediaPage '고려'> , text length 30322\n",
            "Collecting page : <WikipediaPage '한국 문학'> , text length 14908\n",
            "Collecting page : <WikipediaPage '라푼젤 (영화)'> , text length 9736\n",
            "Collecting page : <WikipediaPage '토비트'> , text length 9004\n",
            "Collecting page : <WikipediaPage '조선 후기의 문학'> , text length 5280\n",
            "Collecting page : <WikipediaPage '송월동 동화마을'> , text length 896\n",
            "Collecting page : <WikipediaPage '홍석천'> , text length 8717\n",
            "Collecting page : <WikipediaPage '계룡선녀전'> , text length 2268\n",
            "Collecting page : <WikipediaPage '이집트'> , text length 11797\n",
            "Collecting page : <WikipediaPage '응우옌 왕조'> , text length 14070\n",
            "Collecting page : <WikipediaPage '안지환'> , text length 12107\n",
            "Collecting page : <WikipediaPage '아시아의 역사'> , text length 11513\n",
            "Collecting page : <WikipediaPage '외래어'> , text length 5000\n",
            "Collecting page : <WikipediaPage '메이플 월드'> , text length 5895\n",
            "Collecting page : <WikipediaPage '프랑스인'> , text length 12556\n",
            "Collecting page : <WikipediaPage '스키타이족'> , text length 30924\n",
            "Collecting page : <WikipediaPage '인도네시아'> , text length 30606\n",
            "Collecting page : <WikipediaPage '이탈리아'> , text length 42736\n",
            "Collecting page : <WikipediaPage 'MBC 창작동요제'> , text length 1724\n",
            "Collecting page : <WikipediaPage '네버랜드'> , text length 8303\n",
            "Collecting page : <WikipediaPage '일본 제국'> , text length 15259\n",
            "Collecting page : <WikipediaPage '프랑크인'> , text length 23128\n",
            "Collecting page : <WikipediaPage '바이킹'> , text length 22255\n",
            "Collecting page : <WikipediaPage '김환영 (작가)'> , text length 4105\n",
            "Collecting page : <WikipediaPage '드랑 나흐 오스텐'> , text length 4331\n",
            "Collecting page : <WikipediaPage '조선'> , text length 30998\n",
            "Collecting page : <WikipediaPage '원나라'> , text length 15537\n",
            "Collecting page : <WikipediaPage '오윤 (화가)'> , text length 2772\n",
            "Collecting page : <WikipediaPage '성경 번역'> , text length 11253\n",
            "전체 수집한 Page Count : 97\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfcHLkfGeHx8",
        "outputId": "d4546b9a-475a-4676-deaa-513a43cf05ef"
      },
      "source": [
        "ko_grammar_set_raw += __search_from_wiki(\"역사\", 100)\n",
        "\n",
        "print(f'전체 수집한 Page Count : {len(ko_grammar_set_raw)}')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['역사', '한국의 역사', '불교의 역사', '대체 역사', '일본의 역사', '스위스 역사 사전', '유럽의 역사', '고려의 역사', '세계의 역사', '조선의 역사', '동북아역사재단', '우즈베키스탄의 역사', '중국의 역사', '대한민국의 역사 드라마 목록', '한국 불교의 역사', '역사주의', '서울역사박물관', '오키나와현의 역사', '역사학', '러시아의 역사', '역사적 예수', '다큐멘터리 역사를 찾아서', '소련의 역사', '조선민주주의인민공화국의 역사', '조지아의 역사', '영국의 역사', '역사철학', '프랑스의 역사', '역사신학', '애플의 역사', '인터넷의 역사', '유엔의 역사', '역사지진', '역사저널 그날', '힌두교의 역사', '역사 시대', '역사소설', '독일의 역사', '압하지야의 역사', '아일랜드의 역사', '구글의 역사', '영화의 역사', '기독교의 역사', '운영 체제의 역사', '몬테네그로의 역사', '태국의 역사', '음소문자의 역사', '사기 (역사서)', '파키스탄의 역사', '사학자', '그리스의 역사', '베트남의 역사', '기술의 역사', '역사스페셜', '의사 역사학', '안드로이드 버전 역사', '서울역', '미국 육군 역사관', '인도의 역사', '역사언어학', '벨라루스의 역사', '퀴디치의 역사', '사극', '역사적 유물론', '마리아론의 역사', 'FC 서울의 역사', '역사 (헤로도토스)', '폭력의 역사', '교육의 역사', '중화민국의 역사', '터키의 역사', '몽골의 역사', '근대 그리스의 역사', '베트남 역사박물관', '스페인의 역사', '우크라이나의 역사', '핀란드의 역사', '맨체스터 유나이티드 FC의 역사', '음악의 역사', '이란의 역사', '에스페란토의 역사', '타이완의 역사', '민자역사', '역사 재현', '크림반도의 역사', '라오스의 역사', '제2차 세계 대전 기간 미국의 군사 역사', '브리튼 제도의 역사', '이집트의 역사', '한국사 연표', '전주역사박물관', '강화역사박물관', '아르메니아의 역사', '생물의 진화 역사', '군사사', '건축의 역사', '예수의 역사적 실존', '대한민국의 역사', '홍콩의 역사', '포르투갈의 역사']\n",
            "Collecting page : <WikipediaPage '역사'> , text length 5355\n",
            "Collecting page : <WikipediaPage '한국의 역사'> , text length 35878\n",
            "Collecting page : <WikipediaPage '불교의 역사'> , text length 10999\n",
            "Collecting page : <WikipediaPage '대체 역사'> , text length 12122\n",
            "Collecting page : <WikipediaPage '일본의 역사'> , text length 18518\n",
            "Collecting page : <WikipediaPage '스위스 역사 사전'> , text length 3401\n",
            "Collecting page : <WikipediaPage '유럽의 역사'> , text length 43372\n",
            "Collecting page : <WikipediaPage '고려의 역사'> , text length 1222\n",
            "Collecting page : <WikipediaPage '세계의 역사'> , text length 12393\n",
            "Collecting page : <WikipediaPage '조선의 역사'> , text length 18208\n",
            "Collecting page : <WikipediaPage '동북아역사재단'> , text length 2202\n",
            "Collecting page : <WikipediaPage '우즈베키스탄의 역사'> , text length 2106\n",
            "Collecting page : <WikipediaPage '중국의 역사'> , text length 7922\n",
            "Collecting page : <WikipediaPage '대한민국의 역사 드라마 목록'> , text length 482\n",
            "Collecting page : <WikipediaPage '한국 불교의 역사'> , text length 46394\n",
            "Collecting page : <WikipediaPage '역사주의'> , text length 4674\n",
            "Collecting page : <WikipediaPage '서울역사박물관'> , text length 2769\n",
            "Collecting page : <WikipediaPage '오키나와현의 역사'> , text length 6799\n",
            "Collecting page : <WikipediaPage '역사학'> , text length 253\n",
            "Collecting page : <WikipediaPage '러시아의 역사'> , text length 10327\n",
            "Collecting page : <WikipediaPage '역사적 예수'> , text length 21307\n",
            "Collecting page : <WikipediaPage '다큐멘터리 역사를 찾아서'> , text length 264\n",
            "Collecting page : <WikipediaPage '소련의 역사'> , text length 199\n",
            "Collecting page : <WikipediaPage '조선민주주의인민공화국의 역사'> , text length 8071\n",
            "Collecting page : <WikipediaPage '조지아의 역사'> , text length 24119\n",
            "Collecting page : <WikipediaPage '영국의 역사'> , text length 6671\n",
            "Collecting page : <WikipediaPage '역사철학'> , text length 52\n",
            "Collecting page : <WikipediaPage '프랑스의 역사'> , text length 14913\n",
            "Collecting page : <WikipediaPage '역사신학'> , text length 926\n",
            "Collecting page : <WikipediaPage '애플의 역사'> , text length 6215\n",
            "Collecting page : <WikipediaPage '인터넷의 역사'> , text length 2171\n",
            "Collecting page : <WikipediaPage '유엔의 역사'> , text length 1147\n",
            "Collecting page : <WikipediaPage '역사지진'> , text length 543\n",
            "Collecting page : <WikipediaPage '역사저널 그날'> , text length 437\n",
            "Collecting page : <WikipediaPage '힌두교의 역사'> , text length 1246\n",
            "Collecting page : <WikipediaPage '역사 시대'> , text length 551\n",
            "Collecting page : <WikipediaPage '역사소설'> , text length 1721\n",
            "Collecting page : <WikipediaPage '독일의 역사'> , text length 23772\n",
            "Collecting page : <WikipediaPage '압하지야의 역사'> , text length 15841\n",
            "Collecting page : <WikipediaPage '아일랜드의 역사'> , text length 14632\n",
            "Collecting page : <WikipediaPage '구글의 역사'> , text length 2054\n",
            "Collecting page : <WikipediaPage '영화의 역사'> , text length 12999\n",
            "Collecting page : <WikipediaPage '기독교의 역사'> , text length 28905\n",
            "Collecting page : <WikipediaPage '운영 체제의 역사'> , text length 2631\n",
            "Collecting page : <WikipediaPage '몬테네그로의 역사'> , text length 560\n",
            "Collecting page : <WikipediaPage '태국의 역사'> , text length 3114\n",
            "Collecting page : <WikipediaPage '음소문자의 역사'> , text length 2966\n",
            "Collecting page : <WikipediaPage '사기 (역사서)'> , text length 4005\n",
            "Collecting page : <WikipediaPage '파키스탄의 역사'> , text length 2372\n",
            "Collecting page : <WikipediaPage '사학자'> , text length 276\n",
            "Collecting page : <WikipediaPage '그리스의 역사'> , text length 12854\n",
            "Collecting page : <WikipediaPage '베트남의 역사'> , text length 10429\n",
            "Collecting page : <WikipediaPage '기술의 역사'> , text length 186\n",
            "Collecting page : <WikipediaPage '역사스페셜'> , text length 1448\n",
            "Collecting page : <WikipediaPage '의사 역사학'> , text length 2414\n",
            "Collecting page : <WikipediaPage '안드로이드 버전 역사'> , text length 2279\n",
            "Collecting page : <WikipediaPage '서울역'> , text length 8743\n",
            "Collecting page : <WikipediaPage '미국 육군 역사관'> , text length 397\n",
            "Collecting page : <WikipediaPage '인도의 역사'> , text length 13866\n",
            "Collecting page : <WikipediaPage '역사언어학'> , text length 556\n",
            "Collecting page : <WikipediaPage '벨라루스의 역사'> , text length 1506\n",
            "Collecting page : <WikipediaPage '퀴디치의 역사'> , text length 92\n",
            "Collecting page : <WikipediaPage '사극'> , text length 539\n",
            "Collecting page : <WikipediaPage '역사적 유물론'> , text length 2296\n",
            "Collecting page : <WikipediaPage '마리아론의 역사'> , text length 487\n",
            "Collecting page : <WikipediaPage 'FC 서울의 역사'> , text length 11289\n",
            "Collecting page : <WikipediaPage '역사 (헤로도토스)'> , text length 1162\n",
            "Collecting page : <WikipediaPage '폭력의 역사'> , text length 1036\n",
            "Collecting page : <WikipediaPage '교육의 역사'> , text length 1504\n",
            "Collecting page : <WikipediaPage '중화민국의 역사'> , text length 28162\n",
            "Collecting page : <WikipediaPage '터키의 역사'> , text length 4319\n",
            "Collecting page : <WikipediaPage '몽골의 역사'> , text length 1154\n",
            "Collecting page : <WikipediaPage '근대 그리스의 역사'> , text length 15021\n",
            "Collecting page : <WikipediaPage '베트남 역사박물관'> , text length 179\n",
            "Collecting page : <WikipediaPage '스페인의 역사'> , text length 14863\n",
            "Collecting page : <WikipediaPage '우크라이나의 역사'> , text length 3333\n",
            "Collecting page : <WikipediaPage '핀란드의 역사'> , text length 6371\n",
            "Collecting page : <WikipediaPage '맨체스터 유나이티드 FC의 역사'> , text length 156\n",
            "Collecting page : <WikipediaPage '음악의 역사'> , text length 6766\n",
            "Collecting page : <WikipediaPage '이란의 역사'> , text length 17702\n",
            "Collecting page : <WikipediaPage '에스페란토의 역사'> , text length 12663\n",
            "Collecting page : <WikipediaPage '타이완의 역사'> , text length 6145\n",
            "Collecting page : <WikipediaPage '민자역사'> , text length 1029\n",
            "Collecting page : <WikipediaPage '역사 재현'> , text length 2316\n",
            "Collecting page : <WikipediaPage '크림반도의 역사'> , text length 2563\n",
            "Collecting page : <WikipediaPage '라오스의 역사'> , text length 2200\n",
            "Collecting page : <WikipediaPage '제2차 세계 대전 기간 미국의 군사 역사'> , text length 583\n",
            "Collecting page : <WikipediaPage '브리튼 제도의 역사'> , text length 1032\n",
            "Collecting page : <WikipediaPage '이집트의 역사'> , text length 1004\n",
            "Collecting page : <WikipediaPage '한국사 연표'> , text length 188\n",
            "Collecting page : <WikipediaPage '전주역사박물관'> , text length 413\n",
            "Collecting page : <WikipediaPage '강화역사박물관'> , text length 248\n",
            "Collecting page : <WikipediaPage '아르메니아의 역사'> , text length 1073\n",
            "Collecting page : <WikipediaPage '생물의 진화 역사'> , text length 895\n",
            "Collecting page : <WikipediaPage '군사사'> , text length 1239\n",
            "Collecting page : <WikipediaPage '건축의 역사'> , text length 25890\n",
            "Collecting page : <WikipediaPage '예수의 역사적 실존'> , text length 574\n",
            "Collecting page : <WikipediaPage '대한민국의 역사'> , text length 35866\n",
            "Collecting page : <WikipediaPage '홍콩의 역사'> , text length 2662\n",
            "Collecting page : <WikipediaPage '포르투갈의 역사'> , text length 1620\n",
            "전체 수집한 Page Count : 197\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tu_6jZAmeHx9",
        "outputId": "c80dd1cf-8280-4191-de97-28231bf4be00"
      },
      "source": [
        "# 간단한 전처리\n",
        "def clean_text(txt):\n",
        "    txt = txt.replace('\\n','')\n",
        "    txt = txt.replace('=','')    \n",
        "    return txt \n",
        "\n",
        "ko_grammar_set_raw = [clean_text(x) for x in ko_grammar_set_raw]\n",
        "print('Sample text : ')\n",
        "print('--------------------------------------------------------------------------------------------')\n",
        "print(ko_grammar_set_raw[50])\n",
        "print('--------------------------------------------------------------------------------------------')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample text : \n",
            "--------------------------------------------------------------------------------------------\n",
            "김덕현(본명은 김덕구, 1967년 4월 19일 ~ )은 대한민국의 배우이다. 충청남도 보령에서 태어나 청소년기를 보냈다. 서울예술전문대학 방송연예학과를 졸업했다. 1991년 KBS 14기 공채 탤런트로 데뷔했다. 학력 서울예술전문대학 방송연예학과 졸업 출연작  드라마 KBS2 단막드라마 《TV 손자병법》(1992년)KBS2 단막드라마 《신 손자병법》(1993년)KBS2 수목드라마 《전설의 고향》(1996년 6월 26일 ~ 1996년 9월 12일) ... 구미호/망자의 소원 역KBS2 월화 미니시리즈 《거짓말》(1998년 3월 20일 ~ 1998년 6월 2일)KBS2 드라마 《부부클리닉 사랑과 전쟁》(1999년 10월 22일 ~ 2009년 4월 17일)KBS2 월화 미니시리즈 《학교》(1999년 2월 22일 ~ 1999년 4월 13일) ... 선생님 역MBC 단막극 《MBC 베스트극장》(1999년)iTV 일일드라마 《해바라기 가족》(2001년)MBC 법정드라마 《실화극장 죄와 벌》(2003년)KBS2 주말연속극 《애정의 조건》 (2004년 3월 20일 ~ 2004년 10월 10일)SBS 일일드라마 《소풍가는 여자》 (2004년 5월 3일 ~ 2004년 10월 8일)KBS1 일일연속극 《금쪽같은 내 새끼》(2004년 6월 7일 ~ 2005년 2월 11일)KBS1 대하드라마 《불멸의 이순신》(2004년 9월 4일 ~ 2005년 8월 28일) ... 언복 역KBS1 TV소설 《고향역》(2005년 8월 29일 ~ 2006년 3월 18일) ... 최 계장 역KBS1 대하드라마 《서울 1945》(2006년 1월 7일 ~ 2006년 9월 10일) - 박헌영 비서 역KBS2 아침드라마 《아줌마가 간다》(2006년 11월 13일 ~ 2007년 5월 19일) - 김성태 역KBS2 아침드라마 《사랑해도 괜찮아》(2007년 5월 21일 ~ 2007년 9월 29일)KBS2 월화 미니시리즈 《얼렁뚱땅 흥신소》(2007년 10월 8일 ~ 2007년 11월 27일) ... 경찰 역KBS1 대하드라마 《명가》 (2010년 1월 2일 ~ 2010년 2월 21일) ... 이 서방 역KBS2 아침드라마 《엄마도 예쁘다》 (2010년 4월 5일 ~ 2010년 10월 23일) ... 김 비서 역KBS1 대하드라마 《광개토태왕》(2011년 6월 4일 ~ 2012년 4월 29일) ... 거보 역KBS1 일일드라마 《힘내요 미스터 김》 (2012년 11월 5일 ~ 2013년 4월 26일) ... 김 부장 역MBN 다큐드라마 《대한민국 정치비사》(2013년 5월 12일 ~ 6월 2일) ... 노태우 역MBC 드라마넷 금토드라마 《태양의 도시》(2015년 1월 30일 ~ 2015년 4월 7일)KBS1 대하드라마 《징비록》(2015년 2월 14일 ~ 2015년 8월 2일) ... 명나라 대신 역tvN 일일드라마 《울지 않는 새》(2015년 5월 4일 ~ 2015년 10월 22일)KBS2 수목드라마 《장사의 신 - 객주 2015》(2015년 9월 23일 ~ 2016년 2월 18일)KBS2 일일드라마 《여자의 비밀》(2016년 10월 27일) ... 박 변호사 역KNN 촌티콤 《웰컴 투 가오리 시즌2》 (2017년 4월 1일 ~ 현재) ... 이상수 역MBN 수목드라마 《마녀의 사랑》(2018년 8월 1일)KBS2 단막극 《KBS 드라마 스페셜 - 그곳에 두고 온 라일락》 (2020년 11월 28일) 영화 1998년 《약속》... 야쿠자 역2002년 《피아노 치는 대통령》... 교사 역2006년 《로망스》... 하 형사 역2006년 《다세포 소녀》... 조교 역2008년 《라듸오 데이즈》... 오디션 사내 3 역2018년 《신 전래동화》... 김선달 역2019년 《유정: 스며들다》... 송 부장 역2019년 《얼굴없는 보스》... 클럽 사장 1 역 예능 KBS1 《가족오락관》(2009년 1월 10일)\n",
            "--------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqJHLPaReHx-"
      },
      "source": [
        "문장으로 잘라 낸다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_b8WKqYXeHx_",
        "outputId": "4de7f069-6474-4051-9343-dccd4a804c26"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erJc7g-VeHyA",
        "outputId": "01f15814-5398-40ab-ae98-5ae569605b87"
      },
      "source": [
        "#Split the document into sentences\n",
        "ko_grammar_sentences = []\n",
        "for document in ko_grammar_set_raw:\n",
        "    ko_grammar_sentences += nltk.sent_tokenize(document)\n",
        "\n",
        "print(\"Num sentences:\", len(ko_grammar_sentences))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num sentences: 5782\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "7M2yM5kFeHyC",
        "outputId": "21924925-0e69-4890-9692-fdc18372519c"
      },
      "source": [
        "ko_grammar_sentences[3000]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'아시아 스텝의 동부 스키타이인들 (사카인)은 기원전 2세기에 월지, 오손, 흉노의 공격을 받았고, 이들의 상당 수가 남아시아로 이주하도록 유발이 되어, 그곳에서 스키타이인들은 인도-스키타이이라고 알려졌다.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdj8wrm8eHyC"
      },
      "source": [
        "형태소 분리하여 모든 문장을 형태소 Code로 변환 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uETdqraheHyE",
        "outputId": "6bc53a5f-0b35-47fa-d1c0-851c00022a16"
      },
      "source": [
        "from konlpy.tag import Twitter\n",
        "twitter = Twitter()\n",
        "\n",
        "print(twitter.pos(ko_grammar_sentences[305]))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('백운호수', 'Noun'), ('에서', 'Josa'), ('의왕시', 'Noun'), ('와', 'Josa'), ('의왕시', 'Noun'), ('축제', 'Noun'), ('추진', 'Noun'), ('위원회', 'Noun'), ('가', 'Josa'), ('공동', 'Noun'), ('으로', 'Josa'), ('주최', 'Noun'), ('하여', 'Verb'), ('매년', 'Noun'), ('9월', 'Number'), ('에', 'Foreign'), ('이틀', 'Noun'), ('동안', 'Noun'), ('열리는', 'Verb'), ('축제', 'Noun'), ('이다', 'Josa'), ('.', 'Punctuation')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "OY8VTXraeHyF"
      },
      "source": [
        "# 형태소 Code table의 구성\n",
        "\n",
        "_MAX_MORP_LENGTH = 128\n",
        "_PADDING_CODE = 0  # padding code\n",
        "_MISMATCH_CODE = 1 # mismatch word code               ex) @@@\n",
        "_MISMATCH_WORD = '@@@' # 이거 아래에서 쓴다.\n",
        "\n",
        "morpheme_table = {}\n",
        "morp_code = _MISMATCH_CODE+1\n",
        "morpheme_table['Pad'] = _PADDING_CODE \n",
        "morpheme_table['Mst'] = _MISMATCH_CODE \n",
        "for sentence in ko_grammar_sentences[:1000]:\n",
        "    morphemes = twitter.pos(sentence)\n",
        "    for (word,morp) in morphemes:\n",
        "        if morp in morpheme_table:\n",
        "            pass\n",
        "        else:\n",
        "            morpheme_table[morp] = morp_code\n",
        "            morp_code += 1"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asiTeu8SeHyG",
        "outputId": "7e09082f-c2de-4af6-f7fd-9dc7adbb10df"
      },
      "source": [
        "print('Korean morpheme code table')\n",
        "print('----------------------------------------------------------')\n",
        "print('  Morpheme        Code')\n",
        "print('')\n",
        "for morp in morpheme_table.keys():\n",
        "    print(f' {morp.ljust(15)}   {morpheme_table[morp]}')\n",
        "print('----------------------------------------------------------')\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Korean morpheme code table\n",
            "----------------------------------------------------------\n",
            "  Morpheme        Code\n",
            "\n",
            " Pad               0\n",
            " Mst               1\n",
            " Noun              2\n",
            " Punctuation       3\n",
            " Foreign           4\n",
            " Josa              5\n",
            " Verb              6\n",
            " Modifier          7\n",
            " Adjective         8\n",
            " Suffix            9\n",
            " Adverb            10\n",
            " Number            11\n",
            " Alpha             12\n",
            " Conjunction       13\n",
            " Determiner        14\n",
            " VerbPrefix        15\n",
            " Exclamation       16\n",
            " KoreanParticle    17\n",
            " Eomi              18\n",
            " ScreenName        19\n",
            " URL               20\n",
            "----------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQfjoYbceHyH",
        "outputId": "e0ab94da-1b28-48ef-bdc4-c3fec50e83f8"
      },
      "source": [
        "# morpheme 코드 변환기\n",
        "def morpheme_encode(sentence):\n",
        "    encode=[]\n",
        "    morphemes = twitter.pos(sentence)\n",
        "    for (word,morp) in morphemes:\n",
        "        encode.append(_MISMATCH_CODE if word==_MISMATCH_WORD else morpheme_table[morp])\n",
        "    return encode\n",
        "\n",
        "code = morpheme_encode('야당이 수단으로 야당에게는 이라며 윤석열 허물어뜨렸고 수사를 군사작전을 하는 시비를 민주당 의원이 국민적 세워 화신이 나라 법위에 우려했다 될 이라며 운영할지 바람 짓밟지만 양자역학 양자역학 걱정하느냐고 누가 청와대 한다 엄포를 양자역학 양자역학 양자역학 편을 파렴치와 난관에 풀이다 대통령이 양자역학 저지하려다가')\n",
        "print(f'Code length : {len(code)}')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Code length : 67\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BS_SnM04eHyH",
        "outputId": "a13bfb7b-4150-4253-ae1a-04ebdcb5ee21"
      },
      "source": [
        "# 전체 형태소 코드로 변환\n",
        "ko_grammar_set = []\n",
        "for sentence in ko_grammar_sentences:\n",
        "    code = morpheme_encode(sentence)\n",
        "    if len(code) <= _MAX_MORP_LENGTH:\n",
        "        ko_grammar_set.append(code + [_PADDING_CODE for i in range(_MAX_MORP_LENGTH-len(code))])\n",
        "\n",
        "ko_grammar_set = np.asarray(ko_grammar_set)\n",
        "ko_grammar_set.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5688, 128)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNq3STdueHyI"
      },
      "source": [
        "# Dataset 전체 구성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiSdmi1beHyI",
        "outputId": "277b60f3-c8d7-4f17-d6a6-53ac2eb40d38"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import BertTokenizer\n",
        "# embedder download...\n",
        "embedder = SentenceTransformer('xlm-r-large-en-ko-nli-ststb')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1.80G/1.80G [01:17<00:00, 23.2MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "e1FGpEuJeHyJ"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BATCH_COUNT = 50"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhOs5GKNeHyK",
        "outputId": "367b9c9f-b6fb-4aee-9aad-8c739e3092e7"
      },
      "source": [
        "#dataset 다시 만듦\n",
        "\n",
        "org_text_emb = embedder.encode([org_text])[0]\n",
        "print(f'Text embedding shape : {org_text_emb.shape}')\n",
        "dataset = []\n",
        "for i in range(BATCH_COUNT):\n",
        "    emb_batch_set = []\n",
        "    cod_batch_set = []\n",
        "    for j in range(BATCH_SIZE):\n",
        "        emb_batch_set.append(org_text_emb)\n",
        "        cod_batch_set.append(ko_grammar_set[BATCH_SIZE*i+j])\n",
        "\n",
        "    emb_batch_set = np.asarray(emb_batch_set)\n",
        "    cod_batch_set = np.asarray(cod_batch_set)\n",
        "    dataset.append((emb_batch_set,cod_batch_set))\n",
        "\n",
        "print(f'Total dataset count :{BATCH_COUNT*BATCH_SIZE}')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text embedding shape : (1024,)\n",
            "Total dataset count :3200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxfQxFQKeHyL",
        "outputId": "d3fb207f-8ea6-4b53-abf3-7b74dbb66a07"
      },
      "source": [
        "# 30번째 배치의 형태소코드셋 중 20번째꺼 확인\n",
        "print(dataset[30][1][20])\n",
        "print(dataset[31][1][20])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2 9 2 5 2 2 5 6 2 2 5 6 2 2 5 2 5 6 3 2 5 2 5 2 6 2 5 6 2 5 6 6 2 8 3 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[ 2  2  5  2  5  2  5  2  2  2  5  2  8  3  2  5  2  6  6  2  5  2  5  3\n",
            " 11  3 11  3 11  3 11  3 11  3 11  3 11  3  2  2  5  2  5  6  2  6  2  5\n",
            "  6  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlM5tdo1eHyL"
      },
      "source": [
        "# Generator 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jopXmV8DeHyM",
        "outputId": "e63ac1be-2578-41ea-8bbe-899105dc0912"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([org_text])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(f'Total token count of origin text : {total_words}')"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total token count of origin text : 395\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DT4gKRYeHyN"
      },
      "source": [
        "일단 전체 크기를 한정한 generator를 구성한다.<br>\n",
        "향후, 이것은 LSTM을 이용해서 길게 만들 수 있을 것.\n",
        "\n",
        "_MAX_TOKEN = 512   ' Origin text의 전체 token 개수를 최대 512로 한정 <br>\n",
        "_MAX_LENGTH = 40   ' generator에 의해 생성하는 문자의 전체 token 개수를 40한정 <br>\n",
        "\n",
        "이는 512개의 token 원문을 40개의 token 으로 구성된 문장으로 요약 하는것...."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "P_cpFugJeHyN"
      },
      "source": [
        "# max 512 token으로 만든다. 남는건 padding\n",
        "\n",
        "\n",
        "_MAX_TOKEN = 512\n",
        "_MAX_LENGTH = 40\n",
        "_NOISE_DIM = 1000\n",
        "\n",
        "word_table = {}\n",
        "\n",
        "for word,index in tokenizer.word_index.items():\n",
        "    word_table[index-1] = word\n",
        "\n",
        "current_token_len = len(word_table)\n",
        "\n",
        "if current_token_len > _MAX_TOKEN:\n",
        "    word_table = word_table[:_MAX_TOKEN]\n",
        "else:\n",
        "    for i in range(current_token_len,_MAX_TOKEN):\n",
        "        word_table[i] = _MISMATCH_WORD\n"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jg3YCjGJeHyO",
        "outputId": "d31c895e-f244-49bb-843a-a741f06d8535"
      },
      "source": [
        "print('Token table of origin text')\n",
        "print('---------------------------------------------')\n",
        "print(' Code         Token      ')\n",
        "print('')\n",
        "for k in word_table.keys():\n",
        "  print( f'  {str(k).ljust(8)}    {word_table[k]}')\n",
        "print('---------------------------------------------')\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token table of origin text\n",
            "---------------------------------------------\n",
            " Code         Token      \n",
            "\n",
            "  0           원내대표는\n",
            "  1           주\n",
            "  2           것\n",
            "  3           이라고\n",
            "  4           며\n",
            "  5           문재인\n",
            "  6           않는\n",
            "  7           문\n",
            "  8           이라며\n",
            "  9           이제\n",
            "  10          풀들이\n",
            "  11          수\n",
            "  12          대통령과\n",
            "  13          공수처는\n",
            "  14          될\n",
            "  15          있는\n",
            "  16          향해\n",
            "  17          보인다\n",
            "  18          짓밟힌\n",
            "  19          국민적\n",
            "  20          저항에\n",
            "  21          직면할\n",
            "  22          정권이\n",
            "  23          위해\n",
            "  24          공수처법을\n",
            "  25          제게\n",
            "  26          대통령은\n",
            "  27          '공수처는\n",
            "  28          없는\n",
            "  29          야당이\n",
            "  30          공수처장\n",
            "  31          사람들이\n",
            "  32          민주당\n",
            "  33          그러면서\n",
            "  34          공수처가\n",
            "  35          청와대와\n",
            "  36          왜\n",
            "  37          그\n",
            "  38          인정하지\n",
            "  39          이\n",
            "  40          공수처를\n",
            "  41          권력\n",
            "  42          게\n",
            "  43          정권의\n",
            "  44          민주당이\n",
            "  45          국회에서\n",
            "  46          그걸\n",
            "  47          권력은\n",
            "  48          먼저\n",
            "  49          주호영\n",
            "  50          국민의힘\n",
            "  51          22일\n",
            "  52          고위공직자범죄수사처\n",
            "  53          공수처\n",
            "  54          법\n",
            "  55          개정과\n",
            "  56          가덕도\n",
            "  57          신공항\n",
            "  58          건설\n",
            "  59          등을\n",
            "  60          밀어붙이고\n",
            "  61          정권과\n",
            "  62          더불어민주당을\n",
            "  63          끝이\n",
            "  64          아우성\n",
            "  65          치는\n",
            "  66          경고했다\n",
            "  67          이날\n",
            "  68          자신의\n",
            "  69          페이스북에\n",
            "  70          공수처법\n",
            "  71          개정을\n",
            "  72          위한\n",
            "  73          '군사작전'에\n",
            "  74          돌입하겠다고\n",
            "  75          엄포를\n",
            "  76          놓고\n",
            "  77          있다\n",
            "  78          정의당을\n",
            "  79          끌어들이기\n",
            "  80          꼼수\n",
            "  81          선거법에\n",
            "  82          묶어\n",
            "  83          '패스트트랙'이라는\n",
            "  84          불법·탈법으로\n",
            "  85          만들어낸\n",
            "  86          시행도\n",
            "  87          해보지\n",
            "  88          않고\n",
            "  89          고치려\n",
            "  90          하는\n",
            "  91          지적했다\n",
            "  92          이어\n",
            "  93          야당\n",
            "  94          원내대표인\n",
            "  95          사람\n",
            "  96          좋아보이는\n",
            "  97          표정으로\n",
            "  98          야당의\n",
            "  99          동의\n",
            "  100         없이는\n",
            "  101         절대\n",
            "  102         출범할\n",
            "  103         것'이라고\n",
            "  104         얘기했고\n",
            "  105         유엔\n",
            "  106         안보리\n",
            "  107         상임이사국처럼\n",
            "  108         임명에\n",
            "  109         '비토권'을\n",
            "  110         행사할\n",
            "  111         있는데\n",
            "  112         무얼\n",
            "  113         걱정하느냐고\n",
            "  114         여당\n",
            "  115         우리를\n",
            "  116         속였다\n",
            "  117         거짓말이라는\n",
            "  118         비난을\n",
            "  119         개의치\n",
            "  120         사람들\n",
            "  121         꼬집었다\n",
            "  122         이해찬\n",
            "  123         전\n",
            "  124         대표가\n",
            "  125         얘기한\n",
            "  126         '민주당\n",
            "  127         20년\n",
            "  128         집권'의\n",
            "  129         토대가\n",
            "  130         올해\n",
            "  131         안에\n",
            "  132         완성된다\n",
            "  133         탈원전과\n",
            "  134         동남권\n",
            "  135         신공항은\n",
            "  136         대통령이\n",
            "  137         대선\n",
            "  138         공약으로\n",
            "  139         내건\n",
            "  140         사업이니\n",
            "  141         여기에\n",
            "  142         불법이\n",
            "  143         있었다고\n",
            "  144         시비를\n",
            "  145         거는\n",
            "  146         것은\n",
            "  147         민주주의를\n",
            "  148         부정하는\n",
            "  149         것이라고\n",
            "  150         청와대\n",
            "  151         출신\n",
            "  152         윤건영\n",
            "  153         의원이\n",
            "  154         윽박지른다\n",
            "  155         '민주주의\n",
            "  156         민주당'이\n",
            "  157         법위에\n",
            "  158         군림하는\n",
            "  159         '반민주'를\n",
            "  160         거리낌없이\n",
            "  161         획책하는\n",
            "  162         언급했다\n",
            "  163         표를\n",
            "  164         얻기\n",
            "  165         나라\n",
            "  166         곳간을\n",
            "  167         다\n",
            "  168         허물어뜨렸고\n",
            "  169         재정\n",
            "  170         운용에서\n",
            "  171         신중함은\n",
            "  172         사라졌다\n",
            "  173         괴물\n",
            "  174         출범하면\n",
            "  175         공무원\n",
            "  176         누구나\n",
            "  177         권력이\n",
            "  178         지시하는\n",
            "  179         범죄행위에\n",
            "  180         거리낌\n",
            "  181         없이\n",
            "  182         가담할\n",
            "  183         것이다\n",
            "  184         권부\n",
            "  185         요직에\n",
            "  186         앉아\n",
            "  187         불법으로\n",
            "  188         각종\n",
            "  189         이권을\n",
            "  190         챙기는\n",
            "  191         권력자들에\n",
            "  192         대한\n",
            "  193         사건이\n",
            "  194         불거져도\n",
            "  195         사건을\n",
            "  196         가져가\n",
            "  197         버리면\n",
            "  198         그만\n",
            "  199         우려했다\n",
            "  200         고위\n",
            "  201         공직자들을\n",
            "  202         처벌하는\n",
            "  203         것인데\n",
            "  204         반대하는지\n",
            "  205         이해할\n",
            "  206         없다'고\n",
            "  207         했는데\n",
            "  208         그런\n",
            "  209         분이\n",
            "  210         대통령\n",
            "  211         주변을\n",
            "  212         감시하는\n",
            "  213         특별감찰관은\n",
            "  214         취임\n",
            "  215         이후\n",
            "  216         지금까지\n",
            "  217         임명하지\n",
            "  218         않았는가\n",
            "  219         라며\n",
            "  220         권력형\n",
            "  221         비리의\n",
            "  222         쓰레기\n",
            "  223         하치장\n",
            "  224         종말\n",
            "  225         처리장이\n",
            "  226         비판했다\n",
            "  227         정부를\n",
            "  228         사도들은\n",
            "  229         법치가\n",
            "  230         미치지\n",
            "  231         무오류의\n",
            "  232         화신이\n",
            "  233         오류를\n",
            "  234         존재가\n",
            "  235         바로\n",
            "  236         신이며\n",
            "  237         아래에는\n",
            "  238         자신들의\n",
            "  239         지도자를\n",
            "  240         목숨바쳐\n",
            "  241         지킴으로서\n",
            "  242         정의를\n",
            "  243         실현하겠다는\n",
            "  244         추종자들로\n",
            "  245         넘쳐\n",
            "  246         난다\n",
            "  247         지도자의\n",
            "  248         신성을\n",
            "  249         세력을\n",
            "  250         정죄하는\n",
            "  251         수단으로\n",
            "  252         전락할\n",
            "  253         질타했다\n",
            "  254         저도\n",
            "  255         법조인이지만\n",
            "  256         공수처장이\n",
            "  257         마음대로\n",
            "  258         검사들과\n",
            "  259         수사관들을\n",
            "  260         임명하는\n",
            "  261         끔찍한\n",
            "  262         사법기구가\n",
            "  263         어떤\n",
            "  264         일을\n",
            "  265         할지\n",
            "  266         두렵기만\n",
            "  267         하다\n",
            "  268         검찰과\n",
            "  269         경찰\n",
            "  270         위에\n",
            "  271         사법기구로\n",
            "  272         헌법과\n",
            "  273         법으로\n",
            "  274         독립성을\n",
            "  275         보장하는\n",
            "  276         검찰총장을\n",
            "  277         이렇게\n",
            "  278         핍박하는\n",
            "  279         어떻게\n",
            "  280         운영할지\n",
            "  281         불을\n",
            "  282         보듯\n",
            "  283         뻔한\n",
            "  284         일\n",
            "  285         예측했다\n",
            "  286         추미애\n",
            "  287         법무장관을\n",
            "  288         앞장\n",
            "  289         세워\n",
            "  290         윤석열\n",
            "  291         검찰의\n",
            "  292         비리\n",
            "  293         수사를\n",
            "  294         저지하려다가\n",
            "  295         난관에\n",
            "  296         봉착하자\n",
            "  297         무슨\n",
            "  298         수를\n",
            "  299         써서라도\n",
            "  300         출범시키려\n",
            "  301         한다\n",
            "  302         자리에는\n",
            "  303         추미애보다\n",
            "  304         더\n",
            "  305         한\n",
            "  306         막무가내\n",
            "  307         내\n",
            "  308         편을\n",
            "  309         앉힐\n",
            "  310         분명한\n",
            "  311         파렴치와\n",
            "  312         오만함을\n",
            "  313         최전선에서\n",
            "  314         온\n",
            "  315         몸으로\n",
            "  316         겪어온\n",
            "  317         저로서는\n",
            "  318         내일부터\n",
            "  319         보일\n",
            "  320         행태가\n",
            "  321         환히\n",
            "  322         180석의\n",
            "  323         또\n",
            "  324         군사작전을\n",
            "  325         개시하면\n",
            "  326         누가\n",
            "  327         막겠는가\n",
            "  328         라고\n",
            "  329         성토했다\n",
            "  330         막을\n",
            "  331         힘이\n",
            "  332         우리\n",
            "  333         야당에게는\n",
            "  334         없다\n",
            "  335         삭발하고\n",
            "  336         장외투쟁해\n",
            "  337         봐야\n",
            "  338         눈\n",
            "  339         하나\n",
            "  340         깜짝할\n",
            "  341         아닌\n",
            "  342         대란대치\n",
            "  343         大亂大治\n",
            "  344         세상을\n",
            "  345         온통\n",
            "  346         혼돈\n",
            "  347         속으로\n",
            "  348         밀어넣고\n",
            "  349         유지에\n",
            "  350         이용한다는\n",
            "  351         통치기술\n",
            "  352         규탄했다\n",
            "  353         아울러\n",
            "  354         바람\n",
            "  355         국민은\n",
            "  356         풀이다\n",
            "  357         바람이\n",
            "  358         불면\n",
            "  359         청보리\n",
            "  360         밭의\n",
            "  361         보리가\n",
            "  362         눕는다\n",
            "  363         다시는\n",
            "  364         일어서지\n",
            "  365         못하도록\n",
            "  366         풀을\n",
            "  367         짓밟지만\n",
            "  368         풀들은\n",
            "  369         다시\n",
            "  370         일어난다\n",
            "  371         시인\n",
            "  372         김수영은\n",
            "  373         '바람보다\n",
            "  374         눕지만\n",
            "  375         바람보다\n",
            "  376         일어나는'\n",
            "  377         민초의\n",
            "  378         힘을\n",
            "  379         노래했다\n",
            "  380         고\n",
            "  381         말했다\n",
            "  382         마지막으로\n",
            "  383         정권은\n",
            "  384         곧\n",
            "  385         광장에서\n",
            "  386         일어서서\n",
            "  387         아우성치는\n",
            "  388         모습을\n",
            "  389         지켜보게\n",
            "  390         대란대치를\n",
            "  391         끝장내려는\n",
            "  392         거듭\n",
            "  393         강조했다\n",
            "  394         @@@\n",
            "  395         @@@\n",
            "  396         @@@\n",
            "  397         @@@\n",
            "  398         @@@\n",
            "  399         @@@\n",
            "  400         @@@\n",
            "  401         @@@\n",
            "  402         @@@\n",
            "  403         @@@\n",
            "  404         @@@\n",
            "  405         @@@\n",
            "  406         @@@\n",
            "  407         @@@\n",
            "  408         @@@\n",
            "  409         @@@\n",
            "  410         @@@\n",
            "  411         @@@\n",
            "  412         @@@\n",
            "  413         @@@\n",
            "  414         @@@\n",
            "  415         @@@\n",
            "  416         @@@\n",
            "  417         @@@\n",
            "  418         @@@\n",
            "  419         @@@\n",
            "  420         @@@\n",
            "  421         @@@\n",
            "  422         @@@\n",
            "  423         @@@\n",
            "  424         @@@\n",
            "  425         @@@\n",
            "  426         @@@\n",
            "  427         @@@\n",
            "  428         @@@\n",
            "  429         @@@\n",
            "  430         @@@\n",
            "  431         @@@\n",
            "  432         @@@\n",
            "  433         @@@\n",
            "  434         @@@\n",
            "  435         @@@\n",
            "  436         @@@\n",
            "  437         @@@\n",
            "  438         @@@\n",
            "  439         @@@\n",
            "  440         @@@\n",
            "  441         @@@\n",
            "  442         @@@\n",
            "  443         @@@\n",
            "  444         @@@\n",
            "  445         @@@\n",
            "  446         @@@\n",
            "  447         @@@\n",
            "  448         @@@\n",
            "  449         @@@\n",
            "  450         @@@\n",
            "  451         @@@\n",
            "  452         @@@\n",
            "  453         @@@\n",
            "  454         @@@\n",
            "  455         @@@\n",
            "  456         @@@\n",
            "  457         @@@\n",
            "  458         @@@\n",
            "  459         @@@\n",
            "  460         @@@\n",
            "  461         @@@\n",
            "  462         @@@\n",
            "  463         @@@\n",
            "  464         @@@\n",
            "  465         @@@\n",
            "  466         @@@\n",
            "  467         @@@\n",
            "  468         @@@\n",
            "  469         @@@\n",
            "  470         @@@\n",
            "  471         @@@\n",
            "  472         @@@\n",
            "  473         @@@\n",
            "  474         @@@\n",
            "  475         @@@\n",
            "  476         @@@\n",
            "  477         @@@\n",
            "  478         @@@\n",
            "  479         @@@\n",
            "  480         @@@\n",
            "  481         @@@\n",
            "  482         @@@\n",
            "  483         @@@\n",
            "  484         @@@\n",
            "  485         @@@\n",
            "  486         @@@\n",
            "  487         @@@\n",
            "  488         @@@\n",
            "  489         @@@\n",
            "  490         @@@\n",
            "  491         @@@\n",
            "  492         @@@\n",
            "  493         @@@\n",
            "  494         @@@\n",
            "  495         @@@\n",
            "  496         @@@\n",
            "  497         @@@\n",
            "  498         @@@\n",
            "  499         @@@\n",
            "  500         @@@\n",
            "  501         @@@\n",
            "  502         @@@\n",
            "  503         @@@\n",
            "  504         @@@\n",
            "  505         @@@\n",
            "  506         @@@\n",
            "  507         @@@\n",
            "  508         @@@\n",
            "  509         @@@\n",
            "  510         @@@\n",
            "  511         @@@\n",
            "---------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "bZgNu7SYeHyP"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (Input,\n",
        "                                     Dense, \n",
        "                                     BatchNormalization, \n",
        "                                     LeakyReLU,\n",
        "                                     Softmax,\n",
        "                                     Reshape, \n",
        "                                     Conv2DTranspose,\n",
        "                                     Conv2D,\n",
        "                                     Dropout,\n",
        "                                     Flatten,\n",
        "                                     Concatenate,\n",
        "                                     Lambda)\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6Vj9qaaeHyQ"
      },
      "source": [
        "noise를 token_table을 통해 text로 변환하는 변환기 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "SI2OgqhqeHyR"
      },
      "source": [
        "# noise 생성\n",
        "w = tf.random.normal([2,_MAX_LENGTH,_MAX_TOKEN])"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "72FRz_bDeHyR"
      },
      "source": [
        "import sys\n",
        "\n",
        "def to_text(w):\n",
        "\n",
        "    #tf.print(w)\n",
        "    texts = []\n",
        "    try:\n",
        "        for z in w:\n",
        "            text = \"\"\n",
        "            #code = []\n",
        "            for v in z:\n",
        "                try:\n",
        "                    text += word_table[tf.argmax(v,output_type=tf.int32).numpy()] + ' '\n",
        "                except Exception as ex:\n",
        "                    tf.print(ex,sys.exc_info())\n",
        "                    text += '[   ] '                  \n",
        "            texts.append(text)\n",
        "\n",
        "    except Exception as ex:\n",
        "        tf.print(ex,sys.exc_info())\n",
        "\n",
        "    return tf.constant(texts,dtype=tf.string)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywMSg5noeHyS",
        "outputId": "a08a244b-497a-4b6e-b1b5-5906f7ce7753"
      },
      "source": [
        "# to_text 함수의 test\n",
        "e = to_text(w)\n",
        "for t in e:\n",
        "  print(t.numpy().decode('utf-8'))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "각종 몸으로 행사할 법 정권과 @@@ 처벌하는 묶어 이라고 누구나 @@@ @@@ 제게 사라졌다 문재인 임명하는 분명한 있었다고 속으로 @@@ 할지 대표가 삭발하고 지켜보게 @@@ 세워 다시 개정을 아래에는 출범시키려 @@@ 지시하는 고치려 표정으로 임명하는 원내대표인 사람 추미애 불법으로 화신이 \n",
            "않고 권력 위해 깜짝할 것인데 얻기 편을 정권과 없는 김수영은 @@@ @@@ @@@ @@@ 불을 모습을 일어서서 공약으로 @@@ @@@ @@@ 세상을 @@@ 만들어낸 끔찍한 국민은 @@@ 사건을 대한 사람 @@@ 풀들이 @@@ @@@ @@@ 각종 '비토권'을 장외투쟁해 의원이 @@@ \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGwa9iaPeHyT"
      },
      "source": [
        "생성된 text의 embedding 변환기 구현<br>\n",
        "embedding은 org_text의 embedding과 비교하여 원문과 유사하게 민들기 위한 목적"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "_D7qcRmQeHyX"
      },
      "source": [
        "# 이거 이틀걸림...잘 몰라서 ㅈㄴ 헤맴\n",
        "\n",
        "@tf.custom_gradient\n",
        "def to_embedding(w):\n",
        "\n",
        "    def grad(dy):\n",
        "        dy = tf.nn.batch_normalization(dy, 0, tf.math.reduce_std(dy), 0, 10, 1e-3)  # gradient 증폭 \n",
        "        dy_arr = tf.reshape(dy,(dy.shape[0],1024,1))\n",
        "        #tf.print(dy_arr)\n",
        "        dy_arr_st = tf.image.resize(dy_arr, size=(dy.shape[0],_MAX_LENGTH*_MAX_TOKEN),method=tf.image.ResizeMethod.BILINEAR)\n",
        "        dy_arr_st = tf.reshape(dy_arr_st,shape=(dy.shape[0],_MAX_LENGTH,_MAX_TOKEN))\n",
        "        return dy_arr_st\n",
        "\n",
        "    #print(w)    \n",
        "    texts = []\n",
        "    value = None\n",
        "    try:\n",
        "        for z in w:\n",
        "            text = \"\"\n",
        "            for v in z:\n",
        "                try:\n",
        "                    text += word_table[tf.argmax(v,output_type=tf.int32).numpy()] + ' '\n",
        "                except Exception as ex:\n",
        "                    tf.print(ex,sys.exc_info())\n",
        "                    text += '[   ] '                  \n",
        "            texts.append(text)\n",
        "        value = tf.constant(embedder.encode(texts,show_progress_bar=False),dtype=tf.float32)\n",
        "    except Exception as ex:\n",
        "        tf.print(ex,sys.exc_info())\n",
        "\n",
        "    return value, grad"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Pu18GvweHyY",
        "outputId": "90d28ab3-5d23-41e9-ed12-bb1b5be5a3e6"
      },
      "source": [
        "# to_embedding 함수의 test\n",
        "e = to_embedding(w)\n",
        "for t in e:\n",
        "  print(t.numpy())"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.4220771  -0.58893     0.58104146 ... -0.64870745  0.36622205\n",
            "  0.43991843]\n",
            "[ 0.5899811  -0.31003985  0.5478295  ... -0.8206855   0.07522152\n",
            " -0.2602026 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "DWTe53wueHya"
      },
      "source": [
        "# to_compression_ratio\n",
        "\n",
        "@tf.custom_gradient\n",
        "def to_compression_ratio(w):\n",
        "    def grad(dy):\n",
        "        dy = tf.nn.batch_normalization(dy, 0, tf.math.reduce_std(dy), 0, 10, 1e-3)  # gradient 증폭, 이래도 되는 건지....\n",
        "        dy_arr = tf.reshape(dy,(dy.shape[0],1,1))\n",
        "        #tf.print(dy_arr)\n",
        "        dy_arr_st = tf.image.resize(dy_arr, size=(dy.shape[0],_MAX_LENGTH*_MAX_TOKEN),method=tf.image.ResizeMethod.BILINEAR)\n",
        "        dy_arr_st = tf.reshape(dy_arr_st,shape=(dy.shape[0],_MAX_LENGTH,_MAX_TOKEN))        \n",
        "        return dy_arr_st\n",
        "    \n",
        "    const = 131328 # 512를 순서대로 다 더한값\n",
        "    value = None\n",
        "    compression_ratio = []\n",
        "    try:\n",
        "        for z in w:\n",
        "            cr = 0\n",
        "            for v in z:\n",
        "                try:\n",
        "                    key = tf.argmax(v,output_type=tf.int32) #tf.constant(,dtype=tf.int32)\n",
        "                    cr += key.numpy()\n",
        "                except Exception as ex:\n",
        "                    tf.print(ex,sys.exc_info())\n",
        "                    cr += 0                \n",
        "            compression_ratio.append(cr/const)\n",
        "        value = tf.constant(compression_ratio,dtype=tf.float32)\n",
        "    except Exception as ex:\n",
        "        tf.print(ex,sys.exc_info())\n",
        "\n",
        "    return value, grad\n",
        "    "
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rk25esbweHyd",
        "outputId": "a737b732-8f3c-4bbd-80b9-2ad85392278b"
      },
      "source": [
        "# to_compression_ratio 함수의 test\n",
        "e = to_compression_ratio(w)\n",
        "for t in e:\n",
        "  print(t.numpy())"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0688962\n",
            "0.08656189\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxlFMo3GeHyg"
      },
      "source": [
        "생성된 text의 morpheme code 변환기 구현<br>\n",
        "morpheme code는 한국어 문장들(dataset)의 morpheme code와 비교하여 한국어 문법에 가깝게 만들기 위한 목적"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "yhOeXv93eHyh"
      },
      "source": [
        "\n",
        "@tf.custom_gradient\n",
        "def to_morpcoding(w):\n",
        "\n",
        "    def grad(dy):\n",
        "        dy = tf.nn.batch_normalization(dy, 0, tf.math.reduce_std(dy), 0, 10, 1e-3)  # gradient 증폭, 이래도 되는 건지....\n",
        "        dy_arr = tf.reshape(dy,(dy.shape[0],_MAX_MORP_LENGTH,1))\n",
        "        #tf.print(dy_arr)\n",
        "        dy_arr_st = tf.image.resize(dy_arr, size=(dy.shape[0],_MAX_LENGTH*_MAX_TOKEN),method=tf.image.ResizeMethod.BILINEAR)\n",
        "        dy_arr_st = tf.reshape(dy_arr_st,shape=(dy.shape[0],_MAX_LENGTH,_MAX_TOKEN))\n",
        "        return dy_arr_st\n",
        "\n",
        "    #print(w)    \n",
        "    texts = []\n",
        "    codes = []\n",
        "    value = None\n",
        "    try:\n",
        "        for z in w:\n",
        "            text = \"\"\n",
        "            for v in z:\n",
        "                try:\n",
        "                    text += word_table[tf.argmax(v,output_type=tf.int32).numpy()] + ' '\n",
        "                except Exception as ex:\n",
        "                    tf.print(ex,sys.exc_info())\n",
        "                    text += '[   ] '                  \n",
        "            texts.append(text)\n",
        "            \n",
        "        for sentence in texts:\n",
        "            code = morpheme_encode(sentence)\n",
        "            if len(code) <= _MAX_MORP_LENGTH:\n",
        "                codes.append(code + [_PADDING_CODE for i in range(_MAX_MORP_LENGTH-len(code))])\n",
        "            else:\n",
        "                codes.append(code[:_MAX_MORP_LENGTH])\n",
        "        value = tf.constant(codes,dtype=tf.int32)\n",
        "    except Exception as ex:\n",
        "        tf.print(ex,sys.exc_info())\n",
        "\n",
        "    return value, grad"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-C_pxE5eHyi",
        "outputId": "823880d0-07c1-4b15-e07e-62c9f5ab7e2b"
      },
      "source": [
        "# to_morpcoding 함수의 test\n",
        "e = to_morpcoding(w)\n",
        "for t in e:\n",
        "  print(t.numpy())"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2 2 5 2 6 2 2 5 1 2 6 6 5 2 5 1 1 2 5 6 2 2 6 8 8 2 5 1 6 2 5 2 5 6 1 6 2\n",
            " 2 5 2 5 2 6 1 2 6 6 2 5 2 6 2 5 2 2 2 5 2 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[6 2 2 2 6 2 5 6 2 5 2 5 8 2 5 1 1 1 1 2 5 2 5 6 2 5 1 1 1 2 5 1 6 6 8 2 5\n",
            " 1 2 5 2 2 1 2 9 5 1 1 1 2 3 2 9 3 5 2 2 2 2 5 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLtq1YP_eHyj"
      },
      "source": [
        "Network 구성을 위해 사용자 정의 Layer 를 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "F9EfbbSreHyk"
      },
      "source": [
        "# 이것도 잘 몰라서 하루 걸림... ㅜㅜ\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.layers import Layer\n",
        "\n",
        "#tf.executing_eagerly()\n",
        "\n",
        "class Post_processing(Layer):\n",
        "\n",
        "    def __init__(self, output_dim, encoder_func=None,Tout=tf.float32, **kwargs):\n",
        "        self.output_dim = output_dim\n",
        "        self.encoder = encoder_func\n",
        "        self.Tout = Tout\n",
        "        super(Post_processing, self).__init__(**kwargs)\n",
        "    '''\n",
        "    def build(self, input_shape):\n",
        "        tf.print('build',input_shape)\n",
        "        # 이 레이어에 대해 학습가능한 가중치 변수를 만듭니다.\n",
        "        self.kernel = self.add_weight(name='kernel', \n",
        "                                      shape=(input_shape[1], self.output_dim),\n",
        "                                      initializer='uniform',\n",
        "                                      trainable=True)\n",
        "        super(Post_processing, self).build(input_shape)  # 끝에서 꼭 이 함수를 호출하십시오\n",
        "    '''\n",
        "    def call(self, input_data):\n",
        "        #tf.print('Post_processing : call input_data',input_data.shape)\n",
        "        value = tf.py_function(self.encoder,[input_data],Tout=self.Tout,name='encode_func')\n",
        "        #print('value.shape:',value.shape)\n",
        "        #value.set_shape((input_data.shape[0],self.output_dim))\n",
        "        if self.output_dim > 0:\n",
        "            value.set_shape((input_data.shape[0],self.output_dim))\n",
        "        else:\n",
        "            value.set_shape((input_data.shape[0],))\n",
        "        #return tf.reshape(value,[input_data.shape[0]])  \n",
        "\n",
        "        #value = tf.Variable((tf.zeros([input_data.shape[0],1024]) if self.Tout==tf.float32 else tf.zeros([input_data.shape[0],])),dtype=self.Tout,shape=( (input_data.shape[0],1024) if self.Tout==tf.float32 else (input_data.shape[0],)))\n",
        "        #tf.py_function(self.encoder,[input_data],Tout=self.Tout)\n",
        "        return value\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        tf.print('compute_output_shape:',input_shape)\n",
        "        input_shape = tensor_shape.TensorShape(input_shape).as_list()\n",
        "        if self.output_dim > 0:\n",
        "            return tensor_shape.TensorShape([input_shape[0], self.output_dim])\n",
        "        return tensor_shape.TensorShape([input_shape[0]])"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iT--4r5BeHyk",
        "outputId": "6316c435-1e08-4a3d-d19c-39719f19f527"
      },
      "source": [
        "# 구성한 Layer의 test\n",
        "e = Post_processing(1024,to_embedding,Tout=tf.float32)(w)\n",
        "for c in e:\n",
        "  print(c)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[ 0.4220771  -0.58893     0.58104146 ... -0.64870745  0.36622205\n",
            "  0.43991843], shape=(1024,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[ 0.5899811  -0.31003985  0.5478295  ... -0.8206855   0.07522152\n",
            " -0.2602026 ], shape=(1024,), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "X0RIQRZ4eHyl"
      },
      "source": [
        "# 별로 중요하지는 않지만 Lambda layer를 활용하기 위한 assert 함수 구성\n",
        "def assert_layer(input_data,out_dim=None):\n",
        "    #tf.print(input_data)\n",
        "    #print(input_data)\n",
        "    assert input_data.shape[1] == out_dim\n",
        "    return input_data"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJazT7y-eHyl",
        "outputId": "8daec25a-a6b0-461e-8818-1f948e16bcae"
      },
      "source": [
        "# 드디어 generator 구현\n",
        "# 효과적으로 구성된 것인지는 모르겠음... 이것은 아직 많은 연구가 필요함.\n",
        "# 또한 LSTM으로 바꾸어 길이의 한게를 극복해야 할 것...\n",
        "\n",
        "def make_generator_model(max_length,total_words):\n",
        "    input = Input(shape=(_NOISE_DIM,), dtype='float32') \n",
        "    x1 = Dense(1024*2, use_bias=False)(input)\n",
        "    #x1 = BatchNormalization()(x1)\n",
        "    x1 = LeakyReLU()(x1)\n",
        "    \n",
        "    x1 = Dense(1024*4, use_bias=False)(x1)\n",
        "    #x1 = BatchNormalization()(x1)\n",
        "    x1 = LeakyReLU()(x1)\n",
        "    \n",
        "    #x1 = Dense(max_length*total_words, use_bias=False, activation='tanh')(x1)\n",
        "    x1 = Dense(max_length*total_words, use_bias=False)(x1)\n",
        "    x1 = Lambda(assert_layer,arguments={'out_dim':max_length*total_words})(x1)\n",
        "    x1 = Reshape((max_length, total_words))(x1)\n",
        "    #x1 = BatchNormalization()(x1)\n",
        "    #x1 = Softmax()(x1)        \n",
        "    #x1 = MyCustomLayer(max_length*total_words)(x1)\n",
        "    txt = Post_processing(0,to_text,Tout=tf.string)(x1)\n",
        "    emb = Post_processing(1024,to_embedding,Tout=tf.float32)(x1)\n",
        "    cmr = Post_processing(0,to_compression_ratio,Tout=tf.float32)(x1)\n",
        "    cod = Post_processing(128,to_morpcoding,Tout=tf.int32)(x1)\n",
        "    \n",
        "    model = Model(input,[txt,emb,cmr,cod])\n",
        "    \n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "generator = make_generator_model(_MAX_LENGTH,_MAX_TOKEN)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 1000)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 2048)         2048000     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)         (None, 2048)         0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 4096)         8388608     leaky_re_lu[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)       (None, 4096)         0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 20480)        83886080    leaky_re_lu_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 20480)        0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, 40, 512)      0           lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "post_processing_1 (Post_process (None,)              0           reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "post_processing_2 (Post_process (None, 1024)         0           reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "post_processing_3 (Post_process (None,)              0           reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "post_processing_4 (Post_process (None, 128)          0           reshape[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 94,322,688\n",
            "Trainable params: 94,322,688\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQya4kMheHym",
        "outputId": "262e43d0-472c-4cff-d8cc-640c02c5e015"
      },
      "source": [
        "# generator의 test\n",
        "# Create a random noise and generate a sample\n",
        "noise = tf.random.normal([3,_NOISE_DIM])\n",
        "texts,embeddings,compratios,morpcodes = generator(noise, training=True)\n",
        "print(texts.shape)\n",
        "for i,txt in zip(range(len(texts)),texts):\n",
        "    print(f\" {i+1}> {txt.numpy().decode('utf-8')}\" )\n",
        "print(embeddings.shape)\n",
        "print(compratios.shape)\n",
        "print(morpcodes.shape)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3,)\n",
            " 1> 해보지 @@@ 이라며 @@@ @@@ 사건을 정죄하는 대한 시행도 이 넘쳐 올해 우려했다 말했다 일어나는' @@@ 이어 야당이 넘쳐 사라졌다 더불어민주당을 개정을 @@@ 완성된다 여기에 불면 법 @@@ 불을 군사작전을 윽박지른다 눕지만 @@@ @@@ 법위에 공수처 탈원전과 얘기한 @@@ 신성을 \n",
            " 2> 광장에서 @@@ 거듭 @@@ 신이며 그걸 임명하는 권부 엄포를 언급했다 @@@ 정권은 라며 것이라고 수사를 22일 한 검사들과 불면 검사들과 내건 깜짝할 정권과 @@@ 우리를 가져가 사법기구로 @@@ 오만함을 @@@ @@@ @@@ 시행도 임명하는 바로 풀들은 눈 건설 내 @@@ \n",
            " 3> @@@ @@@ @@@ 거리낌 막을 @@@ @@@ 나라 그 @@@ 깜짝할 @@@ 불법·탈법으로 라고 @@@ @@@ 대란대치 수사를 않고 행사할 검찰총장을 신이며 마음대로 불거져도 우리를 거리낌 엄포를 이용한다는 공약으로 오류를 미치지 시행도 위해 헌법과 자신의 @@@ 끝장내려는 @@@ 청보리 눈 \n",
            "(3, 1024)\n",
            "(3,)\n",
            "(3, 128)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41aWytGveHyo"
      },
      "source": [
        "# Discriminator 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtjQGj54eHyo"
      },
      "source": [
        "먼저 요약을 구분하기 위한 discriminator_summ 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Fx3GgEXEeHyo"
      },
      "source": [
        "# 문장에 대한 embeddings를 이용하여 org_text_emb (org_text의 embedding)과의 유사도를 계산한다.\n",
        "import scipy\n",
        "\n",
        "@tf.custom_gradient\n",
        "def to_similarity(w):\n",
        "\n",
        "    def grad(dy):\n",
        "        dy = tf.nn.batch_normalization(dy, 0, tf.math.reduce_std(dy), 0, 10, 1e-3)  # gradient 증폭, 이래도 되는 건지....\n",
        "        dy_arr = tf.reshape(dy,(dy.shape[0],1,1))\n",
        "        #tf.print(dy_arr)\n",
        "        dy_arr_st = tf.image.resize(dy_arr, size=(dy.shape[0],1024),method=tf.image.ResizeMethod.BILINEAR)\n",
        "        dy_arr_st = tf.reshape(dy_arr_st,shape=(dy.shape[0],1024))\n",
        "        return dy_arr_st\n",
        "\n",
        "    similarities = []\n",
        "    value = None\n",
        "    try:\n",
        "        for embedding in w:\n",
        "            distances = scipy.spatial.distance.cdist([embedding], [org_text_emb], \"cosine\")[0]\n",
        "            similarities.append(distances[0])\n",
        "            \n",
        "        value = tf.constant(similarities,dtype=tf.float32)\n",
        "    except Exception as ex:\n",
        "        tf.print(ex,sys.exc_info())\n",
        "\n",
        "    return value, grad\n"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNIQNvIveHyp",
        "outputId": "084fddba-1193-4bca-c170-7d42b69a0acb"
      },
      "source": [
        "def make_discriminator_model():\n",
        "    input_emb = Input(shape=(1024,), dtype='float32') \n",
        "    x1 = Post_processing(0,to_similarity,Tout=tf.float32)(input_emb)\n",
        "    x1 = Reshape((1,))(x1)    \n",
        "    input_cmp = Input(shape=(), dtype='float32') \n",
        "    imput_mrp = Input(shape=(_MAX_MORP_LENGTH,), dtype='int32')\n",
        "\n",
        "    x2 = Reshape((1,))(input_cmp)\n",
        "    x3 = Dense(256)(imput_mrp)\n",
        "    #x3 = BatchNormalization()(x3)\n",
        "    x3 = LeakyReLU()(x3)\n",
        "\n",
        "    x3 = Dense(256*3)(x3)\n",
        "    #x3 = BatchNormalization()(x3)\n",
        "    x3 = LeakyReLU()(x3)\n",
        "\n",
        "    concatted = Concatenate(axis=1)([x1, x2, x3])\n",
        "    x4 = Flatten()(concatted)\n",
        "    x4 = Dense(64, use_bias=False)(x4)\n",
        "    #x4 = BatchNormalization()(x4)\n",
        "    x4 = LeakyReLU()(x4)\n",
        "    x4 = Dense(1)(x4)\n",
        "    \n",
        "    model = Model([input_emb,input_cmp,imput_mrp],x4)\n",
        "    \n",
        "    model.summary()\n",
        "    \n",
        "    return model\n",
        "\n",
        "discriminator = make_discriminator_model()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 256)          33024       input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 1024)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)       (None, 256)          0           dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "post_processing_5 (Post_process (None,)              0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None,)]            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 768)          197376      leaky_re_lu_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 1)            0           post_processing_5[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "reshape_2 (Reshape)             (None, 1)            0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)       (None, 768)          0           dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 770)          0           reshape_1[0][0]                  \n",
            "                                                                 reshape_2[0][0]                  \n",
            "                                                                 leaky_re_lu_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 770)          0           concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 64)           49280       flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)       (None, 64)           0           dense_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 1)            65          leaky_re_lu_4[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 279,745\n",
            "Trainable params: 279,745\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7owqbg4eHyp",
        "outputId": "0e58341c-2ab1-4af7-a566-c29ed3e30a2d"
      },
      "source": [
        "# discriminator test\n",
        "\n",
        "predict = discriminator([embeddings,compratios,morpcodes])\n",
        "print(predict)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[-0.7977773]\n",
            " [-0.491852 ]\n",
            " [-0.4843918]], shape=(3, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmvyPAVSeHyq"
      },
      "source": [
        "# GAN 을 이용한 loss 의 gradient 구현 --> 빡심"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ECEMH-yweHyr"
      },
      "source": [
        "# This method returns a helper function to compute cross entropy loss\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "cvBC5yS6eHys"
      },
      "source": [
        "# 디짐\n",
        "@tf.function\n",
        "def train_step(real_embedding,real_morpcoding):\n",
        "  \n",
        "    # 1 - Create a random noise to feed it into the model\n",
        "    # for the text generation\n",
        "    noise = tf.random.normal([BATCH_SIZE, _NOISE_DIM])\n",
        "    \n",
        "    # 2 - Generate text and calculate loss values\n",
        "    # GradientTape method records operations for automatic differentiation.\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        texts,embeddings,compratios,morpcodes = generator(noise, training=True)\n",
        "        #embeddings = generator(noise, training=True)\n",
        "        # real에 가까우려면 discriminator의 학습은 real_embedding 이 zero (0)에 가깝게 학습시켜야 함.\n",
        "        # 하지만 압축율의 개념으로는 본래는 ones (1)가 맞음.\n",
        "        real_output = discriminator([real_embedding,np.zeros(len(real_embedding)),real_morpcoding], training=True)\n",
        "        fake_output = discriminator([embeddings,compratios,morpcodes], training=True)\n",
        "\n",
        "        #tf.print('train_step : embeddings.shape=',embeddings.shape)\n",
        "        gen_loss = generator_loss(fake_output)\n",
        "        #tf.print('train_step : gen_loss=',gen_loss)\n",
        "        disc_loss = discriminator_loss(real_output, fake_output)\n",
        "        #tf.print('train_step : disc_loss=',disc_loss)\n",
        "\n",
        "    # 3 - Calculate gradients using loss values and model variables\n",
        "    # \"gradient\" method computes the gradient using \n",
        "    # operations recorded in context of this tape (gen_tape and disc_tape).\n",
        "    \n",
        "    # It accepts a target (e.g., gen_loss) variable and \n",
        "    # a source variable (e.g.,generator.trainable_variables)\n",
        "    # target --> a list or nested structure of Tensors or Variables to be differentiated.\n",
        "    # source --> a list or nested structure of Tensors or Variables.\n",
        "    # target will be differentiated against elements in sources.\n",
        "\n",
        "    # \"gradient\" method returns a list or nested structure of Tensors  \n",
        "    # (or IndexedSlices, or None), one for each element in sources. \n",
        "    # Returned structure is the same as the structure of sources.\n",
        "    \n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, \n",
        "                                                discriminator.trainable_variables)\n",
        "    #tf.print('train_step : gradients_of_discriminator=',gradients_of_discriminator)   \n",
        "    noise = tf.random.normal([BATCH_SIZE, _NOISE_DIM])\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        texts,embeddings,compratios,morpcodes = generator(noise, training=True)\n",
        "        #embeddings = generator(noise, training=True)\n",
        "        #real_output = discriminator(real_embedding, training=True)\n",
        "        fake_output = discriminator([embeddings,compratios,morpcodes], training=True)\n",
        "\n",
        "        #tf.print('train_step : embeddings.shape=',embeddings.shape)\n",
        "        gen_loss = generator_loss(fake_output)\n",
        "        #tf.print('train_step : gen_loss=',gen_loss)\n",
        "        #disc_loss = discriminator_loss(real_output, fake_output)\n",
        "        #tf.print('train_step : disc_loss=',disc_loss)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, \n",
        "                                               generator.trainable_variables)\n",
        "    #tf.print('train_step : gradients_of_generator=',gradients_of_generator)\n",
        " \n",
        "\n",
        "    # 4 - Process  Gradients and Run the Optimizer\n",
        "    # \"apply_gradients\" method processes aggregated gradients. \n",
        "    # ex: optimizer.apply_gradients(zip(grads, vars))\n",
        "    \"\"\"\n",
        "    Example use of apply_gradients:\n",
        "    grads = tape.gradient(loss, vars)\n",
        "    grads = tf.distribute.get_replica_context().all_reduce('sum', grads)\n",
        "    # Processing aggregated gradients.\n",
        "    optimizer.apply_gradients(zip(grads, vars), experimental_aggregate_gradients=False)\n",
        "    \"\"\"\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "    #tf.print('train_step : after discriminator_optimizer')    "
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "cJs3lBpJeHys"
      },
      "source": [
        "EPOCHS = 5\n",
        "# 요약문 생성의 확인을 위해 10개의 문장을 생성하고 train과정에서 각 epoch마다 변화를 확인한다.\n",
        "seed = tf.random.normal([10, _NOISE_DIM])"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "os3b0psFeHyt",
        "outputId": "e756080b-9cbd-4d03-ed82-513372923666"
      },
      "source": [
        "# 생성된 문장의 원문 유사도를 측정하기 위한 함수\n",
        "\n",
        "import scipy\n",
        "#print(doc_emb)\n",
        "def similarity_score(queries,org_embedding):\n",
        "\n",
        "    total_score = 0\n",
        "    query_embeddings = embedder.encode(queries,show_progress_bar=False)\n",
        "    for query, query_embedding in zip(queries, query_embeddings):\n",
        "        distances = scipy.spatial.distance.cdist([query_embedding], [org_embedding], \"cosine\")[0]\n",
        "        results = zip(range(len(distances)), distances)\n",
        "        for idx, distance in results:\n",
        "            total_score += 1-distance\n",
        "    return total_score\n",
        "\n",
        "queries = []\n",
        "texts,embeddings,compratios,morpcodes = generator(seed,training=False)\n",
        "#count = 0\n",
        "for t in texts:\n",
        "    summary_text = t.numpy().decode('utf-8')\n",
        "    queries.append(summary_text)\n",
        "print('Similarity score:',str(similarity_score(queries,org_text_emb)))"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Similarity score: 6.6356144572977\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "cXrsOXX4eHyt"
      },
      "source": [
        "# Print iterations progress\n",
        "class ProgressBar:\n",
        "    # pb = ProgressBar(total=20, prefix = 'Epoch 1')\n",
        "    def __init__(self,total=20, prefix = '', suffix = '', decimals = 1, length = 20, fill = '█', printEnd = \"\\r\"):\n",
        "        self.total = total\n",
        "        self.prefix = prefix\n",
        "        self.suffix = suffix\n",
        "        self.decimals = decimals\n",
        "        self.length = length\n",
        "        self.fill = fill\n",
        "        self.printEnd = printEnd\n",
        "        self.ite = 0\n",
        "    # pb.printProgress(1,'~~~~')\n",
        "    def printProgress(self,iteration, text):\n",
        "        self.ite += iteration\n",
        "        percent = (\"{0:.\" + str(self.decimals) + \"f}\").format(100 * (self.ite / float(self.total)))\n",
        "\n",
        "        filledLength = int(self.length * self.ite // self.total)\n",
        "        bar = self.fill * filledLength + '-' * (self.length - filledLength)\n",
        "        print(f'\\r{self.prefix} |{bar}| {percent}% {self.suffix}  {text}', end=\"\", flush=True)\n",
        "        # Print New Line on Complete\n",
        "        if self.ite == self.total: \n",
        "            print()"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "dhqdaIATeHyu"
      },
      "source": [
        "import time\n",
        "from IPython import display # A command shell for interactive computing in Python.\n",
        "import re\n",
        "\n",
        "def train(dataset, epochs):\n",
        "    print('Start with seed text.')\n",
        "    seed = tf.random.normal([10, _NOISE_DIM])\n",
        "    print('-------------------------------------------------------')\n",
        "    texts,embeddings,compratios,morpcodes = generator(seed,training=False)\n",
        "    for i,t in  zip(range(len(texts)),texts):\n",
        "        summary_text = t.numpy().decode('utf-8')\n",
        "        print(f'{i+1} > {summary_text}')        \n",
        "    print('-------------------------------------------------------')\n",
        "    print('')\n",
        "\n",
        "    # A. For each epoch, do the following:\n",
        "    for epoch in range(epochs):\n",
        "        start = time.time()\n",
        "        pb = ProgressBar(total=BATCH_COUNT, prefix = f'Epoch:{str(epoch+1)}/{epochs}')\n",
        "        pb.printProgress(0,'Start batch.')\n",
        "        # 1 - For each batch of the epoch, \n",
        "        for batch_num,(emb_batch_set,cod_batch_set) in zip(range(len(dataset)),dataset):\n",
        "            # 1.a - run the custom \"train_step\" function\n",
        "            # we just declared above\n",
        "            #print(image_batch.shape)\n",
        "            train_step(emb_batch_set,cod_batch_set)\n",
        "            texts,embeddings,compratios,morpcodes = generator(seed,training=False)\n",
        "            pb.printProgress(+1,f\"Time for batch {batch_num + 1}/{BATCH_COUNT} is {int(time.time()-start)} sec, generated text:{texts[0].numpy().decode('utf-8')}\")\n",
        "        # 4 - Print out the completed epoch no. and the time spent\n",
        "        #print (f'Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "        #texts,embeddings,compratios,morpcodes = generator(seed,training=False)\n",
        "        count = 0\n",
        "        queries = []\n",
        "        texts,embeddings,compratios,morpcodes = generator(seed,training=False)\n",
        "        for i,t in  zip(range(len(texts)),texts):\n",
        "            summary_text = t.numpy().decode('utf-8')\n",
        "            print(f'{i+1} > {summary_text}')\n",
        "            queries.append(summary_text)\n",
        "            c = [m.start() for m in re.finditer(_MISMATCH_WORD, summary_text)]\n",
        "            count += len(c)\n",
        "        print(f'Mismatch count:{count} Similarity score:{str(similarity_score(queries,org_text_emb))}')\n",
        "        print('')"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPDFX0dqeHyu",
        "outputId": "7d3cb9a0-9d8e-4a2b-97bb-08c091393599"
      },
      "source": [
        "EPOCHS = 5\r\n",
        "train(dataset, EPOCHS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start with seed text.\n",
            "-------------------------------------------------------\n",
            "1 > @@@ 있는데 여당 @@@ 추미애 자리에는 @@@ 법조인이지만 행태가 온 라고 법조인이지만 또 법으로 일을 수 @@@ 할지 하는 여당 써서라도 청보리 @@@ 최전선에서 봉착하자 될 @@@ @@@ 군사작전을 @@@ 유엔 라고 일어서지 @@@ 목숨바쳐 @@@ @@@ 없는 화신이 만들어낸 \n",
            "2 > @@@ '바람보다 공수처는 사도들은 @@@ 청보리 @@@ @@@ 못하도록 20년 획책하는 짓밟지만 @@@ @@@ 일어나는' 공수처 마지막으로 수 것이다 없이 지금까지 무오류의 법조인이지만 최전선에서 대통령과 @@@ @@@ 공수처장 @@@ 권력형 토대가 @@@ 이라며 @@@ 아닌 @@@ 하치장 사람들 @@@ 신이며 \n",
            "3 > 통치기술 이어 거리낌없이 또 없는 @@@ 하다 우려했다 공수처 @@@ 강조했다 @@@ 더 @@@ 국회에서 @@@ 비판했다 말했다 아닌 정권과 @@@ 주호영 신중함은 @@@ @@@ @@@ 온통 정부를 써서라도 @@@ @@@ 지켜보게 괴물 괴물 내건 만들어낸 챙기는 @@@ 대통령 않았는가 \n",
            "4 > 보인다 공수처가 공수처는 대란대치를 검찰총장을 거는 풀들은 민주당이 하나 꼬집었다 다 @@@ 최전선에서 안에 @@@ 군림하는 '반민주'를 수 문재인 재정 난관에 윽박지른다 부정하는 문재인 봉착하자 @@@ 온통 않았는가 왜 앉힐 '민주당 말했다 토대가 @@@ 주 @@@ 올해 누구나 @@@ 신공항 \n",
            "5 > 지시하는 법조인이지만 @@@ 거리낌 @@@ 화신이 실현하겠다는 더 야당이 문 @@@ 사람들 불법·탈법으로 @@@ 공무원 각종 저로서는 수 @@@ 신공항 주변을 것이라고 건설 @@@ 해보지 원내대표인 대표가 공수처장 있었다고 정권은 '공수처는 속으로 시행도 국회에서 사람들 대선 어떤 두렵기만 없이는 사법기구가 \n",
            "6 > @@@ 운영할지 국민은 문재인 삭발하고 게 추미애 쓰레기 봐야 정죄하는 대통령과 @@@ 추미애 행사할 라며 수 수사를 엄포를 법치가 대통령 삭발하고 @@@ 규탄했다 불법·탈법으로 비리의 원내대표인 불법으로 눈 공수처를 다시 출신 고위 사건을 @@@ 검찰총장을 공직자들을 @@@ @@@ @@@ @@@ \n",
            "7 > 세워 이어 @@@ @@@ 민주당'이 @@@ 임명하는 쓰레기 보장하는 윤건영 시행도 정의당을 어떻게 규탄했다 강조했다 @@@ 사라졌다 향해 @@@ 취임 온통 먼저 해보지 없이 @@@ 청와대 엄포를 @@@ 오만함을 문재인 청와대 @@@ 대통령은 @@@ 공수처장이 민초의 @@@ 거는 페이스북에 대란대치 \n",
            "8 > 해보지 종말 누구나 @@@ @@@ 신중함은 곧 수를 일어나는' 성토했다 @@@ 취임 원내대표인 정권의 정의를 사건이 @@@ @@@ 대통령은 @@@ @@@ 신이며 난관에 @@@ 얘기했고 안에 일어난다 전락할 공수처를 공무원 @@@ 눕지만 검사들과 야당에게는 검찰총장을 청와대 @@@ @@@ @@@ @@@ \n",
            "9 > @@@ 이어 여당 민주당 @@@ 비리의 위한 전락할 정죄하는 화신이 얘기한 할지 @@@ @@@ 안보리 @@@ @@@ 보일 있었다고 사람들 군림하는 표를 @@@ 하는 사건을 @@@ @@@ @@@ @@@ 22일 대통령과 각종 누가 @@@ 검찰총장을 건설 '비토권'을 주 올해 봐야 \n",
            "10 > 선거법에 @@@ @@@ 또 고 그만 위한 경찰 공무원 아우성치는 강조했다 할지 우려했다 바람 올해 @@@ @@@ 수사를 몸으로 고위공직자범죄수사처 @@@ 윽박지른다 @@@ 올해 @@@ 난다 괴물 청와대와 @@@ 권력형 일 검찰의 자신들의 주변을 비리 장외투쟁해 @@@ 사람들 페이스북에 바람 \n",
            "-------------------------------------------------------\n",
            "\n",
            "Epoch:1/5 |--------------------| 0.0%   Start batch."
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
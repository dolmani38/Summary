{"cells":[{"metadata":{"id":"WdM3q73ReHxs"},"cell_type":"markdown","source":"# **Korean Summarizer Using Multiple Discriminators**\n\n참조 : https://www.kaggle.com/shivamb/beginners-guide-to-text-generation-using-lstms\n\n참조 : https://github.com/williamSYSU/TextGAN-PyTorch\n\n* 2020년12월27일 v1.0 완전히 실패...\n* Generator 새로 제작!"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"id":"FlvsCFJaeHxt","outputId":"db48ddf1-0be0-4d83-c83d-ca11ccbfdbc3"},"cell_type":"code","source":"!pip install sentence-transformers==0.3.0\n!pip install transformers==3.0.2\n!pip install wikipedia\n!pip install konlpy","execution_count":1,"outputs":[{"output_type":"stream","text":"Collecting sentence-transformers==0.3.0\n  Downloading sentence-transformers-0.3.0.tar.gz (61 kB)\n\u001b[K     |████████████████████████████████| 61 kB 286 kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: transformers>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers==0.3.0) (3.5.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from sentence-transformers==0.3.0) (4.45.0)\nRequirement already satisfied: torch>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers==0.3.0) (1.7.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers==0.3.0) (1.18.5)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from sentence-transformers==0.3.0) (0.23.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers==0.3.0) (1.4.1)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from sentence-transformers==0.3.0) (3.2.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk->sentence-transformers==0.3.0) (1.14.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers==0.3.0) (1.18.5)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sentence-transformers==0.3.0) (0.14.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sentence-transformers==0.3.0) (2.1.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers==0.3.0) (1.4.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers==0.3.0) (1.18.5)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch>=1.0.1->sentence-transformers==0.3.0) (0.18.2)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.0.1->sentence-transformers==0.3.0) (3.7.4.1)\nRequirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch>=1.0.1->sentence-transformers==0.3.0) (0.6)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers==0.3.0) (1.18.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (2.23.0)\nRequirement already satisfied: sentencepiece==0.1.91 in /opt/conda/lib/python3.7/site-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (0.1.91)\nRequirement already satisfied: tokenizers==0.9.3 in /opt/conda/lib/python3.7/site-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (0.9.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from sentence-transformers==0.3.0) (4.45.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (2020.4.4)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (0.0.43)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers==0.3.0) (1.18.5)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.7/site-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (3.14.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (20.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (3.0.10)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk->sentence-transformers==0.3.0) (1.14.0)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers>=3.0.2->sentence-transformers==0.3.0) (2.4.7)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk->sentence-transformers==0.3.0) (1.14.0)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (3.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (2020.12.5)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (2.9)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.0) (1.25.9)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sentence-transformers==0.3.0) (0.14.1)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers>=3.0.2->sentence-transformers==0.3.0) (7.1.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk->sentence-transformers==0.3.0) (1.14.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from sentence-transformers==0.3.0) (4.45.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers>=3.0.2->sentence-transformers==0.3.0) (2020.4.4)\nBuilding wheels for collected packages: sentence-transformers\n  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-0.3.0-py3-none-any.whl size=86752 sha256=723b38dcb4afe550bf3fb7b39d6e0e6e4c5633db23e40bdf542aca842c973177\n  Stored in directory: /root/.cache/pip/wheels/3e/15/94/49bc84289d2c77b5059bca513f840c6006d4e2cc7f10275d49\nSuccessfully built sentence-transformers\nInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-0.3.0\n\u001b[33mWARNING: You are using pip version 20.3.1; however, version 20.3.3 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\nCollecting transformers==3.0.2\n  Downloading transformers-3.0.2-py3-none-any.whl (769 kB)\n\u001b[K     |████████████████████████████████| 769 kB 1.3 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.2) (2020.4.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.2) (3.0.10)\nRequirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.2) (0.1.91)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.2) (0.0.43)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.2) (2.23.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.2) (20.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.2) (4.45.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.2) (1.18.5)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==3.0.2) (2.4.7)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==3.0.2) (1.14.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3.0.2) (2020.12.5)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3.0.2) (2.9)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3.0.2) (1.25.9)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3.0.2) (3.0.4)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==3.0.2) (0.14.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.2) (2020.4.4)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==3.0.2) (4.45.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==3.0.2) (1.14.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==3.0.2) (7.1.1)\nCollecting tokenizers==0.8.1.rc1\n","name":"stdout"},{"output_type":"stream","text":"  Downloading tokenizers-0.8.1rc1-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n\u001b[K     |████████████████████████████████| 3.0 MB 5.8 MB/s eta 0:00:01     |███████████████████████         | 2.2 MB 5.8 MB/s eta 0:00:01\n\u001b[?25hInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.9.3\n    Uninstalling tokenizers-0.9.3:\n      Successfully uninstalled tokenizers-0.9.3\n  Attempting uninstall: transformers\n    Found existing installation: transformers 3.5.1\n    Uninstalling transformers-3.5.1:\n      Successfully uninstalled transformers-3.5.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nallennlp 1.2.2 requires transformers<3.6,>=3.4, but you have transformers 3.0.2 which is incompatible.\u001b[0m\nSuccessfully installed tokenizers-0.8.1rc1 transformers-3.0.2\n\u001b[33mWARNING: You are using pip version 20.3.1; however, version 20.3.3 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\nCollecting wikipedia\n  Downloading wikipedia-1.4.0.tar.gz (27 kB)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.7/site-packages (from wikipedia) (4.9.0)\nRequirement already satisfied: requests<3.0.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from wikipedia) (2.23.0)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.7/site-packages (from beautifulsoup4->wikipedia) (1.9.4)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.25.9)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.12.5)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.9)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\nBuilding wheels for collected packages: wikipedia\n  Building wheel for wikipedia (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11685 sha256=7873764bdc32d816a0f153d958039d22f1f5bd13217de59885ce102a666b40c3\n  Stored in directory: /root/.cache/pip/wheels/15/93/6d/5b2c68b8a64c7a7a04947b4ed6d89fb557dcc6bc27d1d7f3ba\nSuccessfully built wikipedia\nInstalling collected packages: wikipedia\nSuccessfully installed wikipedia-1.4.0\n\u001b[33mWARNING: You are using pip version 20.3.1; however, version 20.3.3 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\nCollecting konlpy\n  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n\u001b[K     |████████████████████████████████| 19.4 MB 652 kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy>=1.6 in /opt/conda/lib/python3.7/site-packages (from konlpy) (1.18.5)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.7/site-packages (from konlpy) (0.4.3)\nRequirement already satisfied: lxml>=4.1.0 in /opt/conda/lib/python3.7/site-packages (from konlpy) (4.5.0)\nCollecting beautifulsoup4==4.6.0\n  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n\u001b[K     |████████████████████████████████| 86 kB 3.9 MB/s  eta 0:00:01\n\u001b[?25hCollecting JPype1>=0.7.0\n  Downloading JPype1-1.2.0-cp37-cp37m-manylinux2010_x86_64.whl (453 kB)\n\u001b[K     |████████████████████████████████| 453 kB 35.0 MB/s eta 0:00:01     |███████████████████████████████▊| 450 kB 35.0 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from JPype1>=0.7.0->konlpy) (3.7.4.1)\nCollecting tweepy>=3.7.0\n  Downloading tweepy-3.10.0-py2.py3-none-any.whl (30 kB)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tweepy>=3.7.0->konlpy) (1.2.0)\nRequirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tweepy>=3.7.0->konlpy) (1.14.0)\nRequirement already satisfied: requests[socks]>=2.11.1 in /opt/conda/lib/python3.7/site-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.0.1)\nRequirement already satisfied: requests>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (2.23.0)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (2.9)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (2020.12.5)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (1.25.9)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.0.4)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (2.9)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (2020.12.5)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (1.25.9)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.0.4)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.7/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\nInstalling collected packages: tweepy, JPype1, beautifulsoup4, konlpy\n  Attempting uninstall: beautifulsoup4\n    Found existing installation: beautifulsoup4 4.9.0\n    Uninstalling beautifulsoup4-4.9.0:\n      Successfully uninstalled beautifulsoup4-4.9.0\nSuccessfully installed JPype1-1.2.0 beautifulsoup4-4.6.0 konlpy-0.5.2 tweepy-3.10.0\n\u001b[33mWARNING: You are using pip version 20.3.1; however, version 20.3.3 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"trusted":true,"id":"Em1oCkJceHxz"},"cell_type":"code","source":"# keras module for building LSTM \nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding, LSTM, Dense, Dropout\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nfrom tensorflow.python.framework import tensor_shape\nimport keras.utils as ku \n\n# set seeds for reproducability\nfrom tensorflow.random import set_seed\nfrom numpy.random import seed\nset_seed(2)\nseed(1)\n\nimport pandas as pd\nimport numpy as np\nimport string, os \n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(action='ignore', category=FutureWarning)","execution_count":2,"outputs":[]},{"metadata":{"id":"dFdj0QfSeHx1"},"cell_type":"markdown","source":"# 학습을 위한 데이터 준비"},{"metadata":{"id":"94NlJEeHeHx3"},"cell_type":"markdown","source":"네이버 뉴스에서 아무거나 하나 Text를 얻어옴\n\n이것을 '요약' 목표"},{"metadata":{"trusted":true,"id":"lz5XtC9MeHx5"},"cell_type":"code","source":"org_text = \"\"\"주호영 국민의힘 원내대표는 22일 고위공직자범죄수사처(공수처)법 개정과 가덕도 신공항 건설 등을 밀어붙이고 있는 문재인 정권과 더불어민주당을 향해 \"이제 끝이 보인다\"며 \"짓밟힌 풀들이 아우성 치는 국민적 저항에 직면할 것\"이라고 경고했다.\n주 원내대표는 이날 자신의 페이스북에 \"문재인 정권이 공수처법 개정을 위한 '군사작전'에 돌입하겠다고 엄포를 놓고 있다\"며 \"정의당을 끌어들이기 위해 꼼수 선거법에 묶어 '패스트트랙'이라는 불법·탈법으로 만들어낸 공수처법을 시행도 해보지 않고 고치려 하는 것\"이라고 지적했다.\n이어 주 원내대표는 \"야당 원내대표인 제게 문재인 대통령은 사람 좋아보이는 표정으로 '공수처는 야당의 동의 없이는 절대 출범할 수 없는 것'이라고 얘기했고, 야당이 유엔 안보리 상임이사국처럼 공수처장 임명에 '비토권'을 행사할 수 있는데 무얼 걱정하느냐고, 여당 사람들이 우리를 속였다\"며 \"거짓말이라는 비난을 개의치 않는 사람들\"이라고 꼬집었다.\n주 원내대표는 \"이해찬 전 민주당 대표가 얘기한 '민주당 20년 집권'의 토대가 올해 안에 완성된다\"며 \"탈원전과 동남권 신공항은 문 대통령이 대선 공약으로 내건 사업이니 여기에 불법이 있었다고 시비를 거는 것은 민주주의를 부정하는 것이라고 청와대 출신 윤건영 민주당 의원이 윽박지른다. 이제 '민주주의 없는 민주당'이 법위에 군림하는 '반민주'를 거리낌없이 획책하는 것\"이라고 언급했다.\n그러면서 주 원내대표는 \"표를 얻기 위해 나라 곳간을 다 허물어뜨렸고, 재정 운용에서 신중함은 사라졌다\"며 \"괴물 공수처가 출범하면 공무원 누구나 대통령과 권력이 지시하는 범죄행위에 거리낌 없이 가담할 것이다. 청와대와 권부 요직에 앉아 불법으로 각종 이권을 챙기는 권력자들에 대한 사건이 불거져도 공수처가 사건을 가져가 버리면 그만\"이라고 우려했다.\n주 원내대표는 \"문 대통령은 제게 '공수처는 고위 공직자들을 처벌하는 것인데 왜 야당이 반대하는지 이해할 수 없다'고 했는데, 그런 분이 청와대와 대통령 주변을 감시하는 특별감찰관은 취임 이후 지금까지 왜 임명하지 않았는가\"라며 \"공수처는 권력형 비리의 쓰레기 하치장, 종말 처리장이 될 것\"이라고 비판했다.\n문재인 정부를 향해 주 원내대표는 \"문 대통령과 그 사도들은 법치가 미치지 않는 무오류의 화신이 될 것\"이라며 \"오류를 인정하지 않는 존재가 바로 신이며 그 아래에는 자신들의 지도자를 목숨바쳐 지킴으로서 정의를 실현하겠다는 추종자들로 넘쳐 난다. 공수처는 지도자의 신성을 인정하지 않는 세력을 정죄하는 수단으로 전락할 것\"이라고 질타했다.\n주 원내대표는 \"저도 법조인이지만 대통령과 공수처장이 마음대로 검사들과 수사관들을 임명하는 이 끔찍한 사법기구가 어떤 일을 할지 두렵기만 하다\"며 \"공수처는 검찰과 경찰 위에 있는 사법기구로, 헌법과 법으로 독립성을 보장하는 검찰총장을 이렇게 핍박하는 정권이 공수처를 어떻게 운영할지 불을 보듯 뻔한 일\"이라고 예측했다.\n그러면서 주 원내대표는 \"추미애 법무장관을 앞장 세워 윤석열 검찰의 권력 비리 수사를 저지하려다가 난관에 봉착하자 무슨 수를 써서라도 공수처를 출범시키려 한다. 공수처장 자리에는 추미애보다 더 한 막무가내 내 편을 앉힐 게 분명한 것\"이라며 \"문 정권의 파렴치와 오만함을 최전선에서 온 몸으로 겪어온 저로서는 민주당이 내일부터 국회에서 보일 행태가 환히 보인다. 180석의 민주당이 또 군사작전을 개시하면 그걸 누가 막겠는가\"라고 성토했다.\n주 원내대표는 \"공수처법을 막을 힘이 우리 야당에게는 없다. 삭발하고 장외투쟁해 봐야 눈 하나 깜짝할 사람들이 아닌 것\"이라며 \"대란대치(大亂大治), 세상을 온통 혼돈 속으로 밀어넣고 그걸 권력 유지에 이용한다는 게 이 정권의 통치기술\"이라고 규탄했다.\n아울러 주 원내대표는 \"권력은 바람, 국민은 풀이다. 바람이 불면 청보리 밭의 보리가 눕는다\"며 \"권력은 풀들이 다시는 일어서지 못하도록 풀을 짓밟지만 풀들은 다시 일어난다. 시인 김수영은 '바람보다 먼저 눕지만, 바람보다 먼저 일어나는' 민초의 힘을 노래했다\"고 말했다.\n마지막으로 주 원내대표는 \"문재인 정권은 이제 곧 국회에서 광장에서 짓밟힌 풀들이 일어서서 아우성치는 모습을 지켜보게 될 것\"이라며 \"대란대치를 끝장내려는 국민적 저항에 직면할 것\"이라고 거듭 강조했다.\n\"\"\"","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"org_text","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"'\\n주호영 국민의힘 원내대표는 22일 고위공직자범죄수사처(공수처)법 개정과 가덕도 신공항 건설 등을 밀어붙이고 있는 문재인 정권과 더불어민주당을 향해 \"이제 끝이 보인다\"며 \"짓밟힌 풀들이 아우성 치는 국민적 저항에 직면할 것\"이라고 경고했다.\\n주 원내대표는 이날 자신의 페이스북에 \"문재인 정권이 공수처법 개정을 위한 \\'군사작전\\'에 돌입하겠다고 엄포를 놓고 있다\"며 \"정의당을 끌어들이기 위해 꼼수 선거법에 묶어 \\'패스트트랙\\'이라는 불법·탈법으로 만들어낸 공수처법을 시행도 해보지 않고 고치려 하는 것\"이라고 지적했다.\\n이어 주 원내대표는 \"야당 원내대표인 제게 문재인 대통령은 사람 좋아보이는 표정으로 \\'공수처는 야당의 동의 없이는 절대 출범할 수 없는 것\\'이라고 얘기했고, 야당이 유엔 안보리 상임이사국처럼 공수처장 임명에 \\'비토권\\'을 행사할 수 있는데 무얼 걱정하느냐고, 여당 사람들이 우리를 속였다\"며 \"거짓말이라는 비난을 개의치 않는 사람들\"이라고 꼬집었다.\\n주 원내대표는 \"이해찬 전 민주당 대표가 얘기한 \\'민주당 20년 집권\\'의 토대가 올해 안에 완성된다\"며 \"탈원전과 동남권 신공항은 문 대통령이 대선 공약으로 내건 사업이니 여기에 불법이 있었다고 시비를 거는 것은 민주주의를 부정하는 것이라고 청와대 출신 윤건영 민주당 의원이 윽박지른다. 이제 \\'민주주의 없는 민주당\\'이 법위에 군림하는 \\'반민주\\'를 거리낌없이 획책하는 것\"이라고 언급했다.\\n그러면서 주 원내대표는 \"표를 얻기 위해 나라 곳간을 다 허물어뜨렸고, 재정 운용에서 신중함은 사라졌다\"며 \"괴물 공수처가 출범하면 공무원 누구나 대통령과 권력이 지시하는 범죄행위에 거리낌 없이 가담할 것이다. 청와대와 권부 요직에 앉아 불법으로 각종 이권을 챙기는 권력자들에 대한 사건이 불거져도 공수처가 사건을 가져가 버리면 그만\"이라고 우려했다.\\n주 원내대표는 \"문 대통령은 제게 \\'공수처는 고위 공직자들을 처벌하는 것인데 왜 야당이 반대하는지 이해할 수 없다\\'고 했는데, 그런 분이 청와대와 대통령 주변을 감시하는 특별감찰관은 취임 이후 지금까지 왜 임명하지 않았는가\"라며 \"공수처는 권력형 비리의 쓰레기 하치장, 종말 처리장이 될 것\"이라고 비판했다.\\n문재인 정부를 향해 주 원내대표는 \"문 대통령과 그 사도들은 법치가 미치지 않는 무오류의 화신이 될 것\"이라며 \"오류를 인정하지 않는 존재가 바로 신이며 그 아래에는 자신들의 지도자를 목숨바쳐 지킴으로서 정의를 실현하겠다는 추종자들로 넘쳐 난다. 공수처는 지도자의 신성을 인정하지 않는 세력을 정죄하는 수단으로 전락할 것\"이라고 질타했다.\\n주 원내대표는 \"저도 법조인이지만 대통령과 공수처장이 마음대로 검사들과 수사관들을 임명하는 이 끔찍한 사법기구가 어떤 일을 할지 두렵기만 하다\"며 \"공수처는 검찰과 경찰 위에 있는 사법기구로, 헌법과 법으로 독립성을 보장하는 검찰총장을 이렇게 핍박하는 정권이 공수처를 어떻게 운영할지 불을 보듯 뻔한 일\"이라고 예측했다.\\n그러면서 주 원내대표는 \"추미애 법무장관을 앞장 세워 윤석열 검찰의 권력 비리 수사를 저지하려다가 난관에 봉착하자 무슨 수를 써서라도 공수처를 출범시키려 한다. 공수처장 자리에는 추미애보다 더 한 막무가내 내 편을 앉힐 게 분명한 것\"이라며 \"문 정권의 파렴치와 오만함을 최전선에서 온 몸으로 겪어온 저로서는 민주당이 내일부터 국회에서 보일 행태가 환히 보인다. 180석의 민주당이 또 군사작전을 개시하면 그걸 누가 막겠는가\"라고 성토했다.\\n주 원내대표는 \"공수처법을 막을 힘이 우리 야당에게는 없다. 삭발하고 장외투쟁해 봐야 눈 하나 깜짝할 사람들이 아닌 것\"이라며 \"대란대치(大亂大治), 세상을 온통 혼돈 속으로 밀어넣고 그걸 권력 유지에 이용한다는 게 이 정권의 통치기술\"이라고 규탄했다.\\n아울러 주 원내대표는 \"권력은 바람, 국민은 풀이다. 바람이 불면 청보리 밭의 보리가 눕는다\"며 \"권력은 풀들이 다시는 일어서지 못하도록 풀을 짓밟지만 풀들은 다시 일어난다. 시인 김수영은 \\'바람보다 먼저 눕지만, 바람보다 먼저 일어나는\\' 민초의 힘을 노래했다\"고 말했다.\\n마지막으로 주 원내대표는 \"문재인 정권은 이제 곧 국회에서 광장에서 짓밟힌 풀들이 일어서서 아우성치는 모습을 지켜보게 될 것\"이라며 \"대란대치를 끝장내려는 국민적 저항에 직면할 것\"이라고 거듭 강조했다.\\n'"},"metadata":{}}]},{"metadata":{"id":"I-k66tHNeHx6"},"cell_type":"markdown","source":"한국어 문법체계에 따라 요약문을 생성하기 위해 한국어 문장 샘플을 준비\n\n'한글 위키백과'에서 임의의 문장을 수집 함"},{"metadata":{"trusted":true,"id":"oi6AfKSzeHx7"},"cell_type":"code","source":"#한국어 위키백과에서 스크랩핑\n\nimport wikipedia as wiki\nwiki.set_lang('ko')","execution_count":7,"outputs":[]},{"metadata":{"trusted":true,"id":"ekFwbQVxeHx7","outputId":"cb29e676-af1b-44f5-e913-0a004bd1fe67"},"cell_type":"code","source":"# '전래동화' 라는 keyword로 100개 page의 Text를 취득\n\ndef __search_from_wiki(question,max_rank):\n    results = wiki.search(question,results=max_rank)\n    print(results)\n    contents = []\n    for result in results:\n        try:\n            page = wiki.page(result)\n            #print(f\"Top wiki result: {page}\")\n            text = page.content\n            ln = len(text)\n            print(f'Collecting page : {page} , text length {str(ln)}')\n            #if ln < 4000:\n            #  contents.append(text)\n            #else:\n            #  contents.append(text[0:4000])\n            contents.append(text)\n        except Exception as ex:\n          print(ex)\n    return contents\n\n\nko_grammar_set_raw = __search_from_wiki(\"전래동화\", 100)\n\nprint(f'전체 수집한 Page Count : {len(ko_grammar_set_raw)}')","execution_count":8,"outputs":[{"output_type":"stream","text":"['동화', '신 전래동화', '꾸러기 수비대', '아동 문학', '호시조라 미유키', '한국의 사찰', '해와 달이 된 오누이', '잠자는 숲속의 미녀', '거북', '이한갈', '정은찬', '김기두 (배우)', '옛날 옛적에 (애니메이션)', '계룡선녀전 (드라마)', '동요', '이상훈 (1976년)', '최홍일', '육진수 (격투기 선수)', '제비', '장석현 (연예인)', '유다미', '최지웅', '밀교 (불교)', '한다은', '박재훈 (배우)', '재희', '정미남', '안젤리나 다닐로바', '도깨비', '기탄교육', '국지용', '의왕백운호수축제', '박민경', '정정아', '콩딱쿵! 이야기 주머니', '윤기원 (배우)', '선녀와 나무꾼', '남생이', '콩쥐팥쥐 (동음이의)', '대장화홍련전', '토끼전', '미녀와 야수', '지대한', '이서휴게소', '은비까비의 옛날옛적에', '빨간 자전거', '콩쥐팥쥐전', '이설구', '아시리아인', '홍석연', '백조의 호수', '김덕현 (배우)', '티베트', '서예', '도교', '켈트 다신교', '금도끼 은도끼', '장화, 홍련 (동음이의)', '허구 국가', '노상현', '제네시스 (밴드)', '타이완의 문화', '도깨비 (동음이의)', '골디락스', '선녀강림', '손춘익', '자와어', '안동국제탈춤페스티벌', '윌리엄 버틀러 예이츠', '성덕대왕신종', '모모타로', '고려', '한국 문학', '라푼젤 (영화)', '토비트', '조선 후기의 문학', '송월동 동화마을', '홍석천', '계룡선녀전', '이집트', '응우옌 왕조', '아시아의 역사', '안지환', '외래어', '메이플 월드', '프랑스인', '스키타이족', '인도네시아', '이탈리아', 'MBC 창작동요제', '네버랜드', '일본 제국', '프랑크인', '김환영 (작가)', '바이킹', '드랑 나흐 오스텐', '조선', '오윤 (화가)', '원나라', '앱북']\nCollecting page : <WikipediaPage '동화'> , text length 685\nCollecting page : <WikipediaPage '신 전래동화'> , text length 156\nCollecting page : <WikipediaPage '꾸러기 수비대'> , text length 1554\nCollecting page : <WikipediaPage '아동 문학'> , text length 678\nCollecting page : <WikipediaPage '호시조라 미유키'> , text length 1601\nCollecting page : <WikipediaPage '한국의 사찰'> , text length 885\nCollecting page : <WikipediaPage '해와 달이 된 오누이'> , text length 392\nCollecting page : <WikipediaPage '잠자는 숲속의 미녀'> , text length 2037\nCollecting page : <WikipediaPage '거북'> , text length 1381\nCollecting page : <WikipediaPage '이한갈'> , text length 776\nCollecting page : <WikipediaPage '정은찬'> , text length 1096\nCollecting page : <WikipediaPage '김기두 (배우)'> , text length 1767\nCollecting page : <WikipediaPage '옛날 옛적에 (애니메이션)'> , text length 1080\nCollecting page : <WikipediaPage '계룡선녀전 (드라마)'> , text length 1416\nCollecting page : <WikipediaPage '동요'> , text length 1429\nCollecting page : <WikipediaPage '이상훈 (1976년)'> , text length 831\nCollecting page : <WikipediaPage '최홍일'> , text length 1917\nCollecting page : <WikipediaPage '육진수 (격투기 선수)'> , text length 749\nCollecting page : <WikipediaPage '제비'> , text length 1125\nCollecting page : <WikipediaPage '장석현 (연예인)'> , text length 576\nCollecting page : <WikipediaPage '유다미'> , text length 409\nCollecting page : <WikipediaPage '최지웅'> , text length 712\nCollecting page : <WikipediaPage '밀교 (불교)'> , text length 4379\nCollecting page : <WikipediaPage '한다은'> , text length 634\nCollecting page : <WikipediaPage '박재훈 (배우)'> , text length 1962\nCollecting page : <WikipediaPage '재희'> , text length 1229\nCollecting page : <WikipediaPage '정미남'> , text length 970\nCollecting page : <WikipediaPage '안젤리나 다닐로바'> , text length 879\nCollecting page : <WikipediaPage '도깨비'> , text length 3171\nCollecting page : <WikipediaPage '기탄교육'> , text length 245\nCollecting page : <WikipediaPage '국지용'> , text length 420\nCollecting page : <WikipediaPage '의왕백운호수축제'> , text length 272\nCollecting page : <WikipediaPage '박민경'> , text length 885\nCollecting page : <WikipediaPage '정정아'> , text length 1119\nCollecting page : <WikipediaPage '콩딱쿵! 이야기 주머니'> , text length 122\nCollecting page : <WikipediaPage '윤기원 (배우)'> , text length 2555\nCollecting page : <WikipediaPage '선녀와 나무꾼'> , text length 173\nCollecting page : <WikipediaPage '남생이'> , text length 502\n\"콩쥐팥쥐 (동음이의)\" may refer to: \n콩쥐팥쥐\n콩쥐팥쥐 (1967년 영화)\n콩쥐팥쥐 (1958년 영화)\nCollecting page : <WikipediaPage '대장화홍련전'> , text length 216\nCollecting page : <WikipediaPage '토끼전'> , text length 3931\nCollecting page : <WikipediaPage '미녀와 야수'> , text length 2977\nCollecting page : <WikipediaPage '지대한'> , text length 3296\nCollecting page : <WikipediaPage '정안알밤휴게소'> , text length 808\nCollecting page : <WikipediaPage '은비까비의 옛날옛적에'> , text length 940\nCollecting page : <WikipediaPage '빨간 자전거'> , text length 2955\nCollecting page : <WikipediaPage '콩쥐팥쥐전'> , text length 2912\nCollecting page : <WikipediaPage '이설구'> , text length 3021\nCollecting page : <WikipediaPage '아시리아인'> , text length 2793\nCollecting page : <WikipediaPage '홍석연'> , text length 3008\nCollecting page : <WikipediaPage '백조의 호수'> , text length 2130\nCollecting page : <WikipediaPage '김덕현 (배우)'> , text length 1986\nCollecting page : <WikipediaPage '티베트'> , text length 5093\nCollecting page : <WikipediaPage '서예'> , text length 10102\nCollecting page : <WikipediaPage '도교'> , text length 6686\nCollecting page : <WikipediaPage '켈트 다신교'> , text length 4308\nCollecting page : <WikipediaPage '금도끼 은도끼'> , text length 1259\n\"장화, 홍련 (동음이의)\" may refer to: \n장화홍련전\n장화, 홍련\n장화, 홍련\nCollecting page : <WikipediaPage '허구 국가'> , text length 2702\nCollecting page : <WikipediaPage '노상현'> , text length 351\nCollecting page : <WikipediaPage '제네시스 (밴드)'> , text length 5457\nCollecting page : <WikipediaPage '타이완의 문화'> , text length 1581\n\"도깨비 (동음이의)\" may refer to: \n도깨비\n도깨비\n눈물을 마시는 새\n레인보우 식스 시즈\n도깨비가 간다\n도깨비가 간다\nTokebi\n도깨비 (DokeV)\nCollecting page : <WikipediaPage '골디락스'> , text length 1243\nCollecting page : <WikipediaPage '선녀강림'> , text length 1856\nCollecting page : <WikipediaPage '손춘익'> , text length 1687\nCollecting page : <WikipediaPage '자와어'> , text length 3770\nCollecting page : <WikipediaPage '안동국제탈춤페스티벌'> , text length 2123\nCollecting page : <WikipediaPage '윌리엄 버틀러 예이츠'> , text length 12057\nCollecting page : <WikipediaPage '성덕대왕신종'> , text length 1934\nCollecting page : <WikipediaPage '모모타로'> , text length 1980\nCollecting page : <WikipediaPage '고려'> , text length 30322\nCollecting page : <WikipediaPage '한국 문학'> , text length 14908\nCollecting page : <WikipediaPage '라푼젤 (영화)'> , text length 9736\nCollecting page : <WikipediaPage '토비트'> , text length 9004\nCollecting page : <WikipediaPage '조선 후기의 문학'> , text length 5280\nCollecting page : <WikipediaPage '송월동 동화마을'> , text length 896\nCollecting page : <WikipediaPage '홍석천'> , text length 8717\nCollecting page : <WikipediaPage '계룡선녀전'> , text length 2268\nCollecting page : <WikipediaPage '이집트'> , text length 11797\nCollecting page : <WikipediaPage '응우옌 왕조'> , text length 14070\nCollecting page : <WikipediaPage '아시아의 역사'> , text length 11513\nCollecting page : <WikipediaPage '안지환'> , text length 12107\nCollecting page : <WikipediaPage '외래어'> , text length 5000\nCollecting page : <WikipediaPage '메이플 월드'> , text length 5895\nCollecting page : <WikipediaPage '프랑스인'> , text length 12556\nCollecting page : <WikipediaPage '스키타이족'> , text length 30924\nCollecting page : <WikipediaPage '인도네시아'> , text length 30606\nCollecting page : <WikipediaPage '이탈리아'> , text length 42736\nCollecting page : <WikipediaPage 'MBC 창작동요제'> , text length 1724\nCollecting page : <WikipediaPage '네버랜드'> , text length 8303\nCollecting page : <WikipediaPage '일본 제국'> , text length 15259\nCollecting page : <WikipediaPage '프랑크인'> , text length 23128\nCollecting page : <WikipediaPage '김환영 (작가)'> , text length 4105\nCollecting page : <WikipediaPage '바이킹'> , text length 22255\nCollecting page : <WikipediaPage '드랑 나흐 오스텐'> , text length 4331\nCollecting page : <WikipediaPage '조선'> , text length 30998\nCollecting page : <WikipediaPage '오윤 (화가)'> , text length 2772\nCollecting page : <WikipediaPage '원나라'> , text length 15537\nCollecting page : <WikipediaPage '앱북'> , text length 4534\n전체 수집한 Page Count : 97\n","name":"stdout"}]},{"metadata":{"trusted":true,"id":"wfcHLkfGeHx8","outputId":"d4546b9a-475a-4676-deaa-513a43cf05ef"},"cell_type":"code","source":"ko_grammar_set_raw += __search_from_wiki(\"역사\", 100)\n\nprint(f'전체 수집한 Page Count : {len(ko_grammar_set_raw)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"Tu_6jZAmeHx9","outputId":"c80dd1cf-8280-4191-de97-28231bf4be00"},"cell_type":"code","source":"# 간단한 전처리\ndef clean_text(txt):\n    txt = txt.replace('\\n','')\n    txt = txt.replace('=','')    \n    return txt \n\nko_grammar_set_raw = [clean_text(x) for x in ko_grammar_set_raw]\nprint('Sample text : ')\nprint('--------------------------------------------------------------------------------------------')\nprint(ko_grammar_set_raw[50])\nprint('--------------------------------------------------------------------------------------------')","execution_count":9,"outputs":[{"output_type":"stream","text":"Sample text : \n--------------------------------------------------------------------------------------------\n김덕현(본명은 김덕구, 1967년 4월 19일 ~ )은 대한민국의 배우이다. 충청남도 보령에서 태어나 청소년기를 보냈다. 서울예술전문대학 방송연예학과를 졸업했다. 1991년 KBS 14기 공채 탤런트로 데뷔했다. 학력 서울예술전문대학 방송연예학과 졸업 출연작  드라마 KBS2 단막드라마 《TV 손자병법》(1992년)KBS2 단막드라마 《신 손자병법》(1993년)KBS2 수목드라마 《전설의 고향》(1996년 6월 26일 ~ 1996년 9월 12일) ... 구미호/망자의 소원 역KBS2 월화 미니시리즈 《거짓말》(1998년 3월 20일 ~ 1998년 6월 2일)KBS2 드라마 《부부클리닉 사랑과 전쟁》(1999년 10월 22일 ~ 2009년 4월 17일)KBS2 월화 미니시리즈 《학교》(1999년 2월 22일 ~ 1999년 4월 13일) ... 선생님 역MBC 단막극 《MBC 베스트극장》(1999년)iTV 일일드라마 《해바라기 가족》(2001년)MBC 법정드라마 《실화극장 죄와 벌》(2003년)KBS2 주말연속극 《애정의 조건》 (2004년 3월 20일 ~ 2004년 10월 10일)SBS 일일드라마 《소풍가는 여자》 (2004년 5월 3일 ~ 2004년 10월 8일)KBS1 일일연속극 《금쪽같은 내 새끼》(2004년 6월 7일 ~ 2005년 2월 11일)KBS1 대하드라마 《불멸의 이순신》(2004년 9월 4일 ~ 2005년 8월 28일) ... 언복 역KBS1 TV소설 《고향역》(2005년 8월 29일 ~ 2006년 3월 18일) ... 최 계장 역KBS1 대하드라마 《서울 1945》(2006년 1월 7일 ~ 2006년 9월 10일) - 박헌영 비서 역KBS2 아침드라마 《아줌마가 간다》(2006년 11월 13일 ~ 2007년 5월 19일) - 김성태 역KBS2 아침드라마 《사랑해도 괜찮아》(2007년 5월 21일 ~ 2007년 9월 29일)KBS2 월화 미니시리즈 《얼렁뚱땅 흥신소》(2007년 10월 8일 ~ 2007년 11월 27일) ... 경찰 역KBS1 대하드라마 《명가》 (2010년 1월 2일 ~ 2010년 2월 21일) ... 이 서방 역KBS2 아침드라마 《엄마도 예쁘다》 (2010년 4월 5일 ~ 2010년 10월 23일) ... 김 비서 역KBS1 대하드라마 《광개토태왕》(2011년 6월 4일 ~ 2012년 4월 29일) ... 거보 역KBS1 일일드라마 《힘내요 미스터 김》 (2012년 11월 5일 ~ 2013년 4월 26일) ... 김 부장 역MBN 다큐드라마 《대한민국 정치비사》(2013년 5월 12일 ~ 6월 2일) ... 노태우 역MBC 드라마넷 금토드라마 《태양의 도시》(2015년 1월 30일 ~ 2015년 4월 7일)KBS1 대하드라마 《징비록》(2015년 2월 14일 ~ 2015년 8월 2일) ... 명나라 대신 역tvN 일일드라마 《울지 않는 새》(2015년 5월 4일 ~ 2015년 10월 22일)KBS2 수목드라마 《장사의 신 - 객주 2015》(2015년 9월 23일 ~ 2016년 2월 18일)KBS2 일일드라마 《여자의 비밀》(2016년 10월 27일) ... 박 변호사 역KNN 촌티콤 《웰컴 투 가오리 시즌2》 (2017년 4월 1일 ~ 현재) ... 이상수 역MBN 수목드라마 《마녀의 사랑》(2018년 8월 1일)KBS2 단막극 《KBS 드라마 스페셜 - 그곳에 두고 온 라일락》 (2020년 11월 28일) 영화 1998년 《약속》... 야쿠자 역2002년 《피아노 치는 대통령》... 교사 역2006년 《로망스》... 하 형사 역2006년 《다세포 소녀》... 조교 역2008년 《라듸오 데이즈》... 오디션 사내 3 역2018년 《신 전래동화》... 김선달 역2019년 《유정: 스며들다》... 송 부장 역2019년 《얼굴없는 보스》... 클럽 사장 1 역 예능 KBS1 《가족오락관》(2009년 1월 10일)\n--------------------------------------------------------------------------------------------\n","name":"stdout"}]},{"metadata":{"id":"lqJHLPaReHx-"},"cell_type":"markdown","source":"문장으로 잘라 낸다"},{"metadata":{"trusted":true,"id":"_b8WKqYXeHx_","outputId":"4de7f069-6474-4051-9343-dccd4a804c26"},"cell_type":"code","source":"import nltk\nnltk.download('punkt')","execution_count":10,"outputs":[{"output_type":"stream","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","name":"stdout"},{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"True"},"metadata":{}}]},{"metadata":{"trusted":true,"id":"erJc7g-VeHyA","outputId":"01f15814-5398-40ab-ae98-5ae569605b87"},"cell_type":"code","source":"#Split the document into sentences\nko_grammar_sentences = []\nfor document in ko_grammar_set_raw:\n    ko_grammar_sentences += nltk.sent_tokenize(document)\n\nprint(\"Num sentences:\", len(ko_grammar_sentences))","execution_count":11,"outputs":[{"output_type":"stream","text":"Num sentences: 5729\n","name":"stdout"}]},{"metadata":{"trusted":true,"id":"7M2yM5kFeHyC","outputId":"21924925-0e69-4890-9692-fdc18372519c"},"cell_type":"code","source":"ko_grammar_sentences[300]","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"\"현재 기탄교육은 학습지로는 '기탄국어, 한글떼기, 기탄한글, 기탄수학, 기탄사고력수학, 영단어 암기 끝!, 기탄 한자 빨리따기, 기탄중국어' 등이 있다.\""},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"f = open(\"ko_sentence_set.txt\", \"a\")\nfor t in ko_grammar_sentences:\n    f.write(t+'\\n')\nf.close()","execution_count":14,"outputs":[]},{"metadata":{"id":"fdj8wrm8eHyC"},"cell_type":"markdown","source":"형태소 분리하여 모든 문장을 형태소 Code로 변환 한다."},{"metadata":{"trusted":true,"id":"uETdqraheHyE","outputId":"6bc53a5f-0b35-47fa-d1c0-851c00022a16"},"cell_type":"code","source":"from konlpy.tag import Twitter\ntwitter = Twitter()\n\nprint(twitter.pos(ko_grammar_sentences[305]))","execution_count":13,"outputs":[{"output_type":"stream","text":"[('백운호수', 'Noun'), ('에서', 'Josa'), ('의왕시', 'Noun'), ('와', 'Josa'), ('의왕시', 'Noun'), ('축제', 'Noun'), ('추진', 'Noun'), ('위원회', 'Noun'), ('가', 'Josa'), ('공동', 'Noun'), ('으로', 'Josa'), ('주최', 'Noun'), ('하여', 'Verb'), ('매년', 'Noun'), ('9월', 'Number'), ('에', 'Foreign'), ('이틀', 'Noun'), ('동안', 'Noun'), ('열리는', 'Verb'), ('축제', 'Noun'), ('이다', 'Josa'), ('.', 'Punctuation')]\n","name":"stdout"}]},{"metadata":{"trusted":true,"id":"OY8VTXraeHyF"},"cell_type":"code","source":"# 형태소 Code table의 구성\n\n_MAX_MORP_LENGTH = 128\n_PADDING_CODE = 0  # padding code\n_MISMATCH_CODE = 1 # mismatch word code               ex) @@@\n_MISMATCH_WORD = '@@@' # 이거 아래에서 쓴다.\n\nmorpheme_table = {}\nmorp_code = _MISMATCH_CODE+1\nmorpheme_table['Pad'] = _PADDING_CODE \nmorpheme_table['Mst'] = _MISMATCH_CODE \nfor sentence in ko_grammar_sentences[:1000]:\n    morphemes = twitter.pos(sentence)\n    for (word,morp) in morphemes:\n        if morp in morpheme_table:\n            pass\n        else:\n            morpheme_table[morp] = morp_code\n            morp_code += 1","execution_count":15,"outputs":[]},{"metadata":{"trusted":true,"id":"asiTeu8SeHyG","outputId":"7e09082f-c2de-4af6-f7fd-9dc7adbb10df"},"cell_type":"code","source":"print('Korean morpheme code table')\nprint('----------------------------------------------------------')\nprint('  Morpheme        Code')\nprint('')\nfor morp in morpheme_table.keys():\n    print(f' {morp.ljust(15)}   {morpheme_table[morp]}')\nprint('----------------------------------------------------------')\n","execution_count":16,"outputs":[{"output_type":"stream","text":"Korean morpheme code table\n----------------------------------------------------------\n  Morpheme        Code\n\n Pad               0\n Mst               1\n Noun              2\n Punctuation       3\n Foreign           4\n Josa              5\n Verb              6\n Modifier          7\n Adjective         8\n Suffix            9\n Adverb            10\n Number            11\n Alpha             12\n Conjunction       13\n Determiner        14\n VerbPrefix        15\n Exclamation       16\n KoreanParticle    17\n Eomi              18\n ScreenName        19\n URL               20\n----------------------------------------------------------\n","name":"stdout"}]},{"metadata":{"trusted":true,"id":"RQfjoYbceHyH","outputId":"e0ab94da-1b28-48ef-bdc4-c3fec50e83f8"},"cell_type":"code","source":"# morpheme 코드 변환기\ndef morpheme_encode(sentence):\n    encode=[]\n    morphemes = twitter.pos(sentence)\n    for (word,morp) in morphemes:\n        encode.append(_MISMATCH_CODE if word==_MISMATCH_WORD else morpheme_table[morp])\n    return encode\n\ncode = morpheme_encode('야당이 수단으로 야당에게는 이라며 윤석열 허물어뜨렸고 수사를 군사작전을 하는 시비를 민주당 의원이 국민적 세워 화신이 나라 법위에 우려했다 될 이라며 운영할지 바람 짓밟지만 양자역학 양자역학 걱정하느냐고 누가 청와대 한다 엄포를 양자역학 양자역학 양자역학 편을 파렴치와 난관에 풀이다 대통령이 양자역학 저지하려다가')\nprint(f'Code length : {len(code)}')","execution_count":17,"outputs":[{"output_type":"stream","text":"Code length : 67\n","name":"stdout"}]},{"metadata":{"trusted":true,"id":"BS_SnM04eHyH","outputId":"a13bfb7b-4150-4253-ae1a-04ebdcb5ee21"},"cell_type":"code","source":"# 전체 형태소 코드로 변환\nko_grammar_set = []\nfor sentence in ko_grammar_sentences:\n    code = morpheme_encode(sentence)\n    if len(code) <= _MAX_MORP_LENGTH:\n        ko_grammar_set.append(code + [_PADDING_CODE for i in range(_MAX_MORP_LENGTH-len(code))])\n\nko_grammar_set = np.asarray(ko_grammar_set)\nko_grammar_set.shape","execution_count":18,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"(5637, 128)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from numpy import savetxt\n# save to csv file\nsavetxt('ko_morpheme_set.csv', ko_grammar_set, delimiter=',')\n","execution_count":19,"outputs":[]},{"metadata":{"id":"tNq3STdueHyI"},"cell_type":"markdown","source":"# Dataset 전체 구성"},{"metadata":{"trusted":true,"id":"PiSdmi1beHyI","outputId":"277b60f3-c8d7-4f17-d6a6-53ac2eb40d38"},"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nfrom transformers import BertTokenizer\n# embedder download...\nembedder = SentenceTransformer('xlm-r-large-en-ko-nli-ststb')","execution_count":20,"outputs":[{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n100%|██████████| 1.80G/1.80G [08:32<00:00, 3.51MB/s] \n","name":"stderr"}]},{"metadata":{"trusted":true,"id":"e1FGpEuJeHyJ"},"cell_type":"code","source":"BATCH_SIZE = 64\nBATCH_COUNT = 50","execution_count":21,"outputs":[]},{"metadata":{"trusted":true,"id":"bhOs5GKNeHyK","outputId":"367b9c9f-b6fb-4aee-9aad-8c739e3092e7"},"cell_type":"code","source":"#dataset 다시 만듦\n\norg_text_emb = embedder.encode([org_text])[0]\nprint(f'Text embedding shape : {org_text_emb.shape}')\ndataset = []\nfor i in range(BATCH_COUNT):\n    emb_batch_set = []\n    cod_batch_set = []\n    for j in range(BATCH_SIZE):\n        emb_batch_set.append(org_text_emb)\n        cod_batch_set.append(ko_grammar_set[BATCH_SIZE*i+j])\n\n    emb_batch_set = np.asarray(emb_batch_set)\n    cod_batch_set = np.asarray(cod_batch_set)\n    dataset.append((emb_batch_set,cod_batch_set))\n\nprint(f'Total dataset count :{BATCH_COUNT*BATCH_SIZE}')","execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Batches', max=1.0, style=ProgressStyle(description_width=…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fad87a71871b4097a7d25f32a1941675"}},"metadata":{}},{"output_type":"stream","text":"\nText embedding shape : (1024,)\nTotal dataset count :3200\n","name":"stdout"}]},{"metadata":{"trusted":true,"id":"VxfQxFQKeHyL","outputId":"d3fb207f-8ea6-4b53-abf3-7b74dbb66a07"},"cell_type":"code","source":"# 30번째 배치의 형태소코드셋 중 20번째꺼 확인\nprint(dataset[30][1][20])\nprint(dataset[31][1][20])","execution_count":23,"outputs":[{"output_type":"stream","text":"[2 9 2 5 2 2 5 6 2 2 5 6 2 2 5 2 5 6 3 2 5 2 5 2 6 2 5 6 2 5 6 6 2 8 3 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n[ 2  2  5  2  5  2  5  2  2  2  5  2  8  3  2  5  2  6  6  2  5  2  5  3\n 11  3 11  3 11  3 11  3 11  3 11  3 11  3  2  2  5  2  5  6  2  6  2  5\n  6  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0]\n","name":"stdout"}]},{"metadata":{"id":"NlM5tdo1eHyL"},"cell_type":"markdown","source":"# Generator 구현"},{"metadata":{"trusted":true,"id":"jopXmV8DeHyM","outputId":"e63ac1be-2578-41ea-8bbe-899105dc0912"},"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts([org_text])\ntotal_words = len(tokenizer.word_index)\n\nprint(f'Total token count of origin text : {total_words}')","execution_count":228,"outputs":[{"output_type":"stream","text":"Total token count of origin text : 394\n","name":"stdout"}]},{"metadata":{"id":"1DT4gKRYeHyN"},"cell_type":"markdown","source":"원문을 40개의 token 으로 구성된 문장으로 요약 하는것....<br>\n향후 LSTM으로 G 구성 필요"},{"metadata":{"trusted":true,"id":"P_cpFugJeHyN"},"cell_type":"code","source":"\n_MAX_GEN_TOKEN = 40\n_NOISE_DIM = total_words\n\nword_table = {}\n\nfor word,index in tokenizer.word_index.items():\n    word_table[index-1] = word\n\ncurrent_token_len = len(word_table)\n\n","execution_count":229,"outputs":[]},{"metadata":{"trusted":true,"id":"Jg3YCjGJeHyO","outputId":"d31c895e-f244-49bb-843a-a741f06d8535"},"cell_type":"code","source":"print('Token table of origin text')\nprint('---------------------------------------------')\nprint(' Code         Token      ')\nprint('')\nfor k in word_table.keys():\n  print( f'  {str(k).ljust(8)}    {word_table[k]}')\nprint('---------------------------------------------')\n","execution_count":230,"outputs":[{"output_type":"stream","text":"Token table of origin text\n---------------------------------------------\n Code         Token      \n\n  0           원내대표는\n  1           주\n  2           것\n  3           이라고\n  4           며\n  5           문재인\n  6           않는\n  7           문\n  8           이라며\n  9           이제\n  10          풀들이\n  11          수\n  12          대통령과\n  13          공수처는\n  14          될\n  15          있는\n  16          향해\n  17          보인다\n  18          짓밟힌\n  19          국민적\n  20          저항에\n  21          직면할\n  22          정권이\n  23          위해\n  24          공수처법을\n  25          제게\n  26          대통령은\n  27          '공수처는\n  28          없는\n  29          야당이\n  30          공수처장\n  31          사람들이\n  32          민주당\n  33          그러면서\n  34          공수처가\n  35          청와대와\n  36          왜\n  37          그\n  38          인정하지\n  39          이\n  40          공수처를\n  41          권력\n  42          게\n  43          정권의\n  44          민주당이\n  45          국회에서\n  46          그걸\n  47          권력은\n  48          먼저\n  49          주호영\n  50          국민의힘\n  51          22일\n  52          고위공직자범죄수사처\n  53          공수처\n  54          법\n  55          개정과\n  56          가덕도\n  57          신공항\n  58          건설\n  59          등을\n  60          밀어붙이고\n  61          정권과\n  62          더불어민주당을\n  63          끝이\n  64          아우성\n  65          치는\n  66          경고했다\n  67          이날\n  68          자신의\n  69          페이스북에\n  70          공수처법\n  71          개정을\n  72          위한\n  73          '군사작전'에\n  74          돌입하겠다고\n  75          엄포를\n  76          놓고\n  77          있다\n  78          정의당을\n  79          끌어들이기\n  80          꼼수\n  81          선거법에\n  82          묶어\n  83          '패스트트랙'이라는\n  84          불법·탈법으로\n  85          만들어낸\n  86          시행도\n  87          해보지\n  88          않고\n  89          고치려\n  90          하는\n  91          지적했다\n  92          이어\n  93          야당\n  94          원내대표인\n  95          사람\n  96          좋아보이는\n  97          표정으로\n  98          야당의\n  99          동의\n  100         없이는\n  101         절대\n  102         출범할\n  103         것'이라고\n  104         얘기했고\n  105         유엔\n  106         안보리\n  107         상임이사국처럼\n  108         임명에\n  109         '비토권'을\n  110         행사할\n  111         있는데\n  112         무얼\n  113         걱정하느냐고\n  114         여당\n  115         우리를\n  116         속였다\n  117         거짓말이라는\n  118         비난을\n  119         개의치\n  120         사람들\n  121         꼬집었다\n  122         이해찬\n  123         전\n  124         대표가\n  125         얘기한\n  126         '민주당\n  127         20년\n  128         집권'의\n  129         토대가\n  130         올해\n  131         안에\n  132         완성된다\n  133         탈원전과\n  134         동남권\n  135         신공항은\n  136         대통령이\n  137         대선\n  138         공약으로\n  139         내건\n  140         사업이니\n  141         여기에\n  142         불법이\n  143         있었다고\n  144         시비를\n  145         거는\n  146         것은\n  147         민주주의를\n  148         부정하는\n  149         것이라고\n  150         청와대\n  151         출신\n  152         윤건영\n  153         의원이\n  154         윽박지른다\n  155         '민주주의\n  156         민주당'이\n  157         법위에\n  158         군림하는\n  159         '반민주'를\n  160         거리낌없이\n  161         획책하는\n  162         언급했다\n  163         표를\n  164         얻기\n  165         나라\n  166         곳간을\n  167         다\n  168         허물어뜨렸고\n  169         재정\n  170         운용에서\n  171         신중함은\n  172         사라졌다\n  173         괴물\n  174         출범하면\n  175         공무원\n  176         누구나\n  177         권력이\n  178         지시하는\n  179         범죄행위에\n  180         거리낌\n  181         없이\n  182         가담할\n  183         것이다\n  184         권부\n  185         요직에\n  186         앉아\n  187         불법으로\n  188         각종\n  189         이권을\n  190         챙기는\n  191         권력자들에\n  192         대한\n  193         사건이\n  194         불거져도\n  195         사건을\n  196         가져가\n  197         버리면\n  198         그만\n  199         우려했다\n  200         고위\n  201         공직자들을\n  202         처벌하는\n  203         것인데\n  204         반대하는지\n  205         이해할\n  206         없다'고\n  207         했는데\n  208         그런\n  209         분이\n  210         대통령\n  211         주변을\n  212         감시하는\n  213         특별감찰관은\n  214         취임\n  215         이후\n  216         지금까지\n  217         임명하지\n  218         않았는가\n  219         라며\n  220         권력형\n  221         비리의\n  222         쓰레기\n  223         하치장\n  224         종말\n  225         처리장이\n  226         비판했다\n  227         정부를\n  228         사도들은\n  229         법치가\n  230         미치지\n  231         무오류의\n  232         화신이\n  233         오류를\n  234         존재가\n  235         바로\n  236         신이며\n  237         아래에는\n  238         자신들의\n  239         지도자를\n  240         목숨바쳐\n  241         지킴으로서\n  242         정의를\n  243         실현하겠다는\n  244         추종자들로\n  245         넘쳐\n  246         난다\n  247         지도자의\n  248         신성을\n  249         세력을\n  250         정죄하는\n  251         수단으로\n  252         전락할\n  253         질타했다\n  254         저도\n  255         법조인이지만\n  256         공수처장이\n  257         마음대로\n  258         검사들과\n  259         수사관들을\n  260         임명하는\n  261         끔찍한\n  262         사법기구가\n  263         어떤\n  264         일을\n  265         할지\n  266         두렵기만\n  267         하다\n  268         검찰과\n  269         경찰\n  270         위에\n  271         사법기구로\n  272         헌법과\n  273         법으로\n  274         독립성을\n  275         보장하는\n  276         검찰총장을\n  277         이렇게\n  278         핍박하는\n  279         어떻게\n  280         운영할지\n  281         불을\n  282         보듯\n  283         뻔한\n  284         일\n  285         예측했다\n  286         추미애\n  287         법무장관을\n  288         앞장\n  289         세워\n  290         윤석열\n  291         검찰의\n  292         비리\n  293         수사를\n  294         저지하려다가\n  295         난관에\n  296         봉착하자\n  297         무슨\n  298         수를\n  299         써서라도\n  300         출범시키려\n  301         한다\n  302         자리에는\n  303         추미애보다\n  304         더\n  305         한\n  306         막무가내\n  307         내\n  308         편을\n  309         앉힐\n  310         분명한\n  311         파렴치와\n  312         오만함을\n  313         최전선에서\n  314         온\n  315         몸으로\n  316         겪어온\n  317         저로서는\n  318         내일부터\n  319         보일\n  320         행태가\n  321         환히\n  322         180석의\n  323         또\n  324         군사작전을\n  325         개시하면\n  326         누가\n  327         막겠는가\n  328         라고\n  329         성토했다\n  330         막을\n  331         힘이\n  332         우리\n  333         야당에게는\n  334         없다\n  335         삭발하고\n  336         장외투쟁해\n  337         봐야\n  338         눈\n  339         하나\n  340         깜짝할\n  341         아닌\n  342         대란대치\n  343         大亂大治\n  344         세상을\n  345         온통\n  346         혼돈\n  347         속으로\n  348         밀어넣고\n  349         유지에\n  350         이용한다는\n  351         통치기술\n  352         규탄했다\n  353         아울러\n  354         바람\n  355         국민은\n  356         풀이다\n  357         바람이\n  358         불면\n  359         청보리\n  360         밭의\n  361         보리가\n  362         눕는다\n  363         다시는\n  364         일어서지\n  365         못하도록\n  366         풀을\n  367         짓밟지만\n  368         풀들은\n  369         다시\n  370         일어난다\n  371         시인\n  372         김수영은\n  373         '바람보다\n  374         눕지만\n  375         바람보다\n  376         일어나는'\n  377         민초의\n  378         힘을\n  379         노래했다\n  380         고\n  381         말했다\n  382         마지막으로\n  383         정권은\n  384         곧\n  385         광장에서\n  386         일어서서\n  387         아우성치는\n  388         모습을\n  389         지켜보게\n  390         대란대치를\n  391         끝장내려는\n  392         거듭\n  393         강조했다\n---------------------------------------------\n","name":"stdout"}]},{"metadata":{"trusted":true,"id":"bZgNu7SYeHyP"},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import (Input,\n                                     Dense, \n                                     BatchNormalization, \n                                     LeakyReLU,\n                                     Softmax,\n                                     Reshape, \n                                     Conv2DTranspose,\n                                     Conv2D,\n                                     Dropout,\n                                     Flatten,\n                                     Concatenate,\n                                     Lambda)\nimport matplotlib.pyplot as plt","execution_count":29,"outputs":[]},{"metadata":{"id":"k6Vj9qaaeHyQ"},"cell_type":"markdown","source":"noise를 token_table을 통해 text로 변환하는 변환기 구현"},{"metadata":{"trusted":true,"id":"72FRz_bDeHyR"},"cell_type":"code","source":"import sys\n\ndef to_text(w):\n\n    #tf.print(w)\n    texts = []\n    try:\n        s_w = tf.sort(w,axis=-1,direction='DESCENDING')\n        #print(s_w)\n        for w_0r,s_w_0r in zip(w,s_w):\n            text = \"\"\n            wh = tf.where(tf.math.greater(w_0r,s_w_0r[_MAX_GEN_TOKEN]))\n\n            for k in tf.reshape(wh,(_MAX_GEN_TOKEN,)).numpy():\n                try:\n                    text += word_table[k]+' '\n                except Exception as ex:\n                    tf.print('to_text : word_table' + str(ex),k)\n                \n            texts.append(text)\n    except Exception as ex:\n        tf.print('to_text : ' + str(ex),sys.exc_info())\n\n    return tf.constant(texts,dtype=tf.string)","execution_count":234,"outputs":[]},{"metadata":{"trusted":true,"id":"ywMSg5noeHyS","outputId":"a08a244b-497a-4b6e-b1b5-5906f7ce7753"},"cell_type":"code","source":"# to_text 함수의 test\nw = tf.random.normal([3,_NOISE_DIM])\nprint(w.shape)\ne = to_text(w)\nfor t in e:\n    print(t.numpy().decode('utf-8'))","execution_count":236,"outputs":[{"output_type":"stream","text":"(3, 394)\n공수처는 공수처가 게 그걸 등을 자신의 지적했다 표정으로 상임이사국처럼 있는데 전 토대가 대통령이 윤건영 이권을 반대하는지 지금까지 법치가 무오류의 오류를 추종자들로 넘쳐 전락할 법조인이지만 할지 법으로 독립성을 검찰총장을 운영할지 수사를 추미애보다 더 180석의 군사작전을 막겠는가 풀이다 풀을 시인 일어나는' 일어서서 \n공수처는 향해 위해 '공수처는 없는 사람들이 공수처를 22일 경고했다 놓고 만들어낸 해보지 않고 출범할 상임이사국처럼 시비를 것이라고 윤건영 의원이 언급했다 불거져도 가져가 않았는가 오류를 자신들의 지킴으로서 난다 지도자의 신성을 마음대로 사법기구가 추미애 저지하려다가 써서라도 또 온통 바람 불면 밭의 못하도록 \n이라며 위해 민주당이 먼저 가덕도 등을 엄포를 이어 표정으로 없이는 행사할 '민주당 것은 민주주의를 거리낌없이 공무원 앉아 우려했다 분이 주변을 지금까지 비리의 비판했다 저도 공수처장이 수사관들을 위에 헌법과 핍박하는 일 파렴치와 겪어온 행태가 라고 삭발하고 이용한다는 아울러 짓밟지만 민초의 광장에서 \n","name":"stdout"}]},{"metadata":{"id":"rGwa9iaPeHyT"},"cell_type":"markdown","source":"생성된 text의 embedding 변환기 구현<br>\nembedding은 org_text의 embedding과 비교하여 원문과 유사하게 민들기 위한 목적"},{"metadata":{"trusted":true,"id":"_D7qcRmQeHyX"},"cell_type":"code","source":"# 이거 이틀걸림...잘 몰라서 ㅈㄴ 헤맴\n\n@tf.custom_gradient\ndef to_embedding(w):\n\n    def grad(dy):\n        dy = tf.nn.batch_normalization(dy, 0, tf.math.reduce_std(dy), 0, 4, 1e-3)  # gradient 증폭 \n        dy_arr = tf.reshape(dy,(dy.shape[0],1024,1))\n        #tf.print(dy_arr)\n        dy_arr_st = tf.image.resize(dy_arr, size=(dy.shape[0],_MAX_LENGTH*_MAX_TOKEN),method=tf.image.ResizeMethod.BILINEAR)\n        dy_arr_st = tf.reshape(dy_arr_st,shape=(dy.shape[0],_MAX_LENGTH,_MAX_TOKEN))\n        return dy_arr_st\n\n    #print(w)    \n    texts = []\n    value = None\n    texts = []\n    try:\n        s_w = tf.sort(w,axis=-1,direction='DESCENDING')\n        #print(s_w)\n        for w_0r,s_w_0r in zip(w,s_w):\n            text = \"\"\n            wh = tf.where(tf.math.greater(w_0r,s_w_0r[_MAX_GEN_TOKEN]))\n            for k in tf.reshape(wh,(_MAX_GEN_TOKEN,)).numpy():\n                text += word_table[k]+' '\n                 \n            texts.append(text)                 \n        value = tf.constant(embedder.encode(texts,show_progress_bar=False),dtype=tf.float32)\n    except Exception as ex:\n        tf.print(ex,sys.exc_info())\n\n    return value, grad","execution_count":63,"outputs":[]},{"metadata":{"trusted":true,"id":"9Pu18GvweHyY","outputId":"90d28ab3-5d23-41e9-ed12-bb1b5be5a3e6"},"cell_type":"code","source":"# to_embedding 함수의 test\ne = to_embedding(w)\nfor t in e:\n    print(t.numpy())","execution_count":71,"outputs":[{"output_type":"stream","text":"[ 0.49004325 -0.2691142   0.6026438  ... -0.16845913 -0.03569949\n -0.00082758]\n[ 0.35038516 -0.4458071   0.5340477  ... -0.5159919  -0.03732621\n  0.25392553]\n","name":"stdout"}]},{"metadata":{"trusted":true,"id":"DWTe53wueHya"},"cell_type":"code","source":"# to_compression_ratio\n\n@tf.custom_gradient\ndef to_compression_ratio(w):\n    def grad(dy):\n        dy = tf.nn.batch_normalization(dy, 0, tf.math.reduce_std(dy), 0, 4, 1e-3)  # gradient 증폭, 이래도 되는 건지....\n        dy_arr = tf.reshape(dy,(dy.shape[0],1,1))\n        #tf.print(dy_arr)\n        dy_arr_st = tf.image.resize(dy_arr, size=(dy.shape[0],_MAX_LENGTH*_MAX_TOKEN),method=tf.image.ResizeMethod.BILINEAR)\n        dy_arr_st = tf.reshape(dy_arr_st,shape=(dy.shape[0],_MAX_LENGTH,_MAX_TOKEN))        \n        return dy_arr_st\n\n    const = tf.math.reduce_sum(tf.constant([x for x in range(tf.size(w[0]))],tf.int64)) \n    #print(const)\n    value = None\n    compression_ratio = [] #tf.Variable()\n    texts = []\n    try:\n        s_w = tf.sort(w,axis=-1,direction='DESCENDING')\n        #print(s_w)\n        for w_0r,s_w_0r in zip(w,s_w):\n            wh = tf.where(tf.math.greater(w_0r,s_w_0r[_MAX_GEN_TOKEN]))\n            cr = tf.math.reduce_sum(tf.reshape(wh,(_MAX_GEN_TOKEN,)))\n            compression_ratio.append(cr.numpy()/const.numpy())\n        #print(compression_ratio)\n        value = tf.constant(compression_ratio,dtype=tf.float32)\n    except Exception as ex:\n        tf.print(ex,sys.exc_info())\n\n    return value, grad\n    ","execution_count":147,"outputs":[]},{"metadata":{"trusted":true,"id":"Rk25esbweHyd","outputId":"a737b732-8f3c-4bbd-80b9-2ad85392278b"},"cell_type":"code","source":"# to_compression_ratio 함수의 test\ne = to_compression_ratio(w)\nfor t in e:\n    print(t.numpy())","execution_count":148,"outputs":[{"output_type":"stream","text":"0.09176894\n0.10754996\n","name":"stdout"}]},{"metadata":{"id":"JxlFMo3GeHyg"},"cell_type":"markdown","source":"생성된 text의 morpheme code 변환기 구현<br>\nmorpheme code는 한국어 문장들(dataset)의 morpheme code와 비교하여 한국어 문법에 가깝게 만들기 위한 목적"},{"metadata":{"trusted":true,"id":"yhOeXv93eHyh"},"cell_type":"code","source":"\n@tf.custom_gradient\ndef to_morpcoding(w):\n\n    def grad(dy):\n        dy = tf.nn.batch_normalization(dy, 0, tf.math.reduce_std(dy), 0, 4, 1e-3)  # gradient 증폭, 이래도 되는 건지....\n        dy_arr = tf.reshape(dy,(dy.shape[0],_MAX_MORP_LENGTH,1))\n        #tf.print(dy_arr)\n        dy_arr_st = tf.image.resize(dy_arr, size=(dy.shape[0],_MAX_LENGTH*_MAX_TOKEN),method=tf.image.ResizeMethod.BILINEAR)\n        dy_arr_st = tf.reshape(dy_arr_st,shape=(dy.shape[0],_MAX_LENGTH,_MAX_TOKEN))\n        return dy_arr_st\n\n    #print(w)    \n    codes = []\n    value = None\n    texts = []\n    try:\n        s_w = tf.sort(w,axis=-1,direction='DESCENDING')\n        #print(s_w)\n        for w_0r,s_w_0r in zip(w,s_w):\n            text = \"\"\n            wh = tf.where(tf.math.greater(w_0r,s_w_0r[_MAX_GEN_TOKEN]))\n            for k in tf.reshape(wh,(_MAX_GEN_TOKEN,)).numpy():\n                text += word_table[k]+' '\n                 \n            texts.append(text)    \n            \n        for sentence in texts:\n            code = morpheme_encode(sentence)\n            if len(code) <= _MAX_MORP_LENGTH:\n                codes.append(code + [_PADDING_CODE for i in range(_MAX_MORP_LENGTH-len(code))])\n            else:\n                codes.append(code[:_MAX_MORP_LENGTH])\n        value = tf.constant(codes,dtype=tf.int32)\n    except Exception as ex:\n        tf.print(ex,sys.exc_info())\n\n    return value, grad","execution_count":149,"outputs":[]},{"metadata":{"trusted":true,"id":"S-C_pxE5eHyi","outputId":"823880d0-07c1-4b15-e07e-62c9f5ab7e2b"},"cell_type":"code","source":"# to_morpcoding 함수의 test\ne = to_morpcoding(w)\nfor t in e:\n    print(t.numpy())","execution_count":150,"outputs":[{"output_type":"stream","text":"[ 2  2  9  2  5  2  5  7  7  2  2  5  7  7  2  2  5  2  6  3  2  2  3  5\n  6  2  2  3  5  2  5  6  2  3  2  2  2  9  5  3  7  2  3  2  2  5  2  5\n  2  5  7  2  5  2  5  6  2  6  2  5  7  7  2  9  2  9  5  2  6  6  2 10\n  8  8  2  5  6  6  2  2  5  6  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0]\n[ 2  2  2  9  2  2  2  2  2  5  7  7  2  6  2  5  2  5  2  2  6  8  2  5\n  3  2  2  5  2  2  2  2  2  2  6  2  7  2  9  5  2  2  5  2  5 11  2  5\n  2  8  2  5  2  2  2  2  5  2  5  2  6  2  5  2  2  5  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0]\n","name":"stdout"}]},{"metadata":{"id":"hLtq1YP_eHyj"},"cell_type":"markdown","source":"Network 구성을 위해 사용자 정의 Layer 를 구현"},{"metadata":{"trusted":true,"id":"F9EfbbSreHyk"},"cell_type":"code","source":"# 이것도 잘 몰라서 하루 걸림... ㅜㅜ\n\nfrom keras import backend as K\nfrom keras.layers import Layer\n\n#tf.executing_eagerly()\n\nclass Post_processing(Layer):\n\n    def __init__(self, output_dim, encoder_func=None,Tout=tf.float32, **kwargs):\n        self.output_dim = output_dim\n        self.encoder = encoder_func\n        self.Tout = Tout\n        super(Post_processing, self).__init__(**kwargs)\n    '''\n    def build(self, input_shape):\n        tf.print('build',input_shape)\n        # 이 레이어에 대해 학습가능한 가중치 변수를 만듭니다.\n        self.kernel = self.add_weight(name='kernel', \n                                      shape=(input_shape[1], self.output_dim),\n                                      initializer='uniform',\n                                      trainable=True)\n        super(Post_processing, self).build(input_shape)  # 끝에서 꼭 이 함수를 호출하십시오\n    '''\n    def call(self, input_data):\n        #tf.print('Post_processing : call input_data',input_data.shape)\n        value = tf.py_function(self.encoder,[input_data],Tout=self.Tout,name='encode_func')\n        #print('value.shape:',value.shape)\n        #value.set_shape((input_data.shape[0],self.output_dim))\n        if self.output_dim > 0:\n            value.set_shape((input_data.shape[0],self.output_dim))\n        else:\n            value.set_shape((input_data.shape[0],))\n        #return tf.reshape(value,[input_data.shape[0]])  \n\n        #value = tf.Variable((tf.zeros([input_data.shape[0],1024]) if self.Tout==tf.float32 else tf.zeros([input_data.shape[0],])),dtype=self.Tout,shape=( (input_data.shape[0],1024) if self.Tout==tf.float32 else (input_data.shape[0],)))\n        #tf.py_function(self.encoder,[input_data],Tout=self.Tout)\n        return value\n\n    def compute_output_shape(self, input_shape):\n        tf.print('compute_output_shape:',input_shape)\n        input_shape = tensor_shape.TensorShape(input_shape).as_list()\n        if self.output_dim > 0:\n            return tensor_shape.TensorShape([input_shape[0], self.output_dim])\n        return tensor_shape.TensorShape([input_shape[0]])","execution_count":151,"outputs":[]},{"metadata":{"trusted":true,"id":"iT--4r5BeHyk","outputId":"6316c435-1e08-4a3d-d19c-39719f19f527"},"cell_type":"code","source":"# 구성한 Layer의 test\ne = Post_processing(1024,to_embedding,Tout=tf.float32)(w)\nfor c in e:\n    print(c)","execution_count":152,"outputs":[{"output_type":"stream","text":"tf.Tensor(\n[ 0.49004325 -0.2691142   0.6026438  ... -0.16845913 -0.03569949\n -0.00082758], shape=(1024,), dtype=float32)\ntf.Tensor(\n[ 0.35038516 -0.4458071   0.5340477  ... -0.5159919  -0.03732621\n  0.25392553], shape=(1024,), dtype=float32)\n","name":"stdout"}]},{"metadata":{"trusted":true,"id":"X0RIQRZ4eHyl"},"cell_type":"code","source":"# 별로 중요하지는 않지만 Lambda layer를 활용하기 위한 assert 함수 구성\ndef assert_layer(input_data,out_dim=None):\n    #tf.print(input_data)\n    #print(input_data)\n    assert input_data.shape[1] == out_dim\n    return input_data","execution_count":153,"outputs":[]},{"metadata":{"trusted":true,"id":"VJazT7y-eHyl","outputId":"8daec25a-a6b0-461e-8818-1f948e16bcae"},"cell_type":"code","source":"# 드디어 generator 구현\n# 효과적으로 구성된 것인지는 모르겠음... 이것은 아직 많은 연구가 필요함.\n# 또한 LSTM으로 바꾸어 길이의 한게를 극복해야 할 것...\n\ndef make_generator_model(org_words):\n    input = Input(shape=(org_words,), dtype='float32') \n    x1 = Dense(org_words*2, use_bias=False)(input)\n    #x1 = BatchNormalization()(x1)\n    x1 = LeakyReLU()(x1)\n    \n    x1 = Dense(org_words*4, use_bias=False)(x1)\n    #x1 = BatchNormalization()(x1)\n    x1 = LeakyReLU()(x1)\n    \n    #x1 = Dense(max_length*total_words, use_bias=False, activation='tanh')(x1)\n    x1 = Dense(org_words, use_bias=False)(x1)\n    x1 = Lambda(assert_layer,arguments={'out_dim':org_words})(x1)\n    #x1 = Reshape((max_length, total_words))(x1)\n    #x1 = BatchNormalization()(x1)\n    #x1 = Softmax()(x1)        \n    #x1 = MyCustomLayer(max_length*total_words)(x1)\n    txt = Post_processing(0,to_text,Tout=tf.string)(x1)\n    emb = Post_processing(1024,to_embedding,Tout=tf.float32)(x1)\n    cmr = Post_processing(0,to_compression_ratio,Tout=tf.float32)(x1)\n    cod = Post_processing(128,to_morpcoding,Tout=tf.int32)(x1)\n    \n    model = Model(input,[txt,emb,cmr,cod])\n    \n    model.summary()\n    return model\n\ngenerator = make_generator_model(_NOISE_DIM)","execution_count":237,"outputs":[{"output_type":"stream","text":"Model: \"functional_11\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_10 (InputLayer)           [(None, 394)]        0                                            \n__________________________________________________________________________________________________\ndense_19 (Dense)                (None, 788)          310472      input_10[0][0]                   \n__________________________________________________________________________________________________\nleaky_re_lu_14 (LeakyReLU)      (None, 788)          0           dense_19[0][0]                   \n__________________________________________________________________________________________________\ndense_20 (Dense)                (None, 1576)         1241888     leaky_re_lu_14[0][0]             \n__________________________________________________________________________________________________\nleaky_re_lu_15 (LeakyReLU)      (None, 1576)         0           dense_20[0][0]                   \n__________________________________________________________________________________________________\ndense_21 (Dense)                (None, 394)          620944      leaky_re_lu_15[0][0]             \n__________________________________________________________________________________________________\nlambda_3 (Lambda)               (None, 394)          0           dense_21[0][0]                   \n__________________________________________________________________________________________________\npost_processing_13 (Post_proces (None,)              0           lambda_3[0][0]                   \n__________________________________________________________________________________________________\npost_processing_14 (Post_proces (None, 1024)         0           lambda_3[0][0]                   \n__________________________________________________________________________________________________\npost_processing_15 (Post_proces (None,)              0           lambda_3[0][0]                   \n__________________________________________________________________________________________________\npost_processing_16 (Post_proces (None, 128)          0           lambda_3[0][0]                   \n==================================================================================================\nTotal params: 2,173,304\nTrainable params: 2,173,304\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true,"id":"sQya4kMheHym","outputId":"262e43d0-472c-4cff-d8cc-640c02c5e015"},"cell_type":"code","source":"# generator의 test\n# Create a random noise and generate a sample\nnoise = tf.random.normal([3,_NOISE_DIM])\ntexts,embeddings,compratios,morpcodes = generator(noise, training=True)\nprint(texts.shape)\nfor i,txt in zip(range(len(texts)),texts):\n    print(f\" {i+1}> {txt.numpy().decode('utf-8')}\" )\nprint(embeddings.shape)\nprint(compratios.shape)\nprint(morpcodes.shape)","execution_count":238,"outputs":[{"output_type":"stream","text":"(3,)\n 1> 며 공수처법을 야당이 공수처장 민주당 민주당이 22일 가덕도 정의당을 꼼수 만들어낸 원내대표인 출범할 유엔 비난을 완성된다 것은 없이 챙기는 이해할 없다'고 감시하는 쓰레기 화신이 존재가 자신들의 지도자를 두렵기만 일 법무장관을 자리에는 편을 군사작전을 눈 아닌 풀을 다시 시인 마지막으로 광장에서 \n 2> 짓밟힌 국회에서 권력은 건설 밀어붙이고 꼼수 선거법에 불법·탈법으로 유엔 '비토권'을 걱정하느냐고 거짓말이라는 비난을 공약으로 민주주의를 부정하는 군림하는 재정 괴물 앉아 챙기는 권력자들에 가져가 않았는가 바로 지도자를 전락할 질타했다 검찰과 세워 윤석열 봉착하자 편을 파렴치와 보일 대란대치 온통 바람 다시 일어서서 \n 3> 이제 수 될 대통령은 공수처장 그러면서 경고했다 끌어들이기 안보리 거는 것은 출신 요직에 권력자들에 버리면 것인데 대통령 않았는가 정부를 무오류의 지도자를 수단으로 할지 앞장 검찰의 봉착하자 수를 자리에는 편을 막을 없다 봐야 온통 유지에 바람이 풀을 일어난다 시인 민초의 정권은 \n(3, 1024)\n(3,)\n(3, 128)\n","name":"stdout"}]},{"metadata":{"id":"41aWytGveHyo"},"cell_type":"markdown","source":"# Discriminator 구현"},{"metadata":{"id":"dtjQGj54eHyo"},"cell_type":"markdown","source":"먼저 요약을 구분하기 위한 discriminator_summ 구현"},{"metadata":{"trusted":true,"id":"Fx3GgEXEeHyo"},"cell_type":"code","source":"# 문장에 대한 embeddings를 이용하여 org_text_emb (org_text의 embedding)과의 유사도를 계산한다.\nimport scipy\n\n@tf.custom_gradient\ndef to_similarity(w):\n\n    def grad(dy):\n        dy = tf.nn.batch_normalization(dy, 0, tf.math.reduce_std(dy), 0, 10, 1e-3)  # gradient 증폭, 이래도 되는 건지....\n        dy_arr = tf.reshape(dy,(dy.shape[0],1,1))\n        #tf.print(dy_arr)\n        dy_arr_st = tf.image.resize(dy_arr, size=(dy.shape[0],1024),method=tf.image.ResizeMethod.BILINEAR)\n        dy_arr_st = tf.reshape(dy_arr_st,shape=(dy.shape[0],1024))\n        return dy_arr_st\n\n    similarities = []\n    value = None\n    try:\n        for embedding in w:\n            distances = scipy.spatial.distance.cdist([embedding], [org_text_emb], \"cosine\")[0]\n            similarities.append(distances[0])\n            \n        value = tf.constant(similarities,dtype=tf.float32)\n    except Exception as ex:\n        tf.print(ex,sys.exc_info())\n\n    return value, grad\n","execution_count":159,"outputs":[]},{"metadata":{"trusted":true,"id":"DNIQNvIveHyp","outputId":"084fddba-1193-4bca-c170-7d42b69a0acb"},"cell_type":"code","source":"def make_discriminator_model():\n    input_emb = Input(shape=(1024,), dtype='float32') \n    #x1 = Post_processing(0,to_similarity,Tout=tf.float32)(input_emb)\n    #x1 = Reshape((1,))(x1)    \n    x1 = Dense(1024*2)(input_emb)\n    x1 = BatchNormalization()(x1)\n    x1 = LeakyReLU()(x1)\n\n    \n    input_cmp = Input(shape=(), dtype='float32') \n    imput_mrp = Input(shape=(_MAX_MORP_LENGTH,), dtype='int32')\n\n    x2 = Reshape((1,))(input_cmp)\n    x3 = Dense(256)(imput_mrp)\n    #x3 = BatchNormalization()(x3)\n    x3 = LeakyReLU()(x3)\n\n    x3 = Dense(256*3)(x3)\n    #x3 = BatchNormalization()(x3)\n    x3 = LeakyReLU()(x3)\n\n    concatted = Concatenate(axis=1)([x1, x2, x3])\n    x4 = Flatten()(concatted)\n    x4 = Dense(64, use_bias=False)(x4)\n    #x4 = BatchNormalization()(x4)\n    x4 = LeakyReLU()(x4)\n    x4 = Dense(1)(x4)\n    \n    model = Model([input_emb,input_cmp,imput_mrp],x4)\n    \n    model.summary()\n    \n    return model\n\ndiscriminator = make_discriminator_model()","execution_count":239,"outputs":[{"output_type":"stream","text":"Model: \"functional_13\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_13 (InputLayer)           [(None, 128)]        0                                            \n__________________________________________________________________________________________________\ninput_11 (InputLayer)           [(None, 1024)]       0                                            \n__________________________________________________________________________________________________\ndense_23 (Dense)                (None, 256)          33024       input_13[0][0]                   \n__________________________________________________________________________________________________\ndense_22 (Dense)                (None, 2048)         2099200     input_11[0][0]                   \n__________________________________________________________________________________________________\nleaky_re_lu_17 (LeakyReLU)      (None, 256)          0           dense_23[0][0]                   \n__________________________________________________________________________________________________\nbatch_normalization_2 (BatchNor (None, 2048)         8192        dense_22[0][0]                   \n__________________________________________________________________________________________________\ninput_12 (InputLayer)           [(None,)]            0                                            \n__________________________________________________________________________________________________\ndense_24 (Dense)                (None, 768)          197376      leaky_re_lu_17[0][0]             \n__________________________________________________________________________________________________\nleaky_re_lu_16 (LeakyReLU)      (None, 2048)         0           batch_normalization_2[0][0]      \n__________________________________________________________________________________________________\nreshape_2 (Reshape)             (None, 1)            0           input_12[0][0]                   \n__________________________________________________________________________________________________\nleaky_re_lu_18 (LeakyReLU)      (None, 768)          0           dense_24[0][0]                   \n__________________________________________________________________________________________________\nconcatenate_2 (Concatenate)     (None, 2817)         0           leaky_re_lu_16[0][0]             \n                                                                 reshape_2[0][0]                  \n                                                                 leaky_re_lu_18[0][0]             \n__________________________________________________________________________________________________\nflatten_2 (Flatten)             (None, 2817)         0           concatenate_2[0][0]              \n__________________________________________________________________________________________________\ndense_25 (Dense)                (None, 64)           180288      flatten_2[0][0]                  \n__________________________________________________________________________________________________\nleaky_re_lu_19 (LeakyReLU)      (None, 64)           0           dense_25[0][0]                   \n__________________________________________________________________________________________________\ndense_26 (Dense)                (None, 1)            65          leaky_re_lu_19[0][0]             \n==================================================================================================\nTotal params: 2,518,145\nTrainable params: 2,514,049\nNon-trainable params: 4,096\n__________________________________________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true,"id":"y7owqbg4eHyp","outputId":"0e58341c-2ab1-4af7-a566-c29ed3e30a2d"},"cell_type":"code","source":"# discriminator test\n\npredict = discriminator([embeddings,compratios,morpcodes])\nprint(predict)","execution_count":240,"outputs":[{"output_type":"stream","text":"tf.Tensor(\n[[0.14201309]\n [0.06003584]\n [0.07988645]], shape=(3, 1), dtype=float32)\n","name":"stdout"}]},{"metadata":{"id":"mmvyPAVSeHyq"},"cell_type":"markdown","source":"# GAN 을 이용한 loss 의 gradient 구현 --> 빡심"},{"metadata":{"trusted":true,"id":"ECEMH-yweHyr"},"cell_type":"code","source":"# This method returns a helper function to compute cross entropy loss\ncross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\ndef discriminator_loss(real_output, fake_output):\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n    total_loss = real_loss + fake_loss\n    return total_loss\n\ndef generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), fake_output)\n\ngenerator_optimizer = tf.keras.optimizers.Adam(1e-4)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(1e-4)","execution_count":241,"outputs":[]},{"metadata":{"trusted":true,"id":"cvBC5yS6eHys"},"cell_type":"code","source":"# 디짐\n@tf.function\ndef train_step(real_embedding,real_morpcoding):\n  \n    # 1 - Create a random noise to feed it into the model\n    # for the text generation\n    noise = tf.random.normal([BATCH_SIZE, _NOISE_DIM])\n    \n    # 2 - Generate text and calculate loss values\n    # GradientTape method records operations for automatic differentiation.\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        texts,embeddings,compratios,morpcodes = generator(noise, training=True)\n        #embeddings = generator(noise, training=True)\n        # real에 가까우려면 discriminator의 학습은 real_embedding 이 zero (0)에 가깝게 학습시켜야 함.\n        # 하지만 압축율의 개념으로는 본래는 ones (1)가 맞음.\n        real_output = discriminator([real_embedding,np.zeros(len(real_embedding)),real_morpcoding], training=True)\n        fake_output = discriminator([embeddings,compratios,morpcodes], training=True)\n\n        #tf.print('train_step : embeddings.shape=',embeddings.shape)\n        gen_loss = generator_loss(fake_output)\n        #tf.print('train_step : gen_loss=',gen_loss)\n        disc_loss = discriminator_loss(real_output, fake_output)\n        #tf.print('train_step : disc_loss=',disc_loss)\n\n    # 3 - Calculate gradients using loss values and model variables\n    # \"gradient\" method computes the gradient using \n    # operations recorded in context of this tape (gen_tape and disc_tape).\n    \n    # It accepts a target (e.g., gen_loss) variable and \n    # a source variable (e.g.,generator.trainable_variables)\n    # target --> a list or nested structure of Tensors or Variables to be differentiated.\n    # source --> a list or nested structure of Tensors or Variables.\n    # target will be differentiated against elements in sources.\n\n    # \"gradient\" method returns a list or nested structure of Tensors  \n    # (or IndexedSlices, or None), one for each element in sources. \n    # Returned structure is the same as the structure of sources.\n    \n    gradients_of_discriminator = disc_tape.gradient(disc_loss, \n                                                discriminator.trainable_variables)\n    #tf.print('train_step : gradients_of_discriminator=',gradients_of_discriminator)   \n    noise = tf.random.normal([BATCH_SIZE, _NOISE_DIM])\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        texts,embeddings,compratios,morpcodes = generator(noise, training=True)\n        #embeddings = generator(noise, training=True)\n        #real_output = discriminator(real_embedding, training=True)\n        fake_output = discriminator([embeddings,compratios,morpcodes], training=True)\n\n        #tf.print('train_step : embeddings.shape=',embeddings.shape)\n        gen_loss = generator_loss(fake_output)\n        #tf.print('train_step : gen_loss=',gen_loss)\n        #disc_loss = discriminator_loss(real_output, fake_output)\n        #tf.print('train_step : disc_loss=',disc_loss)\n\n    gradients_of_generator = gen_tape.gradient(gen_loss, \n                                               generator.trainable_variables)\n    #tf.print('train_step : gradients_of_generator=',gradients_of_generator)\n \n\n    # 4 - Process  Gradients and Run the Optimizer\n    # \"apply_gradients\" method processes aggregated gradients. \n    # ex: optimizer.apply_gradients(zip(grads, vars))\n    \"\"\"\n    Example use of apply_gradients:\n    grads = tape.gradient(loss, vars)\n    grads = tf.distribute.get_replica_context().all_reduce('sum', grads)\n    # Processing aggregated gradients.\n    optimizer.apply_gradients(zip(grads, vars), experimental_aggregate_gradients=False)\n    \"\"\"\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n    #tf.print('train_step : after discriminator_optimizer')    ","execution_count":242,"outputs":[]},{"metadata":{"trusted":true,"id":"cJs3lBpJeHys"},"cell_type":"code","source":"EPOCHS = 5\n# 요약문 생성의 확인을 위해 10개의 문장을 생성하고 train과정에서 각 epoch마다 변화를 확인한다.\nseed = tf.random.normal([10, _NOISE_DIM])","execution_count":243,"outputs":[]},{"metadata":{"trusted":true,"id":"os3b0psFeHyt","outputId":"e756080b-9cbd-4d03-ed82-513372923666"},"cell_type":"code","source":"# 생성된 문장의 원문 유사도를 측정하기 위한 함수\n\nimport scipy\n#print(doc_emb)\ndef similarity_score(queries,org_embedding):\n\n    total_score = 0\n    query_embeddings = embedder.encode(queries,show_progress_bar=False)\n    for query, query_embedding in zip(queries, query_embeddings):\n        distances = scipy.spatial.distance.cdist([query_embedding], [org_embedding], \"cosine\")[0]\n        results = zip(range(len(distances)), distances)\n        for idx, distance in results:\n            total_score += 1-distance\n    return total_score\n\nqueries = []\n\ntexts,embeddings,compratios,morpcodes = generator(seed,training=False)\n\n#count = 0\nfor t in texts:\n    summary_text = t.numpy().decode('utf-8')\n    queries.append(summary_text)\nprint('Similarity score:',str(similarity_score(queries,org_text_emb)))\n","execution_count":245,"outputs":[{"output_type":"stream","text":"Similarity score: 6.930801350407938\n","name":"stdout"}]},{"metadata":{"trusted":true,"id":"cXrsOXX4eHyt"},"cell_type":"code","source":"# Print iterations progress\nclass ProgressBar:\n    # pb = ProgressBar(total=20, prefix = 'Epoch 1')\n    def __init__(self,total=20, prefix = '', suffix = '', decimals = 1, length = 20, fill = '█', printEnd = \"\\r\"):\n        self.total = total\n        self.prefix = prefix\n        self.suffix = suffix\n        self.decimals = decimals\n        self.length = length\n        self.fill = fill\n        self.printEnd = printEnd\n        self.ite = 0\n    # pb.printProgress(1,'~~~~')\n    def printProgress(self,iteration, text):\n        self.ite += iteration\n        percent = (\"{0:.\" + str(self.decimals) + \"f}\").format(100 * (self.ite / float(self.total)))\n\n        filledLength = int(self.length * self.ite // self.total)\n        bar = self.fill * filledLength + '-' * (self.length - filledLength)\n        print(f'\\r{self.prefix} |{bar}| {percent}% {self.suffix}  {text}', end=\"\", flush=True)\n        # Print New Line on Complete\n        if self.ite == self.total: \n            print()","execution_count":246,"outputs":[]},{"metadata":{"trusted":true,"id":"dhqdaIATeHyu"},"cell_type":"code","source":"import time\nfrom IPython import display # A command shell for interactive computing in Python.\nimport re\n\ndef train(dataset, epochs):\n    print('Start with seed text.')\n    seed = tf.random.normal([10, _NOISE_DIM])\n    print('-------------------------------------------------------')\n    texts,embeddings,compratios,morpcodes = generator(seed,training=False)\n    for i,t in  zip(range(len(texts)),texts):\n        summary_text = t.numpy().decode('utf-8')\n        print(f'{i+1} > {summary_text}')        \n    print('-------------------------------------------------------')\n    print('')\n\n    # A. For each epoch, do the following:\n    for epoch in range(epochs):\n        start = time.time()\n        pb = ProgressBar(total=BATCH_COUNT, prefix = f'Epoch:{str(epoch+1)}/{epochs}')\n        pb.printProgress(0,'Start batch.')\n        # 1 - For each batch of the epoch, \n        for batch_num,(emb_batch_set,cod_batch_set) in zip(range(len(dataset)),dataset):\n            # 1.a - run the custom \"train_step\" function\n            # we just declared above\n            #print(image_batch.shape)\n            train_step(emb_batch_set,cod_batch_set)\n            texts,embeddings,compratios,morpcodes = generator(seed,training=False)\n            pb.printProgress(+1,f\"Time for batch {batch_num + 1}/{BATCH_COUNT} is {int(time.time()-start)} sec, generated text:{texts[0].numpy().decode('utf-8')}\")\n        # 4 - Print out the completed epoch no. and the time spent\n        #print (f'Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n        #texts,embeddings,compratios,morpcodes = generator(seed,training=False)\n        count = 0\n        queries = []\n        texts,embeddings,compratios,morpcodes = generator(seed,training=False)\n        for i,t in  zip(range(len(texts)),texts):\n            summary_text = t.numpy().decode('utf-8')\n            print(f'{i+1} > {summary_text}')\n            queries.append(summary_text)\n            c = [m.start() for m in re.finditer(_MISMATCH_WORD, summary_text)]\n            count += len(c)\n        print(f'Mismatch count:{count} Similarity score:{str(similarity_score(queries,org_text_emb))}')\n        print('')","execution_count":247,"outputs":[]},{"metadata":{"id":"RCAedaSeT6A1","trusted":false},"cell_type":"code","source":"EPOCHS = 5\ntrain(dataset, EPOCHS)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled11.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO6ryrPtWjlXPV0fYUoBM7e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dolmani38/Summary/blob/master/real_summary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vz7O2Dt-VoI0"
      },
      "source": [
        "##Beginners Guide to Text Generation using LSTMs\r\n",
        "https://www.kaggle.com/shivamb/beginners-guide-to-text-generation-using-lstms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1m66KMMVs-P"
      },
      "source": [
        "# keras module for building LSTM \r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.callbacks import EarlyStopping\r\n",
        "from keras.models import Sequential\r\n",
        "import keras.utils as ku \r\n",
        "\r\n",
        "# set seeds for reproducability\r\n",
        "from tensorflow.random import set_seed\r\n",
        "from numpy.random import seed\r\n",
        "set_seed(2)\r\n",
        "seed(1)\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import string, os \r\n",
        "\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings(\"ignore\")\r\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFOzAn2BXr8m"
      },
      "source": [
        "Text dataset을 만들기 위해 위키백과에서 스크랩핑..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SG3xfXYTW4Us",
        "outputId": "62213ba5-973f-46ea-8380-e5593597d442"
      },
      "source": [
        "!pip install wikipedia\r\n",
        "import wikipedia as wiki\r\n",
        "wiki.set_lang('ko')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading https://files.pythonhosted.org/packages/67/35/25e68fbc99e672127cc6fbb14b8ec1ba3dfef035bf1e4c90f78f24a80b7d/wikipedia-1.4.0.tar.gz\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-cp36-none-any.whl size=11686 sha256=47da6791ac7576c2f87c5490c9aca14148a4c1f8add38645debe0017f7e41220\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/2a/18/4e471fd96d12114d16fe4a446d00c3b38fb9efcb744bd31f4a\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJophhN5V-JA",
        "outputId": "8e9a7536-cdc6-45b3-d973-9473a21624ed"
      },
      "source": [
        "def __search_from_wiki(question,max_rank):\r\n",
        "  results = wiki.search(question,results=max_rank)\r\n",
        "  print(results)\r\n",
        "  contents = []\r\n",
        "  for result in results:\r\n",
        "    try:\r\n",
        "      page = wiki.page(result)\r\n",
        "      #print(f\"Top wiki result: {page}\")\r\n",
        "      text = page.content\r\n",
        "      ln = len(text)\r\n",
        "      print(ln)\r\n",
        "      if ln < 4000:\r\n",
        "        contents.append(text)\r\n",
        "      else:\r\n",
        "        contents.append(text[0:4000])\r\n",
        "    except Exception as ex:\r\n",
        "      print(ex)\r\n",
        "  return contents\r\n",
        "\r\n",
        "\r\n",
        "all_headlines = __search_from_wiki(\"전래동화\", 100)\r\n",
        "\r\n",
        "len(all_headlines)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['동화', '신 전래동화', '꾸러기 수비대', '호시조라 미유키', '아동 문학', '한국의 사찰', '해와 달이 된 오누이', '거북', '잠자는 숲속의 미녀', '이한갈', '정은찬', '김기두 (배우)', '옛날 옛적에 (애니메이션)', '계룡선녀전 (드라마)', '동요', '이상훈 (1976년)', '최홍일', '육진수 (격투기 선수)', '제비', '장석현 (연예인)', '유다미', '박재훈 (배우)', '최지웅', '밀교 (불교)', '한다은', '재희', '정미남', '안젤리나 다닐로바', '도깨비', '기탄교육', '국지용', '의왕백운호수축제', '박민경', '정정아', '콩딱쿵! 이야기 주머니', '윤기원 (배우)', '선녀와 나무꾼', '남생이', '대장화홍련전', '콩쥐팥쥐 (동음이의)', '토끼전', '미녀와 야수', '지대한', '이설구', '콩쥐팥쥐전', '이서휴게소', '은비까비의 옛날옛적에', '빨간 자전거', '김덕현 (배우)', '홍석연', '아시리아인', '티베트', '백조의 호수', '도교', '서예', '켈트 다신교', '금도끼 은도끼', '장화, 홍련 (동음이의)', '제네시스 (밴드)', '허구 국가', '노상현', '타이완의 문화', '도깨비 (동음이의)', '골디락스', '선녀강림', '자와어', '손춘익', '안동국제탈춤페스티벌', '윌리엄 버틀러 예이츠', '성덕대왕신종', '고려', '모모타로', '한국 문학', '라푼젤 (영화)', '토비트', '홍석천', '조선 후기의 문학', '송월동 동화마을', '계룡선녀전', '이집트', '응우옌 왕조', '아시아의 역사', '안지환', '외래어', '메이플 월드', '프랑스인', '스키타이족', '인도네시아', '이탈리아', '네버랜드', 'MBC 창작동요제', '프랑크인', '일본 제국', '김환영 (작가)', '드랑 나흐 오스텐', '조선', '바이킹', '오윤 (화가)', '원나라', '앱북']\n",
            "685\n",
            "156\n",
            "1554\n",
            "1601\n",
            "678\n",
            "885\n",
            "392\n",
            "1377\n",
            "2037\n",
            "776\n",
            "1096\n",
            "1767\n",
            "1080\n",
            "1416\n",
            "1429\n",
            "831\n",
            "1917\n",
            "749\n",
            "1125\n",
            "576\n",
            "409\n",
            "1931\n",
            "712\n",
            "4379\n",
            "634\n",
            "1229\n",
            "970\n",
            "879\n",
            "3171\n",
            "245\n",
            "420\n",
            "272\n",
            "885\n",
            "1119\n",
            "122\n",
            "2555\n",
            "173\n",
            "502\n",
            "216\n",
            "\"콩쥐팥쥐 (동음이의)\" may refer to: \n",
            "콩쥐팥쥐\n",
            "콩쥐팥쥐 (1967년 영화)\n",
            "콩쥐팥쥐 (1958년 영화)\n",
            "3931\n",
            "2977\n",
            "3296\n",
            "3021\n",
            "2912\n",
            "808\n",
            "940\n",
            "2955\n",
            "1986\n",
            "3008\n",
            "2793\n",
            "5093\n",
            "2130\n",
            "6686\n",
            "10102\n",
            "4308\n",
            "1259\n",
            "\"장화, 홍련 (동음이의)\" may refer to: \n",
            "장화홍련전\n",
            "장화, 홍련\n",
            "장화, 홍련\n",
            "5457\n",
            "2702\n",
            "351\n",
            "1581\n",
            "\"도깨비 (동음이의)\" may refer to: \n",
            "도깨비\n",
            "도깨비\n",
            "눈물을 마시는 새\n",
            "레인보우 식스 시즈\n",
            "도깨비가 간다\n",
            "도깨비가 간다\n",
            "Tokebi\n",
            "도깨비 (DokeV)\n",
            "1195\n",
            "1856\n",
            "3770\n",
            "1687\n",
            "2123\n",
            "12057\n",
            "1934\n",
            "30322\n",
            "1980\n",
            "14908\n",
            "9736\n",
            "9004\n",
            "8492\n",
            "5280\n",
            "896\n",
            "2268\n",
            "11797\n",
            "14070\n",
            "11513\n",
            "12107\n",
            "5000\n",
            "5895\n",
            "12556\n",
            "30920\n",
            "30608\n",
            "42736\n",
            "8303\n",
            "1724\n",
            "23128\n",
            "15259\n",
            "4105\n",
            "4331\n",
            "30998\n",
            "19194\n",
            "2772\n",
            "15537\n",
            "4534\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "97"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "VRTkdqnDY4Ap",
        "outputId": "6a45b09a-5103-461e-e74b-d2a693374984"
      },
      "source": [
        "all_headlines[3]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'호시조라 미유키(星空 (ほしぞら) みゆき 한국명:김다솜)는 토에이 애니메이션 제작의 애니메이션《스마일 프리큐어!》에 등장하는 가공의 인물이다. 애니메이션에서 목소리를 연기한 성우는 일본판은 후쿠엔 미사토, 한국판은 양정화이다.\\n\\n\\n== 개요 ==\\n프리큐어 시리즈 9번째 작품에 등장하는 7대째 프리큐어 전사 큐어 해피로 변신하는 인물. 가족은 부모님이 계시며, 외동딸이다. 머리 모양은 양갈래를 코로네와 소라빵모양처럼 작게 돌돌 말아 노란색 리본으로 묶고 있다.\\n그림책과 동화책 읽는 것을 가장 좋아하며, 모든지 \\'해피\\'한게 좋다는 항상 긍정적이고 해맑은 여중생이다. 첫사랑 상대는 피터팬과 백마 탄 왕자이며, 가장 좋아하는 책은 안데르센의 동화 신데렐라. 기분이 좋아 흥분하면 \"울트라 해피!(일본어: ウルトラハッピー!)\"라고 말하는 말버릇이 있다. 반면 조금 맘에 안들거나 할때는 입을 삐죽 내밀며 \"합뿌뿌~(일본어: ハップップー)\"라고 하기도 한다.\\n공부도 운동도 별로 못하고 덜렁이에 실수가 많지만, 결코 실망하는 법이 없이 한결같이 앞으로 전진하는 명랑하고 발랄한 성격인 소녀. 겉보기에는 미덥지 못하지만, 항상 모두를 행복하게 하기 위해 최선을 다하며 그 미소를 빼앗으려는 악당들에게도 과감히 맞서는 강하고 상냥한 마음은 친구들과 주변 사람들을 매료시키고있다. 한편 의외로 맘이 여리고 섬세해서 상처받기 쉬운 면도 있지만 눈물 보이는 것을 싫어하며 속으로 꾹꾹 누르며 항상 웃는 얼굴을 보여준다.\\n어릴 적에 순수하고 밝은 성격과 달리 친구만들기가 어려운 부끄럼많고 내성적인 성격이였으나, 예전에 할머니집에서 살았던 마을에서 만난 숲의 소녀 덕분에 자신감과 용기를 얻는 계기가 되었다.\\n다른 바보 3명과 달리 모든 과목을 못하는 것 같다. 16화에서는 다 아는 속담을 전래동화 드립을 쳤다.\\n\\n\\n== 큐어 해피 ==\\n성스러운 빛의 힘을 가진 프리큐어로서 변신한 모습. 이미지 색상은 분홍색.\\n\\n변신 주문은 \"프리큐어 스마일 차지!\"\\n변신 후 대사는 \"반짝반짝 빛나는 미래의 빛! 큐어 해피!\" (일본어: キラキラ輝く未来の光！ キュアハッピー！)필살기프리큐어 해피 샤워스마일 팩트에 모인 힘을 양 손에 담아 크게 하트를 그린 뒤, 하트 모양으로 만들어진 에너지를 손으로 하트 모양으로 담아서 적에게 날리며 정화한다.프리큐어 신데렐라 해피 샤워\\n프리큐어 해피 샤워 샤이닝\\n\\n\\n== 기타 ==\\n한국판 성우인 성우 양정화(투니버스 공채 1기)는 주연급 캐릭터 성우 중 최연장자로 기록되었다(1970년생).\\n전작인 YES! 프리큐어5의 주인공인 큐어 드림과 비슷한 점이 있다.(분홍색, 같은 학년, 주인공, 덜렁이+바보 등) 큐어 드림 본인은 아니지만 그의 아버지가 동화작가라는 점이 있어서 신데렐라와 같은 동화를 좋아하는 면에서는 간접적인 점이 있다.\\n13화 당시에 나왔던 미유키가 대흉(大凶)을 받아서 크게 충격을 받아 얼굴이 둥글넓적해지게 된 장면은 한국판에서도 무수정 및 무편집 상태로 그대로 방송되었다.\\n14화는 일본판 기준으로는 일본 국내 수학여행이기 때문에 미유키와 야요이가 일행들과 떨어진 에피소드 제목을 \\'(일본 국내)미아\\'로 표기하였으나 한국판에서는 일본으로 해외 수학여행을 했다는 설정에 따라 \\'국제 미아\\' 로 표기하였다.\\n\\n\\n== 각주 =='"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1sUNX3FXqgp",
        "outputId": "ad4204d2-117d-4645-8800-50bb95314245"
      },
      "source": [
        "def clean_text(txt):\r\n",
        "    txt = txt.replace('\\n','')\r\n",
        "    return txt \r\n",
        "\r\n",
        "corpus = [clean_text(x) for x in all_headlines]\r\n",
        "corpus[:10]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['동화(童話)란 어린이를 독자층으로 하는 이야기를 말한다. 아동 문학의 한 장르이다.판타지나 우화, 프로파간다와는 구분할 필요가 있다.== 나라별 동화 ==그리스의 이솝우화, 이탈리아의 피노키오, 독일의 헨젤과 그레텔 등 동화로 알려진 문학이 있다.=== 한국 ===한국에서는 흔히 어린이를 위한 옛날 이야기를 동화라고 이해하고 있다. 이와 같은 해석은 이 문학 유형이 갖고 있는 고유한 특성을 간접적으로 설명한다고 볼 수 있다. 왜냐하면 전통적 의미에서 동화는 한 지방이나 국가에서 세대를 거치면서 자기 나라말로 구전된 것에서 그 뿌리를 두고 있기 때문이다. 이러한 전통적 동화는 종류가 대단히 한정될 수밖에 없다. 현대에 들어서 동화의 창작은 간과할 수 없는 문학 작업으로 평가되고 있다.한국의 본격적인 창작동화는 1923년 마해송(馬海松)의 〈바위나리와 아기별〉이 발표되고, 이어서 《어린이》·《아이생활》·《별나라》 등 여러 잡지와 구연회(口演會)를 통하여 방정환·고한승(高漢承)·진장섭(秦長燮)·정인섭·이정호(李定鎬) 등 색동회 동인들과 기타 이주홍(李周洪) 등의 작가들이 창작활동을 활발히 전개함으로써 비로소 기반을 쌓게 되었다. 현재도 보리출판사, 사계절출판사, 예영커뮤니케이션 등 다수의 출판사에서 어린이들을 위한 창작동화들을 출판하고 있다.== 나라별 동화 작가 ==== 각주 ==',\n",
              " '《신 전래동화》는 2018년에 개봉한 대한민국의 영화이다.== 캐스팅 ==== 같이 보기 ==2018년 대한민국의 영화 목록== 외부 링크 ==신 전래동화 - 네이버 영화신 전래동화 - 다음 영화신 전래동화 - 무비스트신 전래동화 - 한국영화 데이터베이스',\n",
              " '《꾸러기 수비대》 (혹은《출동! 12레인저》)는 십이지를 소재로 한 애니메이션이다.일본에서는 《십이전지 폭렬 에토레인저》(十二戦支 爆烈エトレンジャー)라는 이름으로 NHK 위성 제2텔레비전을 통해 1995년에 방영되었다.== 개요 ==한국어 더빙판으로는 1996년 KBS 2TV에서 방영한 《꾸러기 수비대》와 투니버스에서 방영한 《출동! 12레인저》 두 가지가 있는데, KBS 2TV의 선행방송과 공중파방송이라는 이점으로 대한민국에서는 《꾸러기 수비대》라는 명칭으로 잘 알려져 있다. 1999년 11월 15일부터 2000년 1월 28일까지 iTV 인천방송에서 KBS 방영분을 재방영했고, 2004년에 재능TV에서 역시 KBS 방영분을 재방영하기도 했다.크루세이더 퀘스트와 콜라보레이션을 진행했다.== 스토리 ==이야기의 무대는 몽환(원더랜드)(일본어: ムーゲン)라는 수많은 동물 정령(요정)들이 사는 평화로운 이(異)차원의 세계다. 인간이 만들어낸 많은 이야기들이 노벨월드(동화나라)로 구성되어 하늘높이 솟은 노벨폴이 존재의 기둥인 세계이다. 그런데 무겐 바로 아래에 위치한 심해의 어둠 속에는 사령성(일본어: 妖魔城)이 있으며 이곳에서 사령왕 냔마(해라)(일본어: ニャンマー)가 노벨월드를 파괴하기 위해 곳곳에 사령 몬스터(블랙전사)들을 파견한다. 그 상태로는 노벨월드가 파괴되고, 무겐마저 붕괴되고 만다. 오오라 공주(오로라 공주)(일본어: オーラ姫)는 무겐의 수비신인 에토레인저(꾸러기 수비대)를 소집하여, 그들로 하여금 시공간 이동머신 키린다(알바트로스)(일본어: キリンダー)를 타고 노벨월드를 지키게 한다. 사령 몬스터는 자신이 침입한 노벨월드 속의 인물로 변신해 그 동화의 세계관을 망가뜨려 이야기의 진행을 저해하는 방법으로 노벨월드의 파괴를 꾀한다. 에토레인저가 각 노벨월드에 있을 수 있는 시간은 단 이틀뿐. 그 사이에 사령 몬스터를 찾아내어 \"정화\"시켜야만 한다. 망가진 노벨월드에서 에토레인저를 기다리는 것은...== 등장인물 ==== 방영 목록 ==십이전지 폭렬 에토레인저(꾸러기 수비대)에서 편집 또는 삭제된 부분은 다음과 같다.\\' 1화 - 이런 모험이 있어~? \\' , \\' 2화-쓰러트려라 사령 몬스터 \\' 는 일본의 전래동화 \\' 모모타로 \\' 이기 때문에 1화는 편집, 2화는 삭제되었다.\\' 7화 - 오늘 개점? 카구야 공주를 찾아라 \\' 도 일본의 전래동화(대나무 장수)이기 때문에 삭제되었다.\\' 17화 - 알라딘 할아버지와 마법의 램프 \\' 는 일본의 전래동화는 아니지만 배경이 일본의 \\' 에도시대 \\' 여서 삭제되었다.\\' 15화와 29화는 국내 방송은 분명히 했으나, 방영자료가 남아있지 않아, 인터넷이나, 비디오로도 더빙판을 찾을 수 없다. 비디오 테이프 손상으로 방영자료를 찾을 수 없는걸 수도...그 외에 \\' 4화 - 토끼 실례, 거북이 먼저 \\' , \\' 6화 - 결투! 웨스턴으로 서유기 \\' , \\' 14화 - 짚대 장자의 교환전투! \\' , \\' 38화 - 분노와 증오의 끝에서 \\' 등 여러 에피소드들이 편집되었다.== 각주 ==== 외부 링크 ==JEI 재능TV 공식 사이트꾸러기 수비대 일본판 사이트',\n",
              " '호시조라 미유키(星空 (ほしぞら) みゆき 한국명:김다솜)는 토에이 애니메이션 제작의 애니메이션《스마일 프리큐어!》에 등장하는 가공의 인물이다. 애니메이션에서 목소리를 연기한 성우는 일본판은 후쿠엔 미사토, 한국판은 양정화이다.== 개요 ==프리큐어 시리즈 9번째 작품에 등장하는 7대째 프리큐어 전사 큐어 해피로 변신하는 인물. 가족은 부모님이 계시며, 외동딸이다. 머리 모양은 양갈래를 코로네와 소라빵모양처럼 작게 돌돌 말아 노란색 리본으로 묶고 있다.그림책과 동화책 읽는 것을 가장 좋아하며, 모든지 \\'해피\\'한게 좋다는 항상 긍정적이고 해맑은 여중생이다. 첫사랑 상대는 피터팬과 백마 탄 왕자이며, 가장 좋아하는 책은 안데르센의 동화 신데렐라. 기분이 좋아 흥분하면 \"울트라 해피!(일본어: ウルトラハッピー!)\"라고 말하는 말버릇이 있다. 반면 조금 맘에 안들거나 할때는 입을 삐죽 내밀며 \"합뿌뿌~(일본어: ハップップー)\"라고 하기도 한다.공부도 운동도 별로 못하고 덜렁이에 실수가 많지만, 결코 실망하는 법이 없이 한결같이 앞으로 전진하는 명랑하고 발랄한 성격인 소녀. 겉보기에는 미덥지 못하지만, 항상 모두를 행복하게 하기 위해 최선을 다하며 그 미소를 빼앗으려는 악당들에게도 과감히 맞서는 강하고 상냥한 마음은 친구들과 주변 사람들을 매료시키고있다. 한편 의외로 맘이 여리고 섬세해서 상처받기 쉬운 면도 있지만 눈물 보이는 것을 싫어하며 속으로 꾹꾹 누르며 항상 웃는 얼굴을 보여준다.어릴 적에 순수하고 밝은 성격과 달리 친구만들기가 어려운 부끄럼많고 내성적인 성격이였으나, 예전에 할머니집에서 살았던 마을에서 만난 숲의 소녀 덕분에 자신감과 용기를 얻는 계기가 되었다.다른 바보 3명과 달리 모든 과목을 못하는 것 같다. 16화에서는 다 아는 속담을 전래동화 드립을 쳤다.== 큐어 해피 ==성스러운 빛의 힘을 가진 프리큐어로서 변신한 모습. 이미지 색상은 분홍색.변신 주문은 \"프리큐어 스마일 차지!\"변신 후 대사는 \"반짝반짝 빛나는 미래의 빛! 큐어 해피!\" (일본어: キラキラ輝く未来の光！ キュアハッピー！)필살기프리큐어 해피 샤워스마일 팩트에 모인 힘을 양 손에 담아 크게 하트를 그린 뒤, 하트 모양으로 만들어진 에너지를 손으로 하트 모양으로 담아서 적에게 날리며 정화한다.프리큐어 신데렐라 해피 샤워프리큐어 해피 샤워 샤이닝== 기타 ==한국판 성우인 성우 양정화(투니버스 공채 1기)는 주연급 캐릭터 성우 중 최연장자로 기록되었다(1970년생).전작인 YES! 프리큐어5의 주인공인 큐어 드림과 비슷한 점이 있다.(분홍색, 같은 학년, 주인공, 덜렁이+바보 등) 큐어 드림 본인은 아니지만 그의 아버지가 동화작가라는 점이 있어서 신데렐라와 같은 동화를 좋아하는 면에서는 간접적인 점이 있다.13화 당시에 나왔던 미유키가 대흉(大凶)을 받아서 크게 충격을 받아 얼굴이 둥글넓적해지게 된 장면은 한국판에서도 무수정 및 무편집 상태로 그대로 방송되었다.14화는 일본판 기준으로는 일본 국내 수학여행이기 때문에 미유키와 야요이가 일행들과 떨어진 에피소드 제목을 \\'(일본 국내)미아\\'로 표기하였으나 한국판에서는 일본으로 해외 수학여행을 했다는 설정에 따라 \\'국제 미아\\' 로 표기하였다.== 각주 ==',\n",
              " \"아동문학(兒童文學, Children's literature)은 어린이와 어린이의 마음을 갖고 있는 성인을 대상으로 한 문학으로, 어린이의 흥미를 불러일으키거나 교훈적인 내용을 소재로 하며, 대체로 평이한 문장에 그림이 곁들여진다.== 역사 ==고대 그리스의 이솝 우화나 아라비안 나이트는 아동문학으로서 창작되지는 않았지만 현대에서 어린이들에게 읽혀지고 있으며, 17세기에 들어서 순수하게 어린이를 위한 문학이 등장하였고, 19세기에 들어서 문학의 하나로서 자리잡았다.== 한국의 아동문학 ==《삼국유사》에는 백제 무왕이 아이들에게 부르게 했다는 〈서동요〉가 등장한다. 근·현대 이전에는 어린이들을 위해 출간된 문학작품은 없었으나, 각 지방에는 어린이들에게 들려주는 전래 동화가 전해졌다. 개화기와 일제강점기에는 이광수, 최남선 등 어린이를 위한 시나 소설을 창작하는 이들이 등장하였고, 1923년에는 방정환을 중심으로 《어린이》 잡지가 창간되었으나, 일제의 조선어말살정책으로 오래 지속되지 못하였다. 일제의 패망과 광복에 뒤이은 한국 전쟁 또한 아동문학 성장에 좋은 조건이 되지 못했다.== 종류 ==동화동시== 각주 ==== 외부 링크 ==아동문학, 《브리태니커백과》한국아동문학학회 홈페이지동화와번역 연구소 홈페이지, 동화와 번역 연구소는 한국의 유일한 동화연구소이다.\",\n",
              " '한국의 사찰은 한국에 불교가 전래되면서 설립되었으며, 최초의 사찰은 고구려 소수림왕때 지어진 이불란사와 초문사이다.문화관광부에 등록된 사찰의 개수는 926개이다.== 삼보사찰 ==삼보(三寶)는 불교에서 귀하게 여기는 세 가지 보물을 말하며, 부처님(佛)과 부처님이 설하신 법(法), 그리고 그 가르침에 따라 살아가는 스님(僧)을 의미한다.통도사 - 불보(佛寶) 사찰, 부처님의 진신사리를 봉안해인사 - 법보(法寶) 사찰, 부처님의 가르침인 팔만대장경 경판을 봉안송광사 - 승보(僧寶) 사찰, 부처님의 가르침에 따라 수행을 한 스님들중 16명이 국사(國師)의 지위에 오름== 오대 총림과 팔대 총림 ==한국 불교의 조계종에서는 다음의 다섯 사찰은 선원과 강원을 모두 갖추고 있으며 이들을 오대 총림이라 하였다.가야총림 해인사조계총림 송광사영축총림 통도사덕숭총림 수덕사고불총림 백양사2012년 11월 7일 대한불교 조계종 중앙종회는 제192회 정기회의에서 동화사 · 쌍계사 · 범어사의 세 사찰을 만장일치로 총림으로 추가 지정하였다. 이에 따라 팔대 총림이 있게 되었다.팔공총림 동화사쌍계총림 쌍계사금정총림 범어사== 사찰 목록 ==한국의 사찰 목록 (가나다순)한국의 사찰 목록 (지역별)위의 목록들은 2005년 문화관광부(현 문화체육관광부) 자료를 근거로 하여 보충하였으나 완전한 것은 아니다. 조선민주주의인민공화국 지역의 사찰은 단 1개만이 포함되어 있으며, 대한민국의 사찰 역시 빠져 있는 것이 부지기수이다.== 참고자료 ==“삼보사찰 (1) _ 삼보사찰이란”. 달마넷. 2008년 9월 28일에 원본 문서에서 보존된 문서. 2008년 4월 14일에 확인함. === 각주 ===== 외부 링크 ==아미타넷 사찰검색',\n",
              " '《해와 달이 된 오누이》은 대한민국의 전래동화이다. 《햇님 달님》(표준어: 해님 달님)으로도 불린다.== 역사 ==출판된 것은 1922년 잡지 《개벽》에 실린 주요섭의 《해와 달》이 최초이다.== 줄거리 ==산 속에 홀어머니와 오누이 가족이 살고 있었다. 어느 날 밤 떡을 이고 산길을 가는 어머니를 호랑이가 잡아먹었다. 호랑이는 오누이도 잡아먹기 위해 오누이의 집으로 찾아갔다. 오누이가 집문을 열어주도록 하기 위해 호랑이는 어머니 흉내를 냈지만, 오누이는 호랑이인 것을 알아차리고 도망가 나무 위로 올라갔다.나무 위에 올라간 오누이는 하늘에 동아줄을 내려 자신을 살려달라고 빌었다. 오누이는 동아줄을 타고 하늘로 올라가 여동생은 해가, 오빠는 달이 되었다.== 참고 문헌 ==',\n",
              " '거북(문화어: 거부기)은 거북목에 속하는 파충류를 일컫는다. 거북이의 가장 큰 종류로는 길이 250㎝, 몸무게 800㎏에 달하는 것도 있다.== 생태 ===== 보호수단 ===등딱지와 배딱지로 몸을 보호하고 있는데 이것들은 갈비뼈에서 분화된 연골로 이루어져 있다. 식물, 작은 물고기 등 다양한 것을 먹고 사는데, 특히 애완용 거북인 붉은귀거북은 생태계를 교란시킨다고 할 정도로 식탐이 대단하다.=== 사람과의 관계 ===전래동화에 남생이가 등장할 정도로 사람들에게 친숙한 동물이며 오랫동안 사는 동물로 유명하다. 특히 종류에 따라서는 200~300년 이상 생존하는 종도 존재한다. 그러한 인식 때문에 십장생 중에도 거북이가 들어가 있다.=== 성격 ===거북의 등은 단단한 껍질로 싸여 있고 아주 느리게 움직이며 이빨이 없고 비공격적이다. 거북의 암수를 구별하기 위해서는 몸을 뒤집어 항문을 보면 쉽게 알 수 있다. 수컷의 항문은 꼬리 끝 쪽에 있고,  거북의 암컷의 항문은 꼬리가 붙어 있는 부분에 있다.== 분류 ==거북목(Testudines)잠경아목(Cryptodira)늑대거북상과(Chelydroidea)늑대거북과(Chelydridae) - 2속 6종땅거북상과(Testudinoidea)땅거북과(Testudinidae)돌거북과(Geoemydidae)늪거북과(Emydidae)큰머리거북과(Platysternidae) - 1속 1종자라상과(Trionychoidea)돼지코거북과(Carettochelyidae) - 1속 1종자라과(Trionychidae)풀거북상과(Kinosternoidea)강거북과(Dermatemydidae)풀거북과(Kinosternidae)바다거북상과(Chelonioidea)바다거북과(Cheloniidae)장수거북과(Dermochelyidae)곡경아목(Pleurodira)뱀목거북상과(Chelidoidea)뱀목거북과(Chelidae)가로목거북상과(Pelomedusoidea)가로목거북과(Pelomedusidae)견목거북과(Podocnemididae)== 자라와 차이점 ==자라와 거북의 차이점은  등껍질의 무늬가 있고 없고의 차이이다.== 거북과 관련된 캐릭터 ==마리오 시리즈 - 엉금엉금, 쿠파헷지 - 번닌자 거북이디지몬 시리즈 - 왕거북몬록맨 X6 - 레이니 터틀로이드록맨 제로 4 - 히트 겐블럼포켓몬스터 - 거북왕, 코터스, 토대부기, 늑골라, 폭거북스, 갈가부기 계열== 같이 보기 ==자라== 각주 ==== 참고 문헌 ==== 외부 링크 == 위키미디어 공용에 거북 관련 미디어 분류가 있습니다. 위키생물종에 Testudines 관련 자료가 있습니다. 위키낱말사전에 거북 관련 글이 있습니다.',\n",
              " '잠자는 숲속의 미녀( - 美女, 프랑스어: La Belle au Bois dormant, \"The Beauty sleeping in the Wood\", 독일어: Dornröschen, \"Little Briar Rose\", 영어: The Sleeping Beauty)는 고전 동화이다. 샤를 페로의 동화집 및 그림 동화에 수록되었다. 페로는 1697년 《어미 거위 이야기》에 이 동화를 수록했다. 페로 이전인 1634년에 발간된 잠바티스타 바실레의 《Pentamerone》 중 39장 〈해, 달, 그리고 탈리아〉(Sole, Luna e Talia)에도 이 이야기의 요소가 포함되어 있다. 1812년 그림 형제는 독일 북부 포메라니아 지방의 전래 동화를 바탕으로 이 이야기를 자신들의 저서에 수록하였다.  톨킨 교수는 페로의 문화적 존재감은 굉장히 보편적이라, 만약 동화를 이름 대라고 질문하면, 대부분의 사람들이 페로의 모음집 중의 9개의 이야기 중 하나를 인용할 것이라 언급하였다. 그러나 톨킨의 세대 이후, 가장 유명한 \"잠자는 숲속의 미녀\"는 월트 디즈니의 만화영화가 되었고, 이는 페로의 원작에 기초를 둔 차이콥스키의 발레만큼이나 주목을 받았다.== 줄거리 ===== 그림 동화 ===아이를 간절히 원하던 어느 왕과 왕비가 귀여운 딸을 얻게 되었다. 공주의 탄생을 축하하기 위해 왕은 나라안의 마법사를 초대하기로 했다. 손님을 대접할 황금 접시가 12개밖에 없었기 때문에 부득이하게 왕은 열두 명의 마법사밖에 초대할 수가 없었다. 초대받은 마법사들은 한 사람씩 공주에게 마법을 걸어 아름다움, 지혜 등 훌륭한 선물을 주었다. 11번째의 마법사까지 선물을 주었을 때, 초대받지 못한 13번째 마법사가 나타나 \"공주는 방추에 찔려 죽을 것이다\" 라고 저주를 걸었다. 미처 선물을 주지 않았던 12번째 마법사는 그 저주를 풀 수는 없었지만 대신 죽음이 아닌 100년 동안의 깊은 잠이 들도록 바꾸었다. 왕은 나라안의 모든 방추를 불태우도록 했고, 이런 사실을 모른 채 공주는 무럭무럭 자라나 15세가 되었다. 혼자서 성을 돌아다니던 공주는 탑 꼭대기에서 실을 잣는 노파를 발견했고, 방추에 손을 찔려 깊은 잠에 빠졌다. 공주뿐만 아니라 왕과 왕비, 성 안의 모든 사람들이 잠이 들었고 성안은 가시덤불이 에워싸 아무도 들어갈 수 없게 되었다. 많은 왕자들이 공주를 구하기 위해 성으로 들어가려고 했지만 모두 실패했다. 100년이 지난 후 근처를 지나가던 한 왕자가 소문을 듣고 성을 찾아왔고, 그는 성으로 들어가 공주에게 입을 맞추었다. 잠에서 깨어난 공주는 왕자와 결혼해 행복하게 살았다.=== 페로 동화 ===페로의 동화에서는 마법사 대신 여덟 명의 요정이 등장한다. 공주가 잠이 든 후 슬픔에 빠진 왕과 왕비는 성을 떠나고, 나머지 사람들만이 요정의 마법에 의해 잠이 든다. 또한 공주는 왕자의 키스가 아닌 100년의 시간이 흘러 저절로 잠에서 깨어난다. 또한 페로의 버전에서는 결혼한 공주가 두 아이를 얻지만 시어머니인 왕비가 아이들을 잡아먹으려다가 실패한다는 이야기가 있었으나 그림 동화에서는 이 부분이 삭제되었다.== 분석 ==== 각색 ==== 각주 ==== 외부 링크 ==Little Briar-Rose and other variants from SurLaLune Fairy Tale sitePerrault\\'s version discussed by Waller Hastings in \"The sleeping beauty in the woods\"Perrault\\'s version also discussed by Waller Hastings in \"Sol, Luna, e Talia\"Free audio story of The Sleeping Beauty at StorynoryAudio of The Sleeping Beauty Part Two (The Queen Ogre) at StorynorySleeping Beauty by Jacob and Wilhelm Gimm숲속의 잠자는 공주1870 illustrated scanned book via Internet Archive',\n",
              " '이한갈(1971년 9월 1일 ~ )은 대한민국의 배우이다. 1996년 SBS 6기 공채 탤런트로 선발되었다.== 학력 ==경희대학교 체육학경희대학교 언론정보대학원 대중예술학 석사== 출연 작품 ===== 영화 ===《신 전래동화》 (2018년) - 아토 히로부미 역《두 사람이다》 (2007년)《무영검》 (2005년) - 철격대조장 역《바람의 파이터》 (2004년) - 히메지닌자  역《천년호》 (2003년) - 탈위 역《성냥팔이 소녀의 재림》 (2002년) - 오비련 오른팔 역《4발가락》 (2002년) - 곰탕 역《신라의 달밤》 (2001년) - 혁수 역《비천무》 (2000년) - 창룡 역《태백산맥》 (1994년)=== 드라마 ===《해치》 (2019년, SBS) - 민진헌 살수 역《그 겨울, 바람이 분다》 (2013년, SBS)《아테나: 전쟁의 여신》 (2010년, SBS) - 영호 역《근초고왕》 (2010년, KBS1) - 백제 싸울아비 역《대왕 세종》 (2008년, KBS) - 강휘 역《불멸의 이순신》 (2004년, KBS1) - 날발 역《천년지애》 (2003년, SBS)《결혼 이야기》 (2002년, KBS2)《행복의 시작》 (1996년, SBS)《8월의 신부》 (1996년, SBS)《엄마의 깃발》 (1996년, SBS)《부자유친》 (1996년, SBS)=== 예능 ===《도전! 지구탐험대》 (KBS2)== 각주 ==== 외부 링크 ==이한갈 - 한국영화 데이터베이스이한갈 프로필']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKmZrCJSZrz9",
        "outputId": "2f29f509-8ed8-4314-d543-00ec3d68fa4f"
      },
      "source": [
        "tokenizer = Tokenizer()\r\n",
        "\r\n",
        "def get_sequence_of_tokens(corpus):\r\n",
        "    ## tokenization\r\n",
        "    tokenizer.fit_on_texts(corpus)\r\n",
        "    total_words = len(tokenizer.word_index) + 1\r\n",
        "    \r\n",
        "    ## convert data to sequence of tokens \r\n",
        "    input_sequences = []\r\n",
        "    for line in corpus:\r\n",
        "        token_list = tokenizer.texts_to_sequences([line])[0]\r\n",
        "        for i in range(1, len(token_list)):\r\n",
        "            n_gram_sequence = token_list[:i+1]\r\n",
        "            input_sequences.append(n_gram_sequence)\r\n",
        "    return input_sequences, total_words\r\n",
        "\r\n",
        "inp_sequences, total_words = get_sequence_of_tokens(corpus)\r\n",
        "inp_sequences[:10]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[226, 6082],\n",
              " [226, 6082, 1385],\n",
              " [226, 6082, 1385, 1042],\n",
              " [226, 6082, 1385, 1042, 6083],\n",
              " [226, 6082, 1385, 1042, 6083, 67],\n",
              " [226, 6082, 1385, 1042, 6083, 67, 198],\n",
              " [226, 6082, 1385, 1042, 6083, 67, 198, 227],\n",
              " [226, 6082, 1385, 1042, 6083, 67, 198, 227, 6084],\n",
              " [226, 6082, 1385, 1042, 6083, 67, 198, 227, 6084, 417],\n",
              " [226, 6082, 1385, 1042, 6083, 67, 198, 227, 6084, 417, 11]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HI7aIGwGii5c",
        "outputId": "d2b4aaa7-604d-46c1-e0b8-19f8713508fe"
      },
      "source": [
        "len(inp_sequences)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47930"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWiF05wBZwVb"
      },
      "source": [
        "def generate_padded_sequences(input_sequences):\r\n",
        "    max_sequence_len = max([len(x) for x in input_sequences])\r\n",
        "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\r\n",
        "    \r\n",
        "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\r\n",
        "    label = ku.to_categorical(label, num_classes=total_words)\r\n",
        "    return predictors, label, max_sequence_len\r\n",
        "\r\n",
        "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zd-Bn3daZ1_8",
        "outputId": "0185bd9c-7572-47fb-cef3-d8009d3fe2fe"
      },
      "source": [
        "def create_model(max_sequence_len, total_words):\r\n",
        "    input_len = max_sequence_len - 1\r\n",
        "    model = Sequential()\r\n",
        "    \r\n",
        "    # Add Input Embedding Layer\r\n",
        "    model.add(Embedding(total_words, 10, input_length=input_len))\r\n",
        "    \r\n",
        "    # Add Hidden Layer 1 - LSTM Layer\r\n",
        "    model.add(LSTM(100))\r\n",
        "    model.add(Dropout(0.1))\r\n",
        "    \r\n",
        "    # Add Output Layer\r\n",
        "    model.add(Dense(total_words, activation='softmax'))\r\n",
        "\r\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\r\n",
        "    \r\n",
        "    return model\r\n",
        "\r\n",
        "model = create_model(max_sequence_len, total_words)\r\n",
        "model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 1035, 10)          250810    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 100)               44400     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 25081)             2533181   \n",
            "=================================================================\n",
            "Total params: 2,828,391\n",
            "Trainable params: 2,828,391\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMEAkJFsZ8Q2",
        "outputId": "30c26848-caac-449c-ec69-7ca87b770f53"
      },
      "source": [
        "model.fit(predictors, label, epochs=10, verbose=5)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "Epoch 2/10\n",
            "Epoch 3/10\n",
            "Epoch 4/10\n",
            "Epoch 5/10\n",
            "Epoch 6/10\n",
            "Epoch 7/10\n",
            "Epoch 8/10\n",
            "Epoch 9/10\n",
            "Epoch 10/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff35a512d30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lETQV3uUaM-A"
      },
      "source": [
        "def generate_text(seed_text, next_words, model, max_sequence_len):\r\n",
        "    for _ in range(next_words):\r\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\r\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\r\n",
        "        predicted = model.predict_classes(token_list, verbose=0)\r\n",
        "        output_word = \"\"\r\n",
        "        for word,index in tokenizer.word_index.items():\r\n",
        "            if index == predicted:\r\n",
        "                output_word = word\r\n",
        "                break\r\n",
        "        seed_text += \" \"+output_word\r\n",
        "    return seed_text.title()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaFXcN40bDYT",
        "outputId": "c51fbe90-dc49-4de0-8628-987291406305"
      },
      "source": [
        "print (generate_text(\"b\", 50, model, max_sequence_len))\r\n",
        "print (generate_text(\"c\", 30, model, max_sequence_len))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "B 토빗기 영어 지은이와 영어 Children'S 영어 Book 영어 Penmanship 또는 Calligraphy 은 붓으로 글씨를 또는 일컫는다 사는 작품에 왕조이다 이불란사와 초문사이다 문화관광부에 등록된 사찰의 개수는 926개이다 삼보사찰 삼보 三寶 등이 있다 귀하게 1986년 7월 〈퐁당퐁당〉이나 〈커다란 꿀밤 2008년 1991년 1991년 예능 역 예능 1994년 Kbs 연기대상 스페셜 Sbs 연기대상 공채\n",
            "C 聖德大王神鍾 는 러시아의 차이콥스키가 작곡한 발레 음악이자 또는 완조 열리는 열리는 9월에 타로가 Rose · 복서 · 무축 · 무축 · 무축 · 무축 · 무축 · 무축 · 무축\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mw0ROjbJhQV1",
        "outputId": "8c20a3c3-e431-49db-b9fa-d18070a28f84"
      },
      "source": [
        "print (generate_text(\"이탈리아\", 30, model, max_sequence_len))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "이탈리아 영어 Vikings 노르웨이어 Vikinger Vikinger Vikinger Vikinger 스웨덴어 뉘노르스크 뉘노르스크 Vikingar 아이슬란드어 Víkingar 은 노르드어 비킹 는 노르드어 Vikingr 이 골디락스 표현이다 북게르만족 노르드인이고 노르드어를 노르드어를 지역의 구전된 한다 이들은\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9D0W9vI4FB1c"
      },
      "source": [
        "document = \"\"\"\r\n",
        "주호영 국민의힘 원내대표는 22일 고위공직자범죄수사처(공수처)법 개정과 가덕도 신공항 건설 등을 밀어붙이고 있는 문재인 정권과 더불어민주당을 향해 \"이제 끝이 보인다\"며 \"짓밟힌 풀들이 아우성 치는 국민적 저항에 직면할 것\"이라고 경고했다.\r\n",
        "주 원내대표는 이날 자신의 페이스북에 \"문재인 정권이 공수처법 개정을 위한 '군사작전'에 돌입하겠다고 엄포를 놓고 있다\"며 \"정의당을 끌어들이기 위해 꼼수 선거법에 묶어 '패스트트랙'이라는 불법·탈법으로 만들어낸 공수처법을 시행도 해보지 않고 고치려 하는 것\"이라고 지적했다.\r\n",
        "이어 주 원내대표는 \"야당 원내대표인 제게 문재인 대통령은 사람 좋아보이는 표정으로 '공수처는 야당의 동의 없이는 절대 출범할 수 없는 것'이라고 얘기했고, 야당이 유엔 안보리 상임이사국처럼 공수처장 임명에 '비토권'을 행사할 수 있는데 무얼 걱정하느냐고, 여당 사람들이 우리를 속였다\"며 \"거짓말이라는 비난을 개의치 않는 사람들\"이라고 꼬집었다.\r\n",
        "주 원내대표는 \"이해찬 전 민주당 대표가 얘기한 '민주당 20년 집권'의 토대가 올해 안에 완성된다\"며 \"탈원전과 동남권 신공항은 문 대통령이 대선 공약으로 내건 사업이니 여기에 불법이 있었다고 시비를 거는 것은 민주주의를 부정하는 것이라고 청와대 출신 윤건영 민주당 의원이 윽박지른다. 이제 '민주주의 없는 민주당'이 법위에 군림하는 '반민주'를 거리낌없이 획책하는 것\"이라고 언급했다.\r\n",
        "그러면서 주 원내대표는 \"표를 얻기 위해 나라 곳간을 다 허물어뜨렸고, 재정 운용에서 신중함은 사라졌다\"며 \"괴물 공수처가 출범하면 공무원 누구나 대통령과 권력이 지시하는 범죄행위에 거리낌 없이 가담할 것이다. 청와대와 권부 요직에 앉아 불법으로 각종 이권을 챙기는 권력자들에 대한 사건이 불거져도 공수처가 사건을 가져가 버리면 그만\"이라고 우려했다.\r\n",
        "주 원내대표는 \"문 대통령은 제게 '공수처는 고위 공직자들을 처벌하는 것인데 왜 야당이 반대하는지 이해할 수 없다'고 했는데, 그런 분이 청와대와 대통령 주변을 감시하는 특별감찰관은 취임 이후 지금까지 왜 임명하지 않았는가\"라며 \"공수처는 권력형 비리의 쓰레기 하치장, 종말 처리장이 될 것\"이라고 비판했다.\r\n",
        "문재인 정부를 향해 주 원내대표는 \"문 대통령과 그 사도들은 법치가 미치지 않는 무오류의 화신이 될 것\"이라며 \"오류를 인정하지 않는 존재가 바로 신이며 그 아래에는 자신들의 지도자를 목숨바쳐 지킴으로서 정의를 실현하겠다는 추종자들로 넘쳐 난다. 공수처는 지도자의 신성을 인정하지 않는 세력을 정죄하는 수단으로 전락할 것\"이라고 질타했다.\r\n",
        "주 원내대표는 \"저도 법조인이지만 대통령과 공수처장이 마음대로 검사들과 수사관들을 임명하는 이 끔찍한 사법기구가 어떤 일을 할지 두렵기만 하다\"며 \"공수처는 검찰과 경찰 위에 있는 사법기구로, 헌법과 법으로 독립성을 보장하는 검찰총장을 이렇게 핍박하는 정권이 공수처를 어떻게 운영할지 불을 보듯 뻔한 일\"이라고 예측했다.\r\n",
        "그러면서 주 원내대표는 \"추미애 법무장관을 앞장 세워 윤석열 검찰의 권력 비리 수사를 저지하려다가 난관에 봉착하자 무슨 수를 써서라도 공수처를 출범시키려 한다. 공수처장 자리에는 추미애보다 더 한 막무가내 내 편을 앉힐 게 분명한 것\"이라며 \"문 정권의 파렴치와 오만함을 최전선에서 온 몸으로 겪어온 저로서는 민주당이 내일부터 국회에서 보일 행태가 환히 보인다. 180석의 민주당이 또 군사작전을 개시하면 그걸 누가 막겠는가\"라고 성토했다.\r\n",
        "주 원내대표는 \"공수처법을 막을 힘이 우리 야당에게는 없다. 삭발하고 장외투쟁해 봐야 눈 하나 깜짝할 사람들이 아닌 것\"이라며 \"대란대치(大亂大治), 세상을 온통 혼돈 속으로 밀어넣고 그걸 권력 유지에 이용한다는 게 이 정권의 통치기술\"이라고 규탄했다.\r\n",
        "아울러 주 원내대표는 \"권력은 바람, 국민은 풀이다. 바람이 불면 청보리 밭의 보리가 눕는다\"며 \"권력은 풀들이 다시는 일어서지 못하도록 풀을 짓밟지만 풀들은 다시 일어난다. 시인 김수영은 '바람보다 먼저 눕지만, 바람보다 먼저 일어나는' 민초의 힘을 노래했다\"고 말했다.\r\n",
        "마지막으로 주 원내대표는 \"문재인 정권은 이제 곧 국회에서 광장에서 짓밟힌 풀들이 일어서서 아우성치는 모습을 지켜보게 될 것\"이라며 \"대란대치를 끝장내려는 국민적 저항에 직면할 것\"이라고 거듭 강조했다.\r\n",
        "\"\"\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocHnmjt4FRUF"
      },
      "source": [
        "tokenizer = Tokenizer()\r\n",
        "tokenizer.fit_on_texts([document])\r\n",
        "total_words = len(tokenizer.word_index) + 1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8OViY44Fk0J",
        "outputId": "10695118-24ee-4ac7-efda-0da0fd2cf9d8"
      },
      "source": [
        "print(total_words)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "395\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PeHFwITF0Ux",
        "outputId": "1e6c20b0-ec00-4f89-e0d5-8972fa9ffafd"
      },
      "source": [
        "for word,index in tokenizer.word_index.items():\r\n",
        "  print(index,word)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 원내대표는\n",
            "2 주\n",
            "3 것\n",
            "4 이라고\n",
            "5 며\n",
            "6 문재인\n",
            "7 않는\n",
            "8 문\n",
            "9 이라며\n",
            "10 이제\n",
            "11 풀들이\n",
            "12 수\n",
            "13 대통령과\n",
            "14 공수처는\n",
            "15 될\n",
            "16 있는\n",
            "17 향해\n",
            "18 보인다\n",
            "19 짓밟힌\n",
            "20 국민적\n",
            "21 저항에\n",
            "22 직면할\n",
            "23 정권이\n",
            "24 위해\n",
            "25 공수처법을\n",
            "26 제게\n",
            "27 대통령은\n",
            "28 '공수처는\n",
            "29 없는\n",
            "30 야당이\n",
            "31 공수처장\n",
            "32 사람들이\n",
            "33 민주당\n",
            "34 그러면서\n",
            "35 공수처가\n",
            "36 청와대와\n",
            "37 왜\n",
            "38 그\n",
            "39 인정하지\n",
            "40 이\n",
            "41 공수처를\n",
            "42 권력\n",
            "43 게\n",
            "44 정권의\n",
            "45 민주당이\n",
            "46 국회에서\n",
            "47 그걸\n",
            "48 권력은\n",
            "49 먼저\n",
            "50 주호영\n",
            "51 국민의힘\n",
            "52 22일\n",
            "53 고위공직자범죄수사처\n",
            "54 공수처\n",
            "55 법\n",
            "56 개정과\n",
            "57 가덕도\n",
            "58 신공항\n",
            "59 건설\n",
            "60 등을\n",
            "61 밀어붙이고\n",
            "62 정권과\n",
            "63 더불어민주당을\n",
            "64 끝이\n",
            "65 아우성\n",
            "66 치는\n",
            "67 경고했다\n",
            "68 이날\n",
            "69 자신의\n",
            "70 페이스북에\n",
            "71 공수처법\n",
            "72 개정을\n",
            "73 위한\n",
            "74 '군사작전'에\n",
            "75 돌입하겠다고\n",
            "76 엄포를\n",
            "77 놓고\n",
            "78 있다\n",
            "79 정의당을\n",
            "80 끌어들이기\n",
            "81 꼼수\n",
            "82 선거법에\n",
            "83 묶어\n",
            "84 '패스트트랙'이라는\n",
            "85 불법·탈법으로\n",
            "86 만들어낸\n",
            "87 시행도\n",
            "88 해보지\n",
            "89 않고\n",
            "90 고치려\n",
            "91 하는\n",
            "92 지적했다\n",
            "93 이어\n",
            "94 야당\n",
            "95 원내대표인\n",
            "96 사람\n",
            "97 좋아보이는\n",
            "98 표정으로\n",
            "99 야당의\n",
            "100 동의\n",
            "101 없이는\n",
            "102 절대\n",
            "103 출범할\n",
            "104 것'이라고\n",
            "105 얘기했고\n",
            "106 유엔\n",
            "107 안보리\n",
            "108 상임이사국처럼\n",
            "109 임명에\n",
            "110 '비토권'을\n",
            "111 행사할\n",
            "112 있는데\n",
            "113 무얼\n",
            "114 걱정하느냐고\n",
            "115 여당\n",
            "116 우리를\n",
            "117 속였다\n",
            "118 거짓말이라는\n",
            "119 비난을\n",
            "120 개의치\n",
            "121 사람들\n",
            "122 꼬집었다\n",
            "123 이해찬\n",
            "124 전\n",
            "125 대표가\n",
            "126 얘기한\n",
            "127 '민주당\n",
            "128 20년\n",
            "129 집권'의\n",
            "130 토대가\n",
            "131 올해\n",
            "132 안에\n",
            "133 완성된다\n",
            "134 탈원전과\n",
            "135 동남권\n",
            "136 신공항은\n",
            "137 대통령이\n",
            "138 대선\n",
            "139 공약으로\n",
            "140 내건\n",
            "141 사업이니\n",
            "142 여기에\n",
            "143 불법이\n",
            "144 있었다고\n",
            "145 시비를\n",
            "146 거는\n",
            "147 것은\n",
            "148 민주주의를\n",
            "149 부정하는\n",
            "150 것이라고\n",
            "151 청와대\n",
            "152 출신\n",
            "153 윤건영\n",
            "154 의원이\n",
            "155 윽박지른다\n",
            "156 '민주주의\n",
            "157 민주당'이\n",
            "158 법위에\n",
            "159 군림하는\n",
            "160 '반민주'를\n",
            "161 거리낌없이\n",
            "162 획책하는\n",
            "163 언급했다\n",
            "164 표를\n",
            "165 얻기\n",
            "166 나라\n",
            "167 곳간을\n",
            "168 다\n",
            "169 허물어뜨렸고\n",
            "170 재정\n",
            "171 운용에서\n",
            "172 신중함은\n",
            "173 사라졌다\n",
            "174 괴물\n",
            "175 출범하면\n",
            "176 공무원\n",
            "177 누구나\n",
            "178 권력이\n",
            "179 지시하는\n",
            "180 범죄행위에\n",
            "181 거리낌\n",
            "182 없이\n",
            "183 가담할\n",
            "184 것이다\n",
            "185 권부\n",
            "186 요직에\n",
            "187 앉아\n",
            "188 불법으로\n",
            "189 각종\n",
            "190 이권을\n",
            "191 챙기는\n",
            "192 권력자들에\n",
            "193 대한\n",
            "194 사건이\n",
            "195 불거져도\n",
            "196 사건을\n",
            "197 가져가\n",
            "198 버리면\n",
            "199 그만\n",
            "200 우려했다\n",
            "201 고위\n",
            "202 공직자들을\n",
            "203 처벌하는\n",
            "204 것인데\n",
            "205 반대하는지\n",
            "206 이해할\n",
            "207 없다'고\n",
            "208 했는데\n",
            "209 그런\n",
            "210 분이\n",
            "211 대통령\n",
            "212 주변을\n",
            "213 감시하는\n",
            "214 특별감찰관은\n",
            "215 취임\n",
            "216 이후\n",
            "217 지금까지\n",
            "218 임명하지\n",
            "219 않았는가\n",
            "220 라며\n",
            "221 권력형\n",
            "222 비리의\n",
            "223 쓰레기\n",
            "224 하치장\n",
            "225 종말\n",
            "226 처리장이\n",
            "227 비판했다\n",
            "228 정부를\n",
            "229 사도들은\n",
            "230 법치가\n",
            "231 미치지\n",
            "232 무오류의\n",
            "233 화신이\n",
            "234 오류를\n",
            "235 존재가\n",
            "236 바로\n",
            "237 신이며\n",
            "238 아래에는\n",
            "239 자신들의\n",
            "240 지도자를\n",
            "241 목숨바쳐\n",
            "242 지킴으로서\n",
            "243 정의를\n",
            "244 실현하겠다는\n",
            "245 추종자들로\n",
            "246 넘쳐\n",
            "247 난다\n",
            "248 지도자의\n",
            "249 신성을\n",
            "250 세력을\n",
            "251 정죄하는\n",
            "252 수단으로\n",
            "253 전락할\n",
            "254 질타했다\n",
            "255 저도\n",
            "256 법조인이지만\n",
            "257 공수처장이\n",
            "258 마음대로\n",
            "259 검사들과\n",
            "260 수사관들을\n",
            "261 임명하는\n",
            "262 끔찍한\n",
            "263 사법기구가\n",
            "264 어떤\n",
            "265 일을\n",
            "266 할지\n",
            "267 두렵기만\n",
            "268 하다\n",
            "269 검찰과\n",
            "270 경찰\n",
            "271 위에\n",
            "272 사법기구로\n",
            "273 헌법과\n",
            "274 법으로\n",
            "275 독립성을\n",
            "276 보장하는\n",
            "277 검찰총장을\n",
            "278 이렇게\n",
            "279 핍박하는\n",
            "280 어떻게\n",
            "281 운영할지\n",
            "282 불을\n",
            "283 보듯\n",
            "284 뻔한\n",
            "285 일\n",
            "286 예측했다\n",
            "287 추미애\n",
            "288 법무장관을\n",
            "289 앞장\n",
            "290 세워\n",
            "291 윤석열\n",
            "292 검찰의\n",
            "293 비리\n",
            "294 수사를\n",
            "295 저지하려다가\n",
            "296 난관에\n",
            "297 봉착하자\n",
            "298 무슨\n",
            "299 수를\n",
            "300 써서라도\n",
            "301 출범시키려\n",
            "302 한다\n",
            "303 자리에는\n",
            "304 추미애보다\n",
            "305 더\n",
            "306 한\n",
            "307 막무가내\n",
            "308 내\n",
            "309 편을\n",
            "310 앉힐\n",
            "311 분명한\n",
            "312 파렴치와\n",
            "313 오만함을\n",
            "314 최전선에서\n",
            "315 온\n",
            "316 몸으로\n",
            "317 겪어온\n",
            "318 저로서는\n",
            "319 내일부터\n",
            "320 보일\n",
            "321 행태가\n",
            "322 환히\n",
            "323 180석의\n",
            "324 또\n",
            "325 군사작전을\n",
            "326 개시하면\n",
            "327 누가\n",
            "328 막겠는가\n",
            "329 라고\n",
            "330 성토했다\n",
            "331 막을\n",
            "332 힘이\n",
            "333 우리\n",
            "334 야당에게는\n",
            "335 없다\n",
            "336 삭발하고\n",
            "337 장외투쟁해\n",
            "338 봐야\n",
            "339 눈\n",
            "340 하나\n",
            "341 깜짝할\n",
            "342 아닌\n",
            "343 대란대치\n",
            "344 大亂大治\n",
            "345 세상을\n",
            "346 온통\n",
            "347 혼돈\n",
            "348 속으로\n",
            "349 밀어넣고\n",
            "350 유지에\n",
            "351 이용한다는\n",
            "352 통치기술\n",
            "353 규탄했다\n",
            "354 아울러\n",
            "355 바람\n",
            "356 국민은\n",
            "357 풀이다\n",
            "358 바람이\n",
            "359 불면\n",
            "360 청보리\n",
            "361 밭의\n",
            "362 보리가\n",
            "363 눕는다\n",
            "364 다시는\n",
            "365 일어서지\n",
            "366 못하도록\n",
            "367 풀을\n",
            "368 짓밟지만\n",
            "369 풀들은\n",
            "370 다시\n",
            "371 일어난다\n",
            "372 시인\n",
            "373 김수영은\n",
            "374 '바람보다\n",
            "375 눕지만\n",
            "376 바람보다\n",
            "377 일어나는'\n",
            "378 민초의\n",
            "379 힘을\n",
            "380 노래했다\n",
            "381 고\n",
            "382 말했다\n",
            "383 마지막으로\n",
            "384 정권은\n",
            "385 곧\n",
            "386 광장에서\n",
            "387 일어서서\n",
            "388 아우성치는\n",
            "389 모습을\n",
            "390 지켜보게\n",
            "391 대란대치를\n",
            "392 끝장내려는\n",
            "393 거듭\n",
            "394 강조했다\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F740wH5sF9lG"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.layers import (Dense, \r\n",
        "                                     BatchNormalization, \r\n",
        "                                     LeakyReLU,\r\n",
        "                                     Softmax,\r\n",
        "                                     Reshape, \r\n",
        "                                     Conv2DTranspose,\r\n",
        "                                     Conv2D,\r\n",
        "                                     Dropout,\r\n",
        "                                     Flatten)\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tncFJH6gGaWA",
        "outputId": "c9c0ba34-a8df-468b-8229-8bffcef4e86a"
      },
      "source": [
        "\r\n",
        "def make_generator_model():\r\n",
        "    model = tf.keras.Sequential()\r\n",
        "    model.add(Dense(256, use_bias=False, input_shape=(50,)))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(LeakyReLU())\r\n",
        "    #model.add(Reshape((256,)))\r\n",
        "    assert model.output_shape == (None,256) # Note: None is the batch size\r\n",
        "\r\n",
        "    model.add(Dense(64, use_bias=False))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(Softmax())\r\n",
        "\r\n",
        "    #model.add(Reshape((64,)))\r\n",
        "    assert model.output_shape == (None,64) # Note: None is the batch size\r\n",
        "\r\n",
        "    model.summary()\r\n",
        "    return model\r\n",
        "\r\n",
        "generator = make_generator_model()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 256)               12800     \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                16384     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "softmax (Softmax)            (None, 64)                0         \n",
            "=================================================================\n",
            "Total params: 30,464\n",
            "Trainable params: 29,824\n",
            "Non-trainable params: 640\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtDTV8mUINh1"
      },
      "source": [
        "# Create a random noise and generate a sample\r\n",
        "noise = tf.random.normal([10, 50])\r\n",
        "generated_text = generator(noise, training=False)\r\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XH1mBsLwJRX4",
        "outputId": "f06a0378-8875-4177-be6d-b007f6bdaee2"
      },
      "source": [
        "generated_text"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10, 64), dtype=float32, numpy=\n",
              "array([[0.01513626, 0.01168383, 0.00938422, 0.01528631, 0.00754556,\n",
              "        0.01316108, 0.00716574, 0.01651304, 0.02208476, 0.01300555,\n",
              "        0.01769161, 0.02049619, 0.01499179, 0.00776885, 0.01241255,\n",
              "        0.00439605, 0.01365743, 0.02238177, 0.01237988, 0.01307812,\n",
              "        0.02075309, 0.01090393, 0.01609082, 0.01354598, 0.01462952,\n",
              "        0.03287378, 0.01270114, 0.02632295, 0.01381229, 0.01298717,\n",
              "        0.01155249, 0.01093738, 0.01095644, 0.02230717, 0.00636088,\n",
              "        0.01055169, 0.02025925, 0.01624437, 0.01670043, 0.01802511,\n",
              "        0.00973136, 0.01906495, 0.02147042, 0.02702578, 0.01335141,\n",
              "        0.02191122, 0.01410525, 0.01707666, 0.01268106, 0.03204919,\n",
              "        0.00859776, 0.00969196, 0.02576758, 0.01689919, 0.01780741,\n",
              "        0.00386661, 0.01315148, 0.00807407, 0.01212006, 0.02364339,\n",
              "        0.02022686, 0.02732211, 0.02484374, 0.01078396],\n",
              "       [0.01546958, 0.01436912, 0.01060897, 0.02074404, 0.01362762,\n",
              "        0.00977116, 0.01717753, 0.00885346, 0.01009629, 0.00451616,\n",
              "        0.01345653, 0.00556793, 0.01881922, 0.01433741, 0.00921428,\n",
              "        0.01096313, 0.01505206, 0.0136579 , 0.01998211, 0.00664926,\n",
              "        0.00713313, 0.00957573, 0.01817114, 0.01059838, 0.01351245,\n",
              "        0.01500779, 0.03573374, 0.01936068, 0.00828914, 0.03261232,\n",
              "        0.0079224 , 0.01296056, 0.01804772, 0.00952337, 0.00982268,\n",
              "        0.00659513, 0.01705494, 0.03423773, 0.00357069, 0.01786699,\n",
              "        0.00632755, 0.01001706, 0.00772755, 0.0119461 , 0.01625096,\n",
              "        0.00940988, 0.02560709, 0.02756623, 0.02135173, 0.03047958,\n",
              "        0.0308217 , 0.00973815, 0.01533417, 0.01370008, 0.00477179,\n",
              "        0.0396469 , 0.02474044, 0.01549659, 0.00993628, 0.00522959,\n",
              "        0.01691191, 0.0673581 , 0.0105133 , 0.00858683],\n",
              "       [0.00952532, 0.01272411, 0.01367898, 0.01210996, 0.0208166 ,\n",
              "        0.01436357, 0.01510876, 0.00886956, 0.01644327, 0.03034862,\n",
              "        0.03261082, 0.01118901, 0.03132572, 0.00738877, 0.00893765,\n",
              "        0.02969841, 0.02743884, 0.01485116, 0.00562568, 0.02663456,\n",
              "        0.02744018, 0.00719721, 0.01048199, 0.01163425, 0.01236366,\n",
              "        0.02577063, 0.00815552, 0.01053617, 0.01014037, 0.01168128,\n",
              "        0.01041986, 0.01019371, 0.01734833, 0.00869053, 0.02562412,\n",
              "        0.01488559, 0.03194543, 0.01589442, 0.00781022, 0.01331278,\n",
              "        0.01172374, 0.01666391, 0.00635397, 0.02395783, 0.01650936,\n",
              "        0.00636369, 0.0193462 , 0.01134237, 0.02063106, 0.01045869,\n",
              "        0.01295673, 0.02402674, 0.01099097, 0.01662249, 0.01916723,\n",
              "        0.00537473, 0.03333184, 0.00969955, 0.01315035, 0.02319499,\n",
              "        0.00856187, 0.01107709, 0.0129767 , 0.01430233],\n",
              "       [0.01611089, 0.01293905, 0.01205541, 0.00959942, 0.00872853,\n",
              "        0.02278911, 0.01512695, 0.00829126, 0.01586914, 0.02041771,\n",
              "        0.02258497, 0.01269498, 0.0104742 , 0.01751083, 0.01760106,\n",
              "        0.01215094, 0.01259103, 0.03402366, 0.02468957, 0.04116301,\n",
              "        0.01747948, 0.01748924, 0.00902008, 0.00986382, 0.00582531,\n",
              "        0.01071098, 0.01751717, 0.01414546, 0.00943276, 0.02233167,\n",
              "        0.0284985 , 0.00351537, 0.00682335, 0.02138595, 0.02037861,\n",
              "        0.01807255, 0.00850662, 0.01695094, 0.01218033, 0.01073252,\n",
              "        0.03104152, 0.00748916, 0.00363905, 0.03996279, 0.02773985,\n",
              "        0.00740921, 0.01390711, 0.00771287, 0.01132345, 0.01781735,\n",
              "        0.01786784, 0.02784141, 0.01888808, 0.03141552, 0.00741066,\n",
              "        0.00614941, 0.01380876, 0.01158401, 0.00804112, 0.00567925,\n",
              "        0.00566863, 0.00844258, 0.03123452, 0.00965344],\n",
              "       [0.02371453, 0.01018841, 0.01265406, 0.00671824, 0.01183102,\n",
              "        0.01691634, 0.01973569, 0.01009053, 0.00965743, 0.00443966,\n",
              "        0.01674845, 0.00393216, 0.00984052, 0.05641871, 0.01820646,\n",
              "        0.00630338, 0.01743106, 0.0246439 , 0.02115797, 0.01557019,\n",
              "        0.00856372, 0.00662829, 0.01733253, 0.00563521, 0.01254421,\n",
              "        0.00940167, 0.01971079, 0.05481518, 0.01298266, 0.01287152,\n",
              "        0.01878586, 0.01915686, 0.01242726, 0.01074937, 0.02340012,\n",
              "        0.01130535, 0.00733631, 0.02144048, 0.0111861 , 0.00641738,\n",
              "        0.0114207 , 0.00796529, 0.02183608, 0.027937  , 0.01047553,\n",
              "        0.013749  , 0.00958572, 0.02903807, 0.01901896, 0.02076193,\n",
              "        0.03645912, 0.00977513, 0.02991845, 0.02322948, 0.00922964,\n",
              "        0.00593466, 0.00966878, 0.01046385, 0.02614252, 0.00592251,\n",
              "        0.00466126, 0.01785681, 0.01174088, 0.008319  ],\n",
              "       [0.0111755 , 0.02183861, 0.01134395, 0.01605197, 0.02465531,\n",
              "        0.00435398, 0.02254493, 0.02329814, 0.0071706 , 0.01163119,\n",
              "        0.02544517, 0.00934023, 0.00885645, 0.00723332, 0.01806653,\n",
              "        0.02847834, 0.02794555, 0.00862576, 0.0303403 , 0.01001407,\n",
              "        0.0251806 , 0.01032409, 0.02054685, 0.02915403, 0.00933193,\n",
              "        0.00664588, 0.01349963, 0.00913722, 0.01270339, 0.00520199,\n",
              "        0.00661662, 0.02689914, 0.00547832, 0.02173845, 0.01724509,\n",
              "        0.01028171, 0.014894  , 0.03418947, 0.00943759, 0.02993911,\n",
              "        0.00414437, 0.00776553, 0.00869763, 0.01510365, 0.03343856,\n",
              "        0.01315226, 0.03047723, 0.02715367, 0.01419936, 0.01207861,\n",
              "        0.02539965, 0.0113425 , 0.0157454 , 0.00519323, 0.00852959,\n",
              "        0.01137161, 0.0267946 , 0.00674342, 0.01649978, 0.01292186,\n",
              "        0.01354012, 0.01531589, 0.00815614, 0.00938038],\n",
              "       [0.00825552, 0.01832791, 0.00843323, 0.01170894, 0.01339766,\n",
              "        0.02930405, 0.00743948, 0.01942811, 0.01012937, 0.02276994,\n",
              "        0.01255937, 0.01747515, 0.02257296, 0.01602411, 0.01992485,\n",
              "        0.0080689 , 0.025911  , 0.01406379, 0.01035386, 0.01843244,\n",
              "        0.03584534, 0.01432463, 0.03971429, 0.00449217, 0.01824275,\n",
              "        0.05744905, 0.01990121, 0.01577903, 0.00317533, 0.00855491,\n",
              "        0.01996546, 0.01097512, 0.01779249, 0.00872151, 0.00990961,\n",
              "        0.00748598, 0.01399524, 0.01453566, 0.00867795, 0.01089306,\n",
              "        0.01760976, 0.00681209, 0.02994183, 0.01204374, 0.00855403,\n",
              "        0.00470454, 0.02808942, 0.01669988, 0.02779564, 0.01311438,\n",
              "        0.01161866, 0.01734743, 0.01604146, 0.02289196, 0.01191007,\n",
              "        0.01323477, 0.00901366, 0.01592578, 0.00762191, 0.01203209,\n",
              "        0.0129113 , 0.00696463, 0.0119122 , 0.01019734],\n",
              "       [0.00674919, 0.00604343, 0.03843379, 0.00823477, 0.00817374,\n",
              "        0.04069869, 0.01838342, 0.00977471, 0.00849353, 0.01325886,\n",
              "        0.01002473, 0.01282028, 0.01635004, 0.00592506, 0.02005606,\n",
              "        0.01257551, 0.02554651, 0.02229466, 0.00767665, 0.00973077,\n",
              "        0.05863848, 0.00570791, 0.02557932, 0.00531756, 0.01441883,\n",
              "        0.01507437, 0.00998938, 0.02584543, 0.01375816, 0.00743462,\n",
              "        0.02009301, 0.02137258, 0.01186879, 0.01463355, 0.01226827,\n",
              "        0.02042779, 0.01547719, 0.01330846, 0.01161199, 0.02195168,\n",
              "        0.00920953, 0.020184  , 0.02691347, 0.01809036, 0.02470993,\n",
              "        0.00528656, 0.02008237, 0.01619676, 0.0104268 , 0.01029649,\n",
              "        0.02193736, 0.01121121, 0.02057083, 0.00587538, 0.01764902,\n",
              "        0.00473765, 0.01147657, 0.02191734, 0.00315162, 0.01233468,\n",
              "        0.01542781, 0.01485653, 0.01188644, 0.01954946],\n",
              "       [0.00486077, 0.00296264, 0.01463807, 0.01635397, 0.01422451,\n",
              "        0.00900825, 0.00775738, 0.01154976, 0.02153983, 0.01425539,\n",
              "        0.02114437, 0.0156109 , 0.01337672, 0.00562638, 0.01600792,\n",
              "        0.00702458, 0.01242565, 0.0059652 , 0.00686657, 0.00924   ,\n",
              "        0.07546265, 0.01556896, 0.03028549, 0.01545731, 0.01706509,\n",
              "        0.023144  , 0.01324102, 0.0190853 , 0.01060835, 0.00630912,\n",
              "        0.01768478, 0.03073841, 0.0138992 , 0.01385129, 0.01354263,\n",
              "        0.01095119, 0.0122079 , 0.00869817, 0.01199186, 0.01494724,\n",
              "        0.02245152, 0.01706596, 0.02796856, 0.0314247 , 0.01876867,\n",
              "        0.01128437, 0.01896698, 0.00706762, 0.01156181, 0.00837447,\n",
              "        0.00791055, 0.01800492, 0.02177307, 0.01428205, 0.01444932,\n",
              "        0.01057374, 0.01765841, 0.01015346, 0.0165056 , 0.03084184,\n",
              "        0.00837847, 0.01210356, 0.01537564, 0.02187586],\n",
              "       [0.00835209, 0.02004196, 0.02664724, 0.00801856, 0.01101788,\n",
              "        0.00858022, 0.01643707, 0.01509009, 0.01083599, 0.00909059,\n",
              "        0.01386101, 0.03100218, 0.01082798, 0.0079199 , 0.01162738,\n",
              "        0.01243408, 0.01692724, 0.01641161, 0.01256117, 0.01532596,\n",
              "        0.02013535, 0.01966293, 0.01035368, 0.01782987, 0.00868653,\n",
              "        0.011258  , 0.0108098 , 0.01268659, 0.01185266, 0.01395503,\n",
              "        0.00788348, 0.02335201, 0.01001982, 0.01711884, 0.03309549,\n",
              "        0.00881899, 0.01025442, 0.01599812, 0.01979302, 0.01532472,\n",
              "        0.012622  , 0.01768484, 0.01797971, 0.03519057, 0.03913173,\n",
              "        0.02640233, 0.01095484, 0.01180399, 0.03091166, 0.01877883,\n",
              "        0.01464248, 0.01547136, 0.01343699, 0.014505  , 0.01238504,\n",
              "        0.01023448, 0.02047197, 0.01045501, 0.01207596, 0.00826793,\n",
              "        0.00811142, 0.01771117, 0.01873982, 0.02013133]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJtq_dPkITsD",
        "outputId": "9a426e0e-eab5-4f45-e690-addc65b0e2c5"
      },
      "source": [
        "generated_text2 = generated_text[0] * total_words\r\n",
        "generated_text2"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(64,), dtype=float32, numpy=\n",
              "array([ 5.9788213,  4.6151137,  3.7067683,  6.038094 ,  2.9804945,\n",
              "        5.198628 ,  2.8304672,  6.5226507,  8.723481 ,  5.137191 ,\n",
              "        6.9881845,  8.095993 ,  5.921757 ,  3.0686965,  4.9029584,\n",
              "        1.7364391,  5.394686 ,  8.840798 ,  4.8900537,  5.165857 ,\n",
              "        8.197471 ,  4.3070526,  6.355872 ,  5.350662 ,  5.778662 ,\n",
              "       12.985143 ,  5.0169487, 10.397564 ,  5.455855 ,  5.1299314,\n",
              "        4.563235 ,  4.3202634,  4.3277955,  8.811333 ,  2.5125494,\n",
              "        4.1679173,  8.002402 ,  6.416525 ,  6.5966706,  7.119917 ,\n",
              "        3.843886 ,  7.5306535,  8.480815 , 10.675183 ,  5.273806 ,\n",
              "        8.654934 ,  5.5715747,  6.7452807,  5.0090194, 12.659431 ,\n",
              "        3.396117 ,  3.8283262, 10.178193 ,  6.675179 ,  7.033929 ,\n",
              "        1.5273108,  5.1948333,  3.1892574,  4.787424 ,  9.33914  ,\n",
              "        7.9896083, 10.7922325,  9.813275 ,  4.2596645], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHXjReG5JXxK",
        "outputId": "086b3eb9-1347-41ff-a234-a0c0783d5e32"
      },
      "source": [
        "generated_sum = \"\"\r\n",
        "for idx in generated_text2:\r\n",
        "  #print(int(idx))\r\n",
        "  for word,index in tokenizer.word_index.items():\r\n",
        "      if index == int(idx):\r\n",
        "          generated_sum += word + ' '\r\n",
        "          break\r\n",
        "\r\n",
        "print(generated_sum)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "며 이라고 것 문재인 주 며 주 문재인 문 며 문재인 문 며 것 이라고 원내대표는 며 문 이라고 며 문 이라고 문재인 며 며 수 며 이제 며 며 이라고 이라고 이라고 문 주 이라고 문 문재인 문재인 않는 것 않는 문 이제 며 문 며 문재인 며 수 것 것 이제 문재인 않는 원내대표는 며 것 이라고 이라며 않는 이제 이라며 이라고 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-TaYl0WKc91",
        "outputId": "8e0775aa-e0f8-4793-b506-150e986bcc83"
      },
      "source": [
        "def make_discriminator_model():\r\n",
        "    model = tf.keras.Sequential()\r\n",
        "    model.add(Dense(256, use_bias=False, input_shape=(1024,)))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(LeakyReLU())\r\n",
        "    #model.add(Reshape((256,)))\r\n",
        "    assert model.output_shape == (None,256) # Note: None is the batch size\r\n",
        "\r\n",
        "    model.add(Dense(64, use_bias=False))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(LeakyReLU())    \r\n",
        "    #model.add(Reshape((64,)))\r\n",
        "    assert model.output_shape == (None,64) # Note: None is the batch size\r\n",
        "\r\n",
        "    model.add(Dense(32, use_bias=False))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(LeakyReLU())    \r\n",
        "    #model.add(Reshape((64,)))\r\n",
        "    assert model.output_shape == (None,32) # Note: None is the batch size\r\n",
        "\r\n",
        "    model.add(Dense(1))\r\n",
        "    #model.add(Softmax())    \r\n",
        "    model.summary()\r\n",
        "    return model\r\n",
        "\r\n",
        "discriminator = make_discriminator_model()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_6 (Dense)              (None, 256)               262144    \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 64)                16384     \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 32)                2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 282,017\n",
            "Trainable params: 281,313\n",
            "Non-trainable params: 704\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebGt54VjLRxV",
        "outputId": "2278aab0-4f5e-4fa7-acd9-9584dc62c7cc"
      },
      "source": [
        "!pip install sentence-transformers"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.6/dist-packages (0.3.9)\n",
            "Requirement already satisfied: transformers<3.6.0,>=3.1.0 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.5.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.7.0+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.19.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (0.1.91)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (0.0.43)\n",
            "Requirement already satisfied: tokenizers==0.9.3 in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (0.9.3)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (20.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (3.12.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (1.0.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->sentence-transformers) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers<3.6.0,>=3.1.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers<3.6.0,>=3.1.0->sentence-transformers) (2.4.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers<3.6.0,>=3.1.0->sentence-transformers) (50.3.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.6.0,>=3.1.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.6.0,>=3.1.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.6.0,>=3.1.0->sentence-transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.6.0,>=3.1.0->sentence-transformers) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty353n5ILmKw"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\r\n",
        "# embedder download...\r\n",
        "embedder = SentenceTransformer('xlm-r-large-en-ko-nli-ststb')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MszYFgQzM_RF",
        "outputId": "84127f20-cb2e-4f3e-a4a4-ffcc39041f91"
      },
      "source": [
        "#generated_encode = embedder.encode([generated_sum])\r\n",
        "decision = discriminator(generated_embedding)\r\n",
        "print (decision)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[ 0.02082823]\n",
            " [-0.05240061]\n",
            " [ 0.07198735]\n",
            " [ 0.01703282]\n",
            " [ 0.4902227 ]\n",
            " [ 0.00279847]\n",
            " [ 0.27390167]\n",
            " [ 0.4966565 ]\n",
            " [ 0.53378   ]\n",
            " [ 0.14577103]\n",
            " [ 0.00360338]\n",
            " [-0.20994215]\n",
            " [ 0.09074389]\n",
            " [ 0.1196237 ]\n",
            " [ 0.5612309 ]\n",
            " [ 0.09767302]\n",
            " [ 0.25924385]\n",
            " [ 0.22576228]\n",
            " [-0.07519284]\n",
            " [-0.00087412]\n",
            " [ 0.377385  ]\n",
            " [ 0.2900403 ]\n",
            " [ 0.06445952]\n",
            " [ 0.22728139]\n",
            " [ 0.23552147]\n",
            " [ 0.10944294]\n",
            " [ 0.16658542]\n",
            " [ 0.39936018]\n",
            " [ 0.13387027]\n",
            " [-0.29398772]\n",
            " [ 0.2340291 ]\n",
            " [ 0.4496358 ]], shape=(32, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llMmknB8OtEl"
      },
      "source": [
        "# This method returns a helper function to compute cross entropy loss\r\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\r\n",
        "\r\n",
        "def discriminator_loss(real_output, fake_output):\r\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\r\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\r\n",
        "    total_loss = real_loss + fake_loss\r\n",
        "    return total_loss\r\n",
        "\r\n",
        "def generator_loss(fake_output):\r\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\r\n",
        "\r\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\r\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNsIOAWeOx87"
      },
      "source": [
        "BATCH_SIZE = 32\r\n",
        "EPOCHS = 60\r\n",
        "noise_dim = 50\r\n",
        "seed = tf.random.normal([1, noise_dim])"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ui876PeQgJ1"
      },
      "source": [
        "def generated_code_to_text(generated_code,total_words):\r\n",
        "  generated_sum = []\r\n",
        "  generated_code = [c * total_words for c in generated_code]\r\n",
        "  for g in generated_code:\r\n",
        "    summary_txt = \"\"\r\n",
        "    for idx in g:\r\n",
        "      #print(int(idx))\r\n",
        "      for word,index in tokenizer.word_index.items():\r\n",
        "          if index == int(idx):\r\n",
        "              summary_txt += word + ' '\r\n",
        "              break\r\n",
        "    generated_sum.append(summary_txt)\r\n",
        "  return generated_sum, embedder.encode(generated_sum)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrEjBgDKQhbV",
        "outputId": "ac16749f-aa54-4a46-ac81-4ffa854cb2c8"
      },
      "source": [
        "noise = tf.random.normal([BATCH_SIZE, 50])\r\n",
        "generated_code = generator(noise, training=False)\r\n",
        "generated_text,generated_embedding = generated_code_to_text(generated_code,total_words)\r\n",
        "print(generated_text[0])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "이제 문재인 주 문재인 문재인 것 주 않는 며 공수처는 것 것 며 며 것 이제 며 문재인 문 대통령과 않는 이라고 며 풀들이 며 이라고 이라고 문 것 문재인 않는 며 주 문 것 이라며 문재인 않는 문 이라며 것 것 것 문 이라고 주 이라고 문재인 며 며 풀들이 며 문재인 이제 이라고 주 것 주 않는 이라고 주 않는 않는 것 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnZSqk6fPcdM"
      },
      "source": [
        "@tf.function\r\n",
        "def train_step(images):\r\n",
        "  \r\n",
        "    # 1 - Create a random noise to feed it into the model\r\n",
        "    # for the text generation\r\n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\r\n",
        "    \r\n",
        "    # 2 - Generate text and calculate loss values\r\n",
        "    # GradientTape method records operations for automatic differentiation.\r\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\r\n",
        "      generated_code = generator(noise, training=True)\r\n",
        "      generated_text,generated_embedding = generated_code_to_text(generated_code,total_words)\r\n",
        "      real_embedding = embedder.encode(images)\r\n",
        "      real_output = discriminator(real_embedding, training=True)\r\n",
        "      fake_output = discriminator(generated_embedding, training=True)\r\n",
        "\r\n",
        "      gen_loss = generator_loss(fake_output)\r\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\r\n",
        "\r\n",
        "    # 3 - Calculate gradients using loss values and model variables\r\n",
        "    # \"gradient\" method computes the gradient using \r\n",
        "    # operations recorded in context of this tape (gen_tape and disc_tape).\r\n",
        "    \r\n",
        "    # It accepts a target (e.g., gen_loss) variable and \r\n",
        "    # a source variable (e.g.,generator.trainable_variables)\r\n",
        "    # target --> a list or nested structure of Tensors or Variables to be differentiated.\r\n",
        "    # source --> a list or nested structure of Tensors or Variables.\r\n",
        "    # target will be differentiated against elements in sources.\r\n",
        "\r\n",
        "    # \"gradient\" method returns a list or nested structure of Tensors  \r\n",
        "    # (or IndexedSlices, or None), one for each element in sources. \r\n",
        "    # Returned structure is the same as the structure of sources.\r\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, \r\n",
        "                                               generator.trainable_variables)\r\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, \r\n",
        "                                                discriminator.trainable_variables)\r\n",
        "    \r\n",
        "    # 4 - Process  Gradients and Run the Optimizer\r\n",
        "    # \"apply_gradients\" method processes aggregated gradients. \r\n",
        "    # ex: optimizer.apply_gradients(zip(grads, vars))\r\n",
        "    \"\"\"\r\n",
        "    Example use of apply_gradients:\r\n",
        "    grads = tape.gradient(loss, vars)\r\n",
        "    grads = tf.distribute.get_replica_context().all_reduce('sum', grads)\r\n",
        "    # Processing aggregated gradients.\r\n",
        "    optimizer.apply_gradients(zip(grads, vars), experimental_aggregate_gradients=False)\r\n",
        "    \"\"\"\r\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\r\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ml71bju6Rv9I"
      },
      "source": [
        "dataset = []\r\n",
        "for i in range(1000):\r\n",
        "  dataset.append(document)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpu1ggH1Rl17"
      },
      "source": [
        "import time\r\n",
        "from IPython import display # A command shell for interactive computing in Python.\r\n",
        "\r\n",
        "def train(dataset, epochs):\r\n",
        "  # A. For each epoch, do the following:\r\n",
        "  for epoch in range(epochs):\r\n",
        "    start = time.time()\r\n",
        "    # 1 - For each batch of the epoch, \r\n",
        "    for image_batch in dataset:\r\n",
        "      # 1.a - run the custom \"train_step\" function\r\n",
        "      # we just declared above\r\n",
        "      train_step(image_batch)\r\n",
        "\r\n",
        "    # 4 - Print out the completed epoch no. and the time spent\r\n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "v1z2L5zLUIhr",
        "outputId": "321a6389-479f-4e11-d462-f88340227686"
      },
      "source": [
        "train(dataset, EPOCHS)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OperatorNotAllowedInGraphError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-178de3cf74ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-38-036ee645db1b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, epochs)\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0;31m# 1.a - run the custom \"train_step\" function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0;31m# we just declared above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m       \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# 4 - Print out the completed epoch no. and the time spent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    725\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 726\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m: in user code:\n\n    <ipython-input-36-8a09236a5a1b>:12 train_step  *\n        generated_text,generated_embedding = generated_code_to_text(generated_code,total_words)\n    <ipython-input-33-7f344976a69a>:3 generated_code_to_text  *\n        generated_code = [c * total_words for c in generated_code]\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:505 __iter__\n        self._disallow_iteration()\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:498 _disallow_iteration\n        self._disallow_when_autograph_enabled(\"iterating over `tf.Tensor`\")\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:476 _disallow_when_autograph_enabled\n        \" indicate you are trying to use an unsupported feature.\".format(task))\n\n    OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\n"
          ]
        }
      ]
    }
  ]
}